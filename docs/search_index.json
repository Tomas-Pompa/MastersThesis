[["simulace4.html", "Kapitola 9 Simulace 4 9.1 Klasifikace na základě první derivace 9.2 Klasifikace na základě druhé derivace", " Kapitola 9 Simulace 4 V této poslední sekci věnující se simulovaným datům se budeme zabývat stejnými daty jako v sekci 5 (případně také v sekcích 6 a 7), tedy půjde o generování funkcionálních dat z funkcí vypočtených pomocí interpolačních polynomů. Jelikož jsme v sekci 5 generovali data s náhodným vertikálním posunem s parametrem směrodatné odchylky \\(\\sigma_{shift}\\), mohli bychom se pokusit odstranit tento posun a klasifikovat data po odstranění tohoto posunu. Viděli jsme totiž v sekci 7, že se zvětšující se hodnotou parametru směrodatné odchylky \\(\\sigma_{shift}\\) se úspěšnost, zejména klasických klasifikačních metod, poměrně dramaticky zhoršuje. Naopak klasifikační metody beroucí do úvahy funkcionální podstatu dat se zpravidla i se zvětšující se hodnotou \\(\\sigma_{shift}\\) chovají poměrně stabilně. Jednou z možností k odstranění vertikálního posunutí, kterou využijeme v následující části, je klasifikovat data na základě odhadu první derivace dané vygenerované a vyhlazené křivky, neboť jak známo \\[ \\frac{\\text d}{\\text d t} \\big( x(t) + c \\big) = \\frac{\\text d}{\\text d t} x(t)= x&#39;(t). \\] 9.1 Klasifikace na základě první derivace Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro simulaci nejprve: zvolíme vhodné funkce, generujeme body ze zvoleného intervalu, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) library(tikzDevice) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 6]\\). Nyní vytvoříme funkce pomocí interpolačních polynomů. Nejprve si definujeme body, kterými má procházet naše křivka, a následně jimi proložíme interpolační polynom, který použijeme pro generování křivek pro klasifikaci. Code # definujici body pro tridu 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # definujici body pro tridu 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Pro výpočet interpolačních polynomů využijeme funkci poly.calc() z knihovny polynom. Dále definujeme funkce poly.0() a poly.1(), které budou počítat hodnoty polynomů v daném bodě intervalu. K jejich vytvoření použijeme funkci predict(), na jejíž vstup zadáme příslušný polynom a bod, ve kterám chceme polynom vyhodnotit. Code # vypocet polynomu polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # vykresleni polynomu xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 9.1: Znázornění dvou funkcí na intervalu \\(I = [0, 6]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce (parametr sigma_shift). Tento posun budeme generovat s normálního rozdělení s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 9.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 9.1.1 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Opět využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor t, jelikož uvažujeme první derivaci, volíme norder = 5. Budeme penalizovat třetí derivaci funkcí, neboť nyní požadujeme hladké i první derivace. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 5 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(3) # penalizujeme 3. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) Obrázek 9.3: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 9.4: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Obrázek 9.5: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Obrázek 9.6: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 9.1.2 Výpočet derivací K výpočtu derivace pro funkcionální objekt využijeme v R funkci deriv.fd() z balíčku fda. Jelikož chceme klasifikovat na základě první derivace, volíme argument Lfdobj = 1. Code XXder &lt;- deriv.fd(XXfd, 1) Nyní si vykresleme prvních několik prvních derivací pro obě klasifikační třídy. Všimněme si z obrázku níže, že se opravdu vertikální posun pomocí derivování opravdu podařilo odstranit. Ztratili jsme tím ale do jisté míry rozdílnost mezi křivkami, protože jak z obrázku vyplývá, křivky derivací pro obě třídy se liší primárně až ke konci intervalu, tedy pro argument v rozmezí přibližně \\([5, 6]\\). Code fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code abs.labs &lt;- paste(&quot;Klasifikační třída:&quot;, c(&quot;$Y = 0$&quot;, &quot;$Y = 1$&quot;)) names(abs.labs) &lt;- c(&#39;0&#39;, &#39;1&#39;) # fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(eval.fd(fdobj = mean.fd(XXder[1:n]), evalarg = t), eval.fd(fdobj = mean.fd(XXder[(n + 1):(2 * n)]), evalarg = t)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, #group = interaction(time, group), colour = group)) + geom_line(aes(group = time), linewidth = 0.05, alpha = 0.5) + theme_bw() + labs(x = &quot;$t$&quot;, # y = &quot;$\\\\frac{\\\\text d}{\\\\text d t} x_i(t)$&quot;, y =&quot;$x_i&#39;(t)$&quot;, colour = &#39;Klasifikační\\n třída&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;1&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;tomato&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;0&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;deepskyblue2&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + geom_line(data = DFmean |&gt; mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))), aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 0.8, linetype = &#39;dashed&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + facet_wrap(~group, labeller = labeller(group = abs.labs)) + scale_y_continuous(expand = c(0.02, 0.02)) + theme(legend.position = &#39;none&#39;, plot.margin = unit(c(0.1, 0.1, 0.3, 0.5), &quot;cm&quot;)) + coord_cartesian(ylim = c(-1.4, 3.5)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Obrázek 9.7: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Code # ggsave(&quot;figures/kap6_sim_04_curves_1der.tex&quot;, device = tikz, width = 8, height = 4) Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;grey3&#39;, linewidth = 0.7, linetype = &#39;dashed&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1.5, 2)) Obrázek 9.8: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 9.1.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 9.1.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) summary(neighb.model) # shrnuti modelu ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.7464789 0.6376812 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 53 18 ## 2 25 44 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.6143 0.6357 0.6357 0.6000 0.6429 0.6286 0.6357 0.6643 0.6786 0.6857 0.6857 ## 12 ## 0.6929 ## ## -Optimal number of neighbors: knn.opt= 12 ## with highest probability of correct classification max.prob= 0.6928571 ## ## -Probability of correct classification: 0.6929 Code plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K Code neighb.model$max.prob # maximalni presnost ## [1] 0.6928571 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 12 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results ## [1] 0.6527850 0.6398870 0.6691107 0.6308232 0.6556309 0.6512990 0.6556309 ## [8] 0.6772193 0.6875181 0.6799221 0.7086563 0.7174691 0.7338576 0.7406408 ## [15] 0.7239741 0.7266985 0.7220261 0.7106165 0.7129975 0.7183088 0.6819286 ## [22] 0.7020751 0.6656948 0.6732976 Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 14 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.7406. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 9.9: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) summary(neighb.model) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.7183099 0.6666667 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 51 20 ## 2 23 46 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 14 ## 0.6929 ## ## -Optimal number of neighbors: knn.opt= 14 ## with highest probability of correct classification max.prob= 0.6928571 ## ## -Probability of correct classification: 0.6929 Code # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost 1 - presnost ## [1] 0.1833333 Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 14, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.3071 a na testovacích datech 0.1833. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 9.1.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 3, které dohromady vysvětlují 93.96 % variability v datech. První hlavní komponenta potom vysvětluje 50.6 % a druhá 33.44 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 9.10: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 3 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (68.57 %), tak i na testovacích datech (76.67 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 9.11: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (65 %), tak i na testovacích datech (80 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 9.12: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 9.1.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 9.13: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 10, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) CV.results ## 4 5 6 7 8 9 10 11 ## 0.6490101 0.8110996 0.8217872 0.8241638 0.8574350 0.9140240 0.9127461 0.9251521 ## 12 13 14 15 16 17 18 19 ## 0.9152450 0.9043956 0.9315667 0.9155411 0.8812271 0.8873666 0.8741551 0.8754722 ## 20 21 22 23 24 25 ## 0.8862554 0.8714900 0.8847026 0.8727723 0.8679267 0.9137971 Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 14 s validační chybovostí 0.0684. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) Obrázek 9.14: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 14 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) ## Warning in Minverse(t(B) %*% B): System is computationally singular (rank 54) ## ## The matrix inverse is computed by svd (effective rank 52) Code # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) ## Warning in Minverse(t(B) %*% B): System is computationally singular (rank 54) ## ## The matrix inverse is computed by svd (effective rank 52) Code predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) ## Warning in Minverse(t(B) %*% B): System is computationally singular (rank 54) ## ## The matrix inverse is computed by svd (effective rank 52) Code predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 5 %) i testovací chybovost (rovna 11.67 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Třída&#39;) Obrázek 9.15: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Obrázek 9.16: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 6]\\). Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy přesnost klasifikátoru na trénovacích (68.57 %) i na testovacích datech (76.67 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 9.17: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 9.1.3.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [0, 6]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost klasifikátoru na testovacích datech je tedy 81.67 % a na trénovacích datech 73.57 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.18: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 9.19: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 3 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na testovacích datech je tedy 76.67 % a na trénovacích datech 70 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.20: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 9.21: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 73.57 % a na testovacích datech 81.67 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.22: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 9.23: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 9.1.3.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 6]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost náhodného lesu na trénovacích datech je tedy 100 % a na testovacích datech 80 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 3 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 98.57 % a na testovacích datech 70 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost tohoto klasifikátoru na trénovacích datech je 100 % a na testovacích datech 81.67 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 5.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 5.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 5.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 5.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 5.3.7.5. 9.1.3.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 6]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm &lt;- XXder XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 25, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.7, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 55, gamma = 0.0005, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 92.14 % pro lineární jádro, 87.14 % pro polynomiální jádro a 85.71 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 90 % pro lineární jádro, 76.67 % pro polynomiální jádro a 76.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 3 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.1, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.01, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1, gamma = 0.01, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 68.57 % pro lineární jádro, 68.57 % pro polynomiální jádro a 67.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 76.67 % pro lineární jádro, 76.67 % pro polynomiální jádro a 76.67 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 9.24: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 50, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, gamma = 0.001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 92.86 % pro lineární jádro, 80 % pro polynomiální jádro a 87.14 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 93.33 % pro lineární jádro, 78.33 % pro polynomiální jádro a 80 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), # matice diskretnich dat argvals = t.seq, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- t(Projection) # subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 12 0.1559432 ## poly 8 0.1982189 ## radial 8 0.1918956 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 12 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8441, 8 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8018 a 8 pro radiální jádro s hodnotou přesnosti 0.8081. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 9.25: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), # matice diskretnich dat argvals = t.seq, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 13.57 % pro lineární jádro, 13.57 % pro polynomiální jádro a 13.57 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 21.67 % pro lineární jádro, 26.67 % pro polynomiální jádro a 26.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 9.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 9.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 9.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 9.1.3.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 9.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{9.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (9.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{9.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 9.1.3.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku1 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 9.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 9.1.3.7.5.3 Implementace metody v R Z poslední části Tvrzení 9.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). 9.1.3.7.5.4 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0000 0.4833 SVM poly - RKHS 0.0000 0.4167 SVM rbf - RKHS 0.0214 0.3000 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 3, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 9.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 33 1000.0000 0.1278 linear poly 22 7.1969 0.1965 polynomial radial 12 0.2683 0.1782 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 33 a \\(\\gamma={}\\) 1000 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8722, \\(d={}\\) 22 a \\(\\gamma={}\\) 7.1969 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8035 a \\(d={}\\) 12 a \\(\\gamma={}\\) 0.2683 pro radiální jádro s hodnotou přesnosti 0.8218. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 9.26: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.1071 0.3000 SVM poly - RKHS - radial 0.1071 0.2500 SVM rbf - RKHS - radial 0.1357 0.3333 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 10.71 % pro lineární jádro, 10.71 % pro polynomiální jádro a 13.57 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 30 % pro lineární jádro, 25 % pro polynomiální jádro a 33.33 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7.5.5 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 9.4: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 21 3 0.1945 linear poly 7 5 0.1715 polynomial radial 8 5 0.2013 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 21 a \\(p={}\\) 3 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8055, \\(d={}\\) 7 a \\(p={}\\) 5 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8285 a \\(d={}\\) 8 a \\(p={}\\) 5 pro radiální jádro s hodnotou přesnosti 0.7987. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.5: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.1714 0.2333 SVM poly - RKHS - poly 0.1500 0.2833 SVM rbf - RKHS - poly 0.1786 0.3000 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 17.14 % pro lineární jádro, 15 % pro polynomiální jádro a 17.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 23.33 % pro lineární jádro, 28.33 % pro polynomiální jádro a 30 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7.5.6 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 9.6: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 9 0.3417 linear poly 17 0.3395 polynomial radial 19 0.3257 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 9 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.6583, \\(d={}\\) 17 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.6605 a \\(d={}\\) 19 pro radiální jádro s hodnotou přesnosti 0.6743. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.7: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.3000 0.2833 SVM poly - RKHS - linear 0.2857 0.2333 SVM rbf - RKHS - linear 0.3071 0.2667 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 30 % pro lineární jádro, 28.57 % pro polynomiální jádro a 30.71 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 28.33 % pro lineární jádro, 23.33 % pro polynomiální jádro a 26.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.8 Tabulka výsledků Tabulka 9.8: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3071 0.1833 LDA 0.3143 0.2333 QDA 0.3500 0.2000 LR functional 0.0500 0.1167 LR score 0.3143 0.2333 Tree - diskr. 0.2643 0.1833 Tree - score 0.3000 0.2333 Tree - Bbasis 0.2643 0.1833 RForest - diskr 0.0000 0.2000 RForest - score 0.0143 0.3000 RForest - Bbasis 0.0000 0.1833 SVM linear - diskr 0.0786 0.1000 SVM poly - diskr 0.1286 0.2333 SVM rbf - diskr 0.1429 0.2333 SVM linear - PCA 0.3143 0.2333 SVM poly - PCA 0.3143 0.2333 SVM rbf - PCA 0.3214 0.2333 SVM linear - Bbasis 0.0714 0.0667 SVM poly - Bbasis 0.2000 0.2167 SVM rbf - Bbasis 0.1286 0.2000 SVM linear - projection 0.1357 0.2167 SVM poly - projection 0.1357 0.2667 SVM rbf - projection 0.1357 0.2667 SVM linear - RKHS - radial 0.1071 0.3000 SVM poly - RKHS - radial 0.1071 0.2500 SVM rbf - RKHS - radial 0.1357 0.3333 SVM linear - RKHS - poly 0.1714 0.2333 SVM poly - RKHS - poly 0.1500 0.2833 SVM rbf - RKHS - poly 0.1786 0.3000 SVM linear - RKHS - linear 0.3000 0.2833 SVM poly - RKHS - linear 0.2857 0.2333 SVM rbf - RKHS - linear 0.3071 0.2667 9.1.4 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 5 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(3) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd # vypocet derivace XXder &lt;- deriv.fd(XXfd, 1) fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- 1:20 #c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # normovani dat norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm &lt;- XXder XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 25, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.7, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 55, gamma = 0.0005, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.1, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.01, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1, gamma = 0.01, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 50, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, gamma = 0.001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 #length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- t(Projection) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 5) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_04_cv.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/simulace_04_res_cv.RData&#39;) 9.1.4.1 Výsledky Tabulka 9.9: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.2288 0.2557 0.0418 0.0633 LDA 0.2295 0.2383 0.0415 0.0645 QDA 0.2234 0.2485 0.0440 0.0620 LR_functional 0.0384 0.1067 0.0273 0.0433 LR_score 0.2284 0.2400 0.0429 0.0635 Tree_discr 0.1823 0.2493 0.0477 0.0701 Tree_score 0.2092 0.2848 0.0461 0.0693 Tree_Bbasis 0.1854 0.2447 0.0542 0.0723 RF_discr 0.0106 0.2323 0.0085 0.0706 RF_score 0.0359 0.2693 0.0179 0.0694 RF_Bbasis 0.0111 0.2315 0.0082 0.0676 SVM linear - diskr 0.0649 0.1060 0.0277 0.0427 SVM poly - diskr 0.1147 0.2187 0.0537 0.0678 SVM rbf - diskr 0.1393 0.1868 0.0481 0.0611 SVM linear - PCA 0.2289 0.2427 0.0422 0.0646 SVM poly - PCA 0.2389 0.2860 0.0478 0.0720 SVM rbf - PCA 0.2292 0.2477 0.0434 0.0660 SVM linear - Bbasis 0.0584 0.0958 0.0271 0.0397 SVM poly - Bbasis 0.1506 0.2205 0.0492 0.0605 SVM rbf - Bbasis 0.1165 0.1747 0.0491 0.0617 SVM linear - projection 0.1192 0.1522 0.0428 0.0564 SVM poly - projection 0.0994 0.2043 0.0527 0.0632 SVM rbf - projection 0.1284 0.1995 0.0508 0.0676 SVM linear - RKHS - radial 0.1069 0.1782 0.0444 0.0581 SVM poly - RKHS - radial 0.0744 0.1973 0.0443 0.0680 SVM rbf - RKHS - radial 0.1091 0.1958 0.0473 0.0644 SVM linear - RKHS - poly 0.1430 0.2552 0.0626 0.0921 SVM poly - RKHS - poly 0.1032 0.2527 0.0747 0.0826 SVM rbf - RKHS - poly 0.1465 0.2355 0.0499 0.0796 SVM linear - RKHS - linear 0.2829 0.3630 0.0868 0.0907 SVM poly - RKHS - linear 0.2543 0.3568 0.0889 0.0855 SVM rbf - RKHS - linear 0.2802 0.3433 0.0768 0.0893 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # nastavime jinak nazvy klasifikacnich metod methods_names &lt;- c( &#39;$K$ nejbližších sousedů&#39;, &#39;Lineární diskriminační analýza&#39;, &#39;Kvadratická diskriminační analýza&#39;, &#39;Funkcionální logistická regrese&#39;, &#39;Logistické regrese s fPCA&#39;, &#39;Rozhodovací strom -- diskretizace&#39;, &#39;Rozhodovací strom -- fPCA&#39;, &#39;Rozhodovací strom -- bázové koeficienty&#39;, &#39;Náhodný les -- diskretizace&#39;, &#39;Náhodný les -- fPCA&#39;, &#39;Náhodný les -- bázové koeficienty&#39;, &#39;SVM (linear) -- diskretizace&#39;, &#39;SVM (poly) -- diskretizace&#39;, &#39;SVM (radial) -- diskretizace&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (poly) -- fPCA&#39;, &#39;SVM (radial) -- fPCA&#39;, &#39;SVM (linear) -- bázové koeficienty&#39;, &#39;SVM (poly) -- bázové koeficienty&#39;, &#39;SVM (radial) -- bázové koeficienty&#39;, &#39;SVM (linear) -- projekce&#39;, &#39;SVM (poly) -- projekce&#39;, &#39;SVM (radial) -- projekce&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39;, &#39;RKHS (radial SVR) $+$ SVM (poly)&#39;, &#39;RKHS (radial SVR) $+$ SVM (radial)&#39;, &#39;RKHS (poly SVR) $+$ SVM (linear)&#39;, &#39;RKHS (poly SVR) $+$ SVM (poly)&#39;, &#39;RKHS (poly SVR) $+$ SVM (radial)&#39;, &#39;RKHS (linear SVR) $+$ SVM (linear)&#39;, &#39;RKHS (linear SVR) $+$ SVM (poly)&#39;, &#39;RKHS (linear SVR) $+$ SVM (radial)&#39; ) # barvy pro boxploty box_col &lt;- c(&#39;#4dd2ff&#39;, &#39;#0099cc&#39;, &#39;#00ace6&#39;, &#39;#00bfff&#39;, &#39;#1ac5ff&#39;, rep(&#39;#33ccff&#39;, 3), rep(&#39;#0086b3&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff6347&#39;, 3), rep(&#39;#ff7961&#39;, 3), rep(&#39;#ff4d2e&#39;, 3), rep(&#39;#fa2600&#39;, 9)) # box_col &lt;- c(&#39;#CA0A0A&#39;, &#39;#fa2600&#39;, &#39;#fa2600&#39;, &#39;#D15804&#39;, # &#39;#D15804&#39;, rep(&#39;#D3006D&#39;, 3), rep(&#39;#BE090F&#39;, 3), c(&quot;#12DEE8&quot;, &quot;#4ECBF3&quot;, &quot;#127DE8&quot;, &quot;#4C3CD3&quot;, &quot;#4E65F3&quot;, &quot;#4E9EF3&quot;, &quot;#081D58&quot;) |&gt; rep(each = 3)) # alpha pro boxploty box_alpha &lt;- c(0.9, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, seq(0.9, 0.6, length = 9)) #- 0.3 Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 9.27: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = &quot;$\\\\widehat{\\\\textnormal{Err}}_{test}$&quot; # y = expression(widehat(Err)[test]) ) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 50, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 3, color = &quot;black&quot;, alpha = 0.9) + # scale_x_discrete(labels = methods_names) + # theme(plot.margin = unit(c(0.5, 0.5, 2, 2), &quot;cm&quot;)) + # scale_fill_manual(values = box_col) + # scale_alpha_manual(values = box_alpha) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;gray20&#39;, alpha = 0.8) Obrázek 9.28: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # ggsave(&quot;figures/kap6_sim_04_boxplot_test_1der.tex&quot;, device = tikz, width = 9, height = 7) Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 9.10: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 11.0 nharm 3.0 LR_func_n_basis 12.0 SVM_d_Linear 11.5 SVM_d_Poly 11.0 SVM_d_Radial 10.5 SVM_RKHS_radial_gamma1 3.2 SVM_RKHS_radial_gamma2 3.2 SVM_RKHS_radial_gamma3 3.2 SVM_RKHS_radial_d1 20.0 SVM_RKHS_radial_d2 15.0 SVM_RKHS_radial_d3 15.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 4.0 SVM_RKHS_poly_p3 4.0 SVM_RKHS_poly_d1 25.0 SVM_RKHS_poly_d2 25.0 SVM_RKHS_poly_d3 25.0 SVM_RKHS_linear_d1 15.0 SVM_RKHS_linear_d2 15.0 SVM_RKHS_linear_d3 15.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.29: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.30: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.31: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.32: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.33: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. 9.2 Klasifikace na základě druhé derivace V předchozí části jsme uvažovali první derivaci křivek. Nyní zopakujme celý proces na druhých derivacích. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu set.seed(42) n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 9.34: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 9.2.1 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Opět využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor t, jelikož uvažujeme první derivaci, volíme norder = 6. Budeme penalizovat čtvrtou derivaci funkcí, neboť nyní požadujeme hladké i druhé derivace. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # penalizujeme 4. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -4, to = -2, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) Obrázek 9.35: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 9.36: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Obrázek 9.37: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Obrázek 9.38: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 9.2.2 Výpočet derivací K výpočtu derivace pro funkcionální objekt využijeme v R funkci deriv.fd() z balíčku fda. Jelikož chceme klasifikovat na základě 2. derivace, volíme argument Lfdobj = 2. Code XXder &lt;- deriv.fd(XXfd, 2) Nyní si vykresleme prvních několik prvních derivací pro obě klasifikační třídy. Všimněme si z obrázku níže, že se opravdu vertikální posun pomocí derivování opravdu podařilo odstranit. Ztratili jsme tím ale do jisté míry rozdílnost mezi křivkami, protože jak z obrázku vyplývá, křivky derivací pro obě třídy se liší primárně až ke konci intervalu, tedy pro argument v rozmezí přibližně \\([5, 6]\\). Code fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code abs.labs &lt;- paste(&quot;Klasifikační třída:&quot;, c(&quot;$Y = 0$&quot;, &quot;$Y = 1$&quot;)) names(abs.labs) &lt;- c(&#39;0&#39;, &#39;1&#39;) # fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(eval.fd(fdobj = mean.fd(XXder[1:n]), evalarg = t), eval.fd(fdobj = mean.fd(XXder[(n + 1):(2 * n)]), evalarg = t)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, #group = interaction(time, group), colour = group)) + geom_line(aes(group = time), linewidth = 0.05, alpha = 0.5) + theme_bw() + labs(x = &quot;$t$&quot;, # y = &quot;$\\\\frac{\\\\text d}{\\\\text d t} x_i(t)$&quot;, y =&quot;$x_i&#39;(t)$&quot;, colour = &#39;Klasifikační\\n třída&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;1&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;tomato&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;0&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;deepskyblue2&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + geom_line(data = DFmean |&gt; mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))), aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 0.8, linetype = &#39;dashed&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + facet_wrap(~group, labeller = labeller(group = abs.labs)) + scale_y_continuous(expand = c(0.02, 0.02)) + theme(legend.position = &#39;none&#39;, plot.margin = unit(c(0.1, 0.1, 0.3, 0.5), &quot;cm&quot;)) + coord_cartesian(ylim = c(-20, 20)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Obrázek 9.39: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Code # ggsave(&quot;figures/kap6_sim_04_curves_2der.tex&quot;, device = tikz, width = 8, height = 4) Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;grey3&#39;, linewidth = 0.7, linetype = &#39;dashed&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-12, 10)) Obrázek 9.40: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 9.2.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 9.2.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) summary(neighb.model) # shrnuti modelu ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.9154930 0.7826087 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 65 6 ## 2 15 54 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.8071 0.8071 0.8000 0.8214 0.8071 0.8000 0.8143 0.8357 0.8286 0.8214 0.8357 ## 12 ## 0.8500 ## ## -Optimal number of neighbors: knn.opt= 12 ## with highest probability of correct classification max.prob= 0.85 ## ## -Probability of correct classification: 0.85 Code plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K Code neighb.model$max.prob # maximalni presnost ## [1] 0.85 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 12 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results ## [1] 0.8041265 0.7883817 0.7897586 0.7879866 0.8094504 0.8133647 0.8367856 ## [8] 0.7900529 0.8276922 0.8008575 0.8232979 0.8382659 0.8213426 0.8099140 ## [15] 0.8060450 0.8006226 0.8212531 0.8165782 0.7892888 0.7642423 0.7881684 ## [22] 0.7851269 0.7988126 0.7761255 Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 12 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8383. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 9.41: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) summary(neighb.model) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.9154930 0.7826087 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 65 6 ## 2 15 54 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 12 ## 0.85 ## ## -Optimal number of neighbors: knn.opt= 12 ## with highest probability of correct classification max.prob= 0.85 ## ## -Probability of correct classification: 0.85 Code # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost 1 - presnost ## [1] 0.15 Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 12, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.15 a na testovacích datech 0.15. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 9.2.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 3, které dohromady vysvětlují 90.88 % variability v datech. První hlavní komponenta potom vysvětluje 45.83 % a druhá 36.91 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 9.42: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 3 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (86.43 %), tak i na testovacích datech (81.67 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 9.43: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (85.71 %), tak i na testovacích datech (85 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 9.44: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 9.2.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze # basis1 &lt;- X.train$basis nbasis.x &lt;- 20 basis1 &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, nbasis = nbasis.x) Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 9.45: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 20, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, nbasis = nbasis.x) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) CV.results ## 4 5 6 7 8 9 10 11 ## 0.7101786 0.7122836 0.8241269 0.8909731 0.8767027 0.8636775 0.8689406 0.8791537 ## 12 13 14 15 16 17 18 19 ## 0.8720108 0.8811017 0.8863649 0.8935077 0.8760835 0.8526347 0.8687242 0.8492797 ## 20 21 22 23 24 25 26 27 ## 0.8467943 0.8467943 0.8480549 0.8592004 0.8539372 0.8520575 0.8539372 0.8498837 ## 28 29 30 ## 0.8578635 0.8587242 0.8765452 Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 15 s validační chybovostí 0.1065. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) Obrázek 9.46: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 15 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 5 %) i testovací chybovost (rovna 6.67 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Třída&#39;) Obrázek 9.47: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Obrázek 9.48: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 1]\\). Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy přesnost klasifikátoru na trénovacích (86.43 %) i na testovacích datech (81.67 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 9.49: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 9.2.3.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [0, 6]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost klasifikátoru na testovacích datech je tedy 85 % a na trénovacích datech 91.43 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.50: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 9.51: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 3 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na testovacích datech je tedy 71.67 % a na trénovacích datech 83.57 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.52: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 9.53: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 90 % a na testovacích datech 88.33 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.54: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 9.55: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 9.2.3.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 6]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost náhodného lesu na trénovacích datech je tedy 99.29 % a na testovacích datech 86.67 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 3 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 97.14 % a na testovacích datech 80 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost tohoto klasifikátoru na trénovacích datech je 99.29 % a na testovacích datech 90 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 5.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 5.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 5.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 5.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 5.3.7.5. 9.2.3.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 6]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm &lt;- XXder XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.04, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 3, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 12, gamma = 0.001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 92.14 % pro lineární jádro, 97.14 % pro polynomiální jádro a 92.14 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 93.33 % pro lineární jádro, 85 % pro polynomiální jádro a 91.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 3 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.01, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 25, gamma = 0.0001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 86.43 % pro lineární jádro, 87.14 % pro polynomiální jádro a 86.43 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 81.67 % pro lineární jádro, 81.67 % pro polynomiální jádro a 83.33 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 9.56: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 55, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.2, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 110, gamma = 0.001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 97.14 % pro lineární jádro, 95 % pro polynomiální jádro a 96.43 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 91.67 % pro lineární jádro, 90 % pro polynomiální jádro a 93.33 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), # matice diskretnich dat argvals = t.seq, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- t(Projection) # subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 15 0.07711081 ## poly 14 0.10463828 ## radial 15 0.10325549 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 15 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9229, 14 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8954 a 15 pro radiální jádro s hodnotou přesnosti 0.8967. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 9.57: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), # matice diskretnich dat argvals = t.seq, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 6.43 % pro lineární jádro, 3.57 % pro polynomiální jádro a 6.43 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 8.33 % pro lineární jádro, 8.33 % pro polynomiální jádro a 8.33 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 9.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 9.2 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 9.4 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 9.2.3.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 9.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{9.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (9.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{9.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 9.2.3.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku2 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 9.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 9.2.3.7.5.3 Implementace metody v R Z poslední části Tvrzení 9.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). 9.2.3.7.5.4 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.11: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0000 0.3833 SVM poly - RKHS 0.0000 0.2500 SVM rbf - RKHS 0.0143 0.2333 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 1, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 9.12: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 38 1.3895 0.0782 linear poly 38 1.3895 0.0978 polynomial radial 9 2.2758 0.0792 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 38 a \\(\\gamma={}\\) 1.3895 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9218, \\(d={}\\) 38 a \\(\\gamma={}\\) 1.3895 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9022 a \\(d={}\\) 9 a \\(\\gamma={}\\) 2.2758 pro radiální jádro s hodnotou přesnosti 0.9208. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 9.58: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.13: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.00 0.1833 SVM poly - RKHS - radial 0.00 0.1500 SVM rbf - RKHS - radial 0.05 0.0667 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 0 % pro lineární jádro, 0 % pro polynomiální jádro a 5 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 18.33 % pro lineární jádro, 15 % pro polynomiální jádro a 6.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7.5.5 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 9.14: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 20 2 0.0772 linear poly 39 3 0.0907 polynomial radial 33 3 0.1127 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 20 a \\(p={}\\) 2 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9228, \\(d={}\\) 39 a \\(p={}\\) 3 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9093 a \\(d={}\\) 33 a \\(p={}\\) 3 pro radiální jádro s hodnotou přesnosti 0.8873. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.15: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.0643 0.2333 SVM poly - RKHS - poly 0.0286 0.2000 SVM rbf - RKHS - poly 0.0571 0.1167 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 6.43 % pro lineární jádro, 2.86 % pro polynomiální jádro a 5.71 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 23.33 % pro lineární jádro, 20 % pro polynomiální jádro a 11.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7.5.6 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 9.16: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 17 0.1407 linear poly 27 0.1665 polynomial radial 16 0.1575 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 17 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8593, \\(d={}\\) 27 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8335 a \\(d={}\\) 16 pro radiální jádro s hodnotou přesnosti 0.8425. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.17: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.0929 0.2000 SVM poly - RKHS - linear 0.0500 0.1667 SVM rbf - RKHS - linear 0.0929 0.1833 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 9.29 % pro lineární jádro, 5 % pro polynomiální jádro a 9.29 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 20 % pro lineární jádro, 16.67 % pro polynomiální jádro a 18.33 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.8 Tabulka výsledků Tabulka 9.18: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.1500 0.1500 LDA 0.1357 0.1833 QDA 0.1429 0.1500 LR functional 0.0500 0.0667 LR score 0.1357 0.1833 Tree - diskr. 0.0857 0.1500 Tree - score 0.1643 0.2833 Tree - Bbasis 0.1000 0.1167 RForest - diskr 0.0071 0.1333 RForest - score 0.0286 0.2000 RForest - Bbasis 0.0071 0.1000 SVM linear - diskr 0.0786 0.0667 SVM poly - diskr 0.0286 0.1500 SVM rbf - diskr 0.0786 0.0833 SVM linear - PCA 0.1357 0.1833 SVM poly - PCA 0.1286 0.1833 SVM rbf - PCA 0.1357 0.1667 SVM linear - Bbasis 0.0286 0.0833 SVM poly - Bbasis 0.0500 0.1000 SVM rbf - Bbasis 0.0357 0.0667 SVM linear - projection 0.0643 0.0833 SVM poly - projection 0.0357 0.0833 SVM rbf - projection 0.0643 0.0833 SVM linear - RKHS - radial 0.0000 0.1833 SVM poly - RKHS - radial 0.0000 0.1500 SVM rbf - RKHS - radial 0.0500 0.0667 SVM linear - RKHS - poly 0.0643 0.2333 SVM poly - RKHS - poly 0.0286 0.2000 SVM rbf - RKHS - poly 0.0571 0.1167 SVM linear - RKHS - linear 0.0929 0.2000 SVM poly - RKHS - linear 0.0500 0.1667 SVM rbf - RKHS - linear 0.0929 0.1833 9.2.4 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -4.5, to = -1.5, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd # vypocet derivace XXder &lt;- deriv.fd(XXfd, 2) fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- 1:20 #c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze # basis1 &lt;- X.train$basis nbasis.x &lt;- 20 rangeval &lt;- range(tt) norder &lt;- 6 basis1 &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, nbasis = nbasis.x) ### 10-fold cross-validation n.basis.max &lt;- 15 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] # B-spline baze # basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # normovani dat norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm &lt;- XXder XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.04, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 3, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 12, gamma = 0.001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.01, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 25, gamma = 0.0001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 55, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.2, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 110, gamma = 0.001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 # length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- t(Projection) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 3, length = 6) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_04_2der_cv.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/simulace_04_res_2der_cv.RData&#39;) 9.2.4.1 Výsledky Tabulka 9.19: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.1775 0.1895 0.0539 0.0618 LDA 0.1589 0.1610 0.0688 0.0736 QDA 0.1541 0.1665 0.0677 0.0756 LR_functional 0.0518 0.0985 0.0272 0.0431 LR_score 0.1594 0.1640 0.0681 0.0722 Tree_discr 0.1060 0.1443 0.0446 0.0731 Tree_score 0.1690 0.2282 0.0583 0.0812 Tree_Bbasis 0.1031 0.1453 0.0461 0.0765 RF_discr 0.0105 0.1315 0.0077 0.0646 RF_score 0.0311 0.1902 0.0193 0.0867 RF_Bbasis 0.0094 0.1303 0.0071 0.0653 SVM linear - diskr 0.1031 0.1215 0.0473 0.0617 SVM poly - diskr 0.0311 0.1612 0.0249 0.0675 SVM rbf - diskr 0.0962 0.1268 0.0499 0.0666 SVM linear - PCA 0.1656 0.1772 0.0732 0.0918 SVM poly - PCA 0.1696 0.1923 0.0737 0.0941 SVM rbf - PCA 0.2364 0.2755 0.1167 0.1536 SVM linear - Bbasis 0.0396 0.0932 0.0231 0.0386 SVM poly - Bbasis 0.0764 0.1278 0.0437 0.0676 SVM rbf - Bbasis 0.0586 0.1070 0.0309 0.0523 SVM linear - projection 0.0815 0.1062 0.0259 0.0413 SVM poly - projection 0.0547 0.1463 0.0325 0.0598 SVM rbf - projection 0.0821 0.1397 0.0481 0.0656 SVM linear - RKHS - radial 0.0729 0.1227 0.0312 0.0544 SVM poly - RKHS - radial 0.0424 0.1465 0.0297 0.0572 SVM rbf - RKHS - radial 0.0676 0.1428 0.0362 0.0642 SVM linear - RKHS - poly 0.0830 0.1585 0.0388 0.0635 SVM poly - RKHS - poly 0.0465 0.1660 0.0268 0.0650 SVM rbf - RKHS - poly 0.0864 0.1552 0.0335 0.0637 SVM linear - RKHS - linear 0.1194 0.2052 0.0563 0.0636 SVM poly - RKHS - linear 0.0755 0.2047 0.0468 0.0644 SVM rbf - RKHS - linear 0.1177 0.1972 0.0526 0.0655 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 9.59: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = &quot;$\\\\widehat{\\\\textnormal{Err}}_{test}$&quot; # y = expression(widehat(Err)[test]) ) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 50, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 3, color = &quot;black&quot;, alpha = 0.9) + # scale_x_discrete(labels = methods_names) + # theme(plot.margin = unit(c(0.5, 0.5, 2, 2), &quot;cm&quot;)) + # scale_fill_manual(values = box_col) + # scale_alpha_manual(values = box_alpha) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;gray20&#39;, alpha = 0.8) Obrázek 9.60: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # ggsave(&quot;figures/kap6_sim_04_boxplot_test_2der.tex&quot;, device = tikz, width = 9, height = 7) Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 9.20: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 8.5 nharm 3.0 LR_func_n_basis 9.0 SVM_d_Linear 10.0 SVM_d_Poly 10.0 SVM_d_Radial 12.0 SVM_RKHS_radial_gamma1 10.0 SVM_RKHS_radial_gamma2 10.0 SVM_RKHS_radial_gamma3 10.0 SVM_RKHS_radial_d1 17.5 SVM_RKHS_radial_d2 20.0 SVM_RKHS_radial_d3 15.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 4.0 SVM_RKHS_poly_p3 4.0 SVM_RKHS_poly_d1 22.5 SVM_RKHS_poly_d2 30.0 SVM_RKHS_poly_d3 30.0 SVM_RKHS_linear_d1 20.0 SVM_RKHS_linear_d2 25.0 SVM_RKHS_linear_d3 25.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.61: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.62: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.63: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.64: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.65: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
