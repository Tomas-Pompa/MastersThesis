[["index.html", "Diplomová práce SVM pro funkcionální data", " Diplomová práce Tomáš Pompa 10. 04. 2024 SVM pro funkcionální data Cílem bude aplikovat poznatky o metodě podpůrných vektorů (SVM) pro mnohorozměrná data na data funkcionálního typu, tedy nekonečně-rozměrné objekty. K tomu využijeme jednak převod (redukci) objektů z nekonečné dimenze na objekty konečné dimenze a následným využitím známých postupů a také modifikaci SVM přímo pro funkcionální data, k čemuž využijeme poznatky o Hilbertových prostorech a skalárním součinu. Dalším cílem bude porovnání jednotlivých metod pro klasifikaci funkcionálních dat na reálných a simulovaných datech. Bylo by dobré vymyslet nějakou zajímavou simulační studii, která bude demonstrovat různá chování uvažovaných metod. Mezi uvažované klasifikační metody patří: \\(K\\) nejbližších sousedů (KNN), logistická regrese (jak obyčejná (LR) tak její funkcionální modifikace (LR_fda)), lineární (LDA) a kvadratická (QDA) diskriminační analýza, rozhodovací stromy (DT), náhodné lesy (RF) a Support Vector Machines. Postupně jednotlivé metody projdeme, nejprve na simulovaných datech, a následně budeme konstruovat metodu podpůrných vektorů pro funkcionální data (SVM_fda). Základním balíčkem v R pro práci s funkcionálními objekty je fda. Dalšími užitečnými balíčky budou MASS, e1071, fda.usc, refund a další. "],["simulace1.html", "Kapitola 1 Simulace 1 1.1 Simulace funkcionálních dat 1.2 Vyhlazení pozorovaných křivek 1.3 Klasifikace křivek 1.4 Tabulka výsledků 1.5 Simulační studie", " Kapitola 1 Simulace 1 1.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro simulaci nejprve: zvolíme vhodné funkce, generujeme body ze zvoleného intervalu, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 1]\\). Pro třídu \\(Y = 0\\) a \\(Y = 1\\) uvažujme funkce \\[ f_0(x) = 2 \\cdot \\sin\\left(\\frac{\\pi x}{2}\\right) + x^2 + \\frac{1}{2}, \\] \\[ f_1(x) = 3 \\cdot \\sin\\left(\\frac{\\pi x}{2}\\right) + \\frac{1}{2} x. \\] Jejich grafy jsou na obrázcích níže. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- function(x) {return(2 * sin(x*pi/2) + x^2 + 0.5)} # pridat nahodny posun v zacatku nebo periode funkce_1 &lt;- function(x) {return(3 * sin(x*pi/2) + 0.5 * x)} Code x &lt;- seq(0, 1, length = 501) y0 &lt;- funkce_0(x) y1 &lt;- funkce_1(x) df &lt;- data.frame(x = rep(x, 2), y = c(y0, y1), Y = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), each = length(x))) df |&gt; ggplot(aes(x = x, y = y, colour = Y)) + geom_line(linewidth = 1) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 1.1: Znázornění dvou funkcí na intervalu \\([0, 1]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet vygenerovaných funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce. Tento posun budeme generovat s exponenciálního rozdělění s parametrem \\(\\lambda_{\\text{exp}}\\). Code generate_values &lt;- function(t, fun, n, sigma, lambda_exp = Inf) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # lambda_exp ... parameter of exponential distribution # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rexp(n, rate = lambda_exp), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 1] t &lt;- seq(0, 1, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 0.6) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 0.6) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 1.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 1.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nejedná o periodické křivky na intervalu \\(I = [0, 1]\\), využijeme k vyhlazení vhodnější B-sline bázi. Za uzly bereme celý vektor t, standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross-validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;dashed&#39;, linewidth = 0.8) + geom_point(size = 2.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 1.3: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda =\\) 0.1 nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 1.4: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;black&#39;, linewidth = 1, linetype = &#39;twodash&#39;) Obrázek 1.5: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. 1.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 66 74 Code table(Y.test) ## Y.test ## 0 1 ## 34 26 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.4714286 0.5285714 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.5666667 0.4333333 1.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) summary(neighb.model) # shrnuti modelu ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.6666667 0.7972973 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 44 22 ## 2 15 59 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.6857 0.6286 0.7214 0.6714 0.6786 0.6643 0.6714 0.6500 0.7357 0.7214 0.7071 ## 12 ## 0.6500 ## ## -Optimal number of neighbors: knn.opt= 9 ## with highest probability of correct classification max.prob= 0.7357143 ## ## -Probability of correct classification: 0.7357 Code plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K Code neighb.model$max.prob # maximalni presnost ## [1] 0.7357143 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 9 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; t() ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## K 1.0000000 2.0000000 3.0000000 4.0000000 5.0000000 6.0000000 7.0000000 ## CV 0.6975487 0.6598864 0.7335065 0.6862662 0.6552435 0.6464773 0.6758766 ## [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## K 8.0000000 9.0000000 10.0000000 11.0000000 12.0000000 13.0000000 14.0000000 ## CV 0.6648864 0.6973864 0.6935227 0.6576461 0.6088149 0.6362987 0.6478896 ## [,15] [,16] [,17] [,18] [,19] [,20] [,21] ## K 15.0000000 16.0000000 17.0000000 18.0000000 19.0000000 20.0000000 21.0000000 ## CV 0.6191721 0.5973864 0.5924838 0.6066721 0.5831818 0.5946104 0.6023701 ## [,22] [,23] [,24] ## K 22.0000000 23.0000000 24.0000000 ## CV 0.5873864 0.6330032 0.5864286 Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 3 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.7335. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 1.6: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) summary(neighb.model) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.6969697 0.7432432 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 46 20 ## 2 19 55 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 3 ## 0.7214 ## ## -Optimal number of neighbors: knn.opt= 3 ## with highest probability of correct classification max.prob= 0.7214286 ## ## -Probability of correct classification: 0.7214 Code # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost 1 - presnost ## [1] 0.2833333 Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 3, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.2786 a na testovacích datech 0.2833. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 1.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Zároveň kvůli grafickému zakreslení přidáme podmínku, že za počet komponent \\(p\\) volíme alespoň 2. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 99.76 % variability v datech. První hlavní komponenta potom vysvětluje 98.94 % a druhá 0.82 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 1.7: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (78.57 %), tak i na testovacích datech (78.33 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.8: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (77.86 %), tak i na testovacích datech (78.33 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.9: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 1.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. Ten bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřednostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 1.10: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 6, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chybovost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 7 s validační chybovostí 0.2075. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) Obrázek 1.11: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 7 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 18.57 %) i testovací chybovost (rovna 20 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Klasifikační\\n třída&#39;) Obrázek 1.12: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 1, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Obrázek 1.13: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 1]\\). Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro časy \\(t\\) z prostředka intervalu \\([0, 1]\\), zatímco pro krajní časy jsou hodnoty vyšší. To ukazuje na rozdílnost funkcí z klasifikačních tříd na začátku a konci intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy přesnost klasifikátoru na trénovacích (79.29 %) i na testovacích datech (76.67 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.14: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 1.3.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [0, 1]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 1, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost klasifikátoru na testovacích datech je tedy 66.67 % a na trénovacích datech 72.86 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.15: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.16: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 2 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na testovacích datech je tedy 75 % a na trénovacích datech 78.57 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.17: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.18: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 72.14 % a na testovacích datech 73.33 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.19: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.20: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 1.3.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 1]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost náhodného lesu na trénovacích datech je tedy 97.86 % a na testovacích datech 73.33 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních p = 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 92.14 % a na testovacích datech 73.33 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost tohoto klasifikátoru na trénovacích datech je 97.86 % a na testovacích datech 73.33 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 1.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 1.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 1.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 1.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 1.3.7.5. 1.3.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 1]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 80 % pro lineární jádro, 57.14 % pro polynomiální jádro a 74.29 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 86.67 % pro lineární jádro, 48.33 % pro polynomiální jádro a 75 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 77.14 % pro lineární jádro, 71.43 % pro polynomiální jádro a 77.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 76.67 % pro lineární jádro, 73.33 % pro polynomiální jádro a 75 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 1.21: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 80.71 % pro lineární jádro, 57.14 % pro polynomiální jádro a 75 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 85 % pro lineární jádro, 50 % pro polynomiální jádro a 75 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 7 0.1853480 ## poly 4 0.3515018 ## radial 7 0.2773993 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 7 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8147, 4 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.6485 a 7 pro radiální jádro s hodnotou přesnosti 0.7226. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &quot;Jádro&quot;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 1.22: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 17.86 % pro lineární jádro, 32.86 % pro polynomiální jádro a 18.57 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 25 % pro lineární jádro, 43.33 % pro polynomiální jádro a 40 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 1.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše uvedená v Lemmatu 1.1 se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 1.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 1.3.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 1.3.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku1 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 1.3.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). 1.3.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;)^2)) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.2143 0.5000 SVM poly - RKHS 0.2500 0.4833 SVM rbf - RKHS 0.2000 0.4167 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 1, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 1.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 27 10.0000 0.2936 linear poly 36 0.1931 0.2974 polynomial radial 10 0.0439 0.2874 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 27 a \\(\\gamma={}\\) 10 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.7064, \\(d={}\\) 36 a \\(\\gamma={}\\) 0.1931 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.7026 a \\(d={}\\) 10 a \\(\\gamma={}\\) 0.0439 pro radiální jádro s hodnotou přesnosti 0.7126. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) ## Warning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Spectral is 11 ## Returning the palette you asked for with that many colors Obrázek 1.23: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.2286 0.3167 SVM poly - RKHS - radial 0.1714 0.4333 SVM rbf - RKHS - radial 0.2571 0.4000 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 22.86 % pro lineární jádro, 17.14 % pro polynomiální jádro a 25.71 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 31.67 % pro lineární jádro, 43.33 % pro polynomiální jádro a 40 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) d p CV Kernel linear 32 5 0.2770 linear poly 27 3 0.3343 polynomial radial 19 5 0.2693 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 32 a \\(p={}\\) 5 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.723, \\(d={}\\) 27 a \\(p={}\\) 3 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.6657 a \\(d={}\\) 19 a \\(p={}\\) 5 pro radiální jádro s hodnotou přesnosti 0.7307. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.4: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.2571 0.3833 SVM poly - RKHS - poly 0.3143 0.4667 SVM rbf - RKHS - poly 0.2643 0.3667 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 25.71 % pro lineární jádro, 31.43 % pro polynomiální jádro a 26.43 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 38.33 % pro lineární jádro, 46.67 % pro polynomiální jádro a 36.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 1.5: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 22 0.3048 linear poly 27 0.3102 polynomial radial 8 0.3071 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 22 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.6952, \\(d={}\\) 27 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.6898 a \\(d={}\\) 8 pro radiální jádro s hodnotou přesnosti 0.6929. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.6: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.2786 0.3667 SVM poly - RKHS - linear 0.2357 0.2500 SVM rbf - RKHS - linear 0.2929 0.3333 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 27.86 % pro lineární jádro, 23.57 % pro polynomiální jádro a 29.29 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 36.67 % pro lineární jádro, 25 % pro polynomiální jádro a 33.33 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 1.4 Tabulka výsledků Tabulka 1.7: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.2786 0.2833 LDA 0.2143 0.2167 QDA 0.2214 0.2167 LR functional 0.1857 0.2000 LR score 0.2071 0.2333 Tree - diskr. 0.2714 0.3333 Tree - score 0.2143 0.2500 Tree - Bbasis 0.2786 0.2667 RForest - diskr 0.0214 0.2667 RForest - score 0.0786 0.2667 RForest - Bbasis 0.0214 0.2667 SVM linear - diskr 0.2000 0.1333 SVM poly - diskr 0.4286 0.5167 SVM rbf - diskr 0.2571 0.2500 SVM linear - PCA 0.2286 0.2333 SVM poly - PCA 0.2857 0.2667 SVM rbf - PCA 0.2214 0.2500 SVM linear - Bbasis 0.1929 0.1500 SVM poly - Bbasis 0.4286 0.5000 SVM rbf - Bbasis 0.2500 0.2500 SVM linear - projection 0.1786 0.2500 SVM poly - projection 0.3286 0.4333 SVM rbf - projection 0.1857 0.4000 SVM linear - RKHS - radial 0.2286 0.3167 SVM poly - RKHS - radial 0.1714 0.4333 SVM rbf - RKHS - radial 0.2571 0.4000 SVM linear - RKHS - poly 0.2571 0.3833 SVM poly - RKHS - poly 0.3143 0.4667 SVM rbf - RKHS - poly 0.2643 0.3667 SVM linear - RKHS - linear 0.2786 0.3667 SVM poly - RKHS - linear 0.2357 0.2500 SVM rbf - RKHS - linear 0.2929 0.3333 1.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). Code # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 1] t &lt;- seq(0, 1, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 0.6) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 0.6) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 3, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK CV_RESULTS$nharm[sim] &lt;- nharm nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 1, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by =2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 1, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_01.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/simulace_01_res.RData&#39;) 1.5.1 Výsledky Tabulka 1.8: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.3217 0.3640 0.0512 0.0625 LDA 0.4623 0.5293 0.0304 0.0638 QDA 0.4659 0.5213 0.0250 0.0607 LR_functional 0.1974 0.2622 0.0450 0.0650 LR_score 0.4641 0.5218 0.0325 0.0667 Tree_discr 0.2868 0.4157 0.0645 0.0759 Tree_score 0.4083 0.5245 0.0641 0.0576 Tree_Bbasis 0.2900 0.4178 0.0674 0.0796 RF_discr 0.0326 0.3362 0.0137 0.0607 RF_score 0.1366 0.5112 0.0202 0.0643 RF_Bbasis 0.0330 0.3352 0.0139 0.0592 SVM linear - diskr 0.2200 0.2422 0.0370 0.0556 SVM poly - diskr 0.3946 0.4553 0.0592 0.0969 SVM rbf - diskr 0.2789 0.3540 0.0431 0.0811 SVM linear - PCA 0.4691 0.5405 0.0225 0.0475 SVM poly - PCA 0.4731 0.5435 0.0182 0.0388 SVM rbf - PCA 0.4326 0.5138 0.0329 0.0725 SVM linear - Bbasis 0.2216 0.2425 0.0392 0.0547 SVM poly - Bbasis 0.3909 0.4507 0.0569 0.0959 SVM rbf - Bbasis 0.2741 0.3483 0.0436 0.0767 SVM linear - projection 0.1869 0.2695 0.0427 0.0638 SVM poly - projection 0.2683 0.3843 0.0642 0.0997 SVM rbf - projection 0.1836 0.3317 0.0317 0.0870 SVM linear - RKHS - radial 0.2669 0.3583 0.0451 0.0626 SVM poly - RKHS - radial 0.2580 0.3805 0.0459 0.0782 SVM rbf - RKHS - radial 0.2471 0.3573 0.0467 0.0688 SVM linear - RKHS - poly 0.2989 0.3622 0.0429 0.0757 SVM poly - RKHS - poly 0.3097 0.3773 0.0405 0.0759 SVM rbf - RKHS - poly 0.3055 0.3537 0.0382 0.0765 SVM linear - RKHS - linear 0.3024 0.3645 0.0512 0.0756 SVM poly - RKHS - linear 0.2829 0.3822 0.0550 0.0795 SVM rbf - RKHS - linear 0.2936 0.3620 0.0478 0.0683 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 1.24: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[test])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 1.25: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 1.9: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 6.0 nharm 1.0 LR_func_n_basis 9.0 SVM_d_Linear 9.5 SVM_d_Poly 5.0 SVM_d_Radial 6.0 SVM_RKHS_radial_gamma1 0.2 SVM_RKHS_radial_gamma2 0.2 SVM_RKHS_radial_gamma3 0.2 SVM_RKHS_radial_d1 18.0 SVM_RKHS_radial_d2 17.0 SVM_RKHS_radial_d3 17.0 SVM_RKHS_poly_p1 5.0 SVM_RKHS_poly_p2 3.0 SVM_RKHS_poly_p3 5.0 SVM_RKHS_poly_d1 11.0 SVM_RKHS_poly_d2 15.0 SVM_RKHS_poly_d3 9.0 SVM_RKHS_linear_d1 9.0 SVM_RKHS_linear_d2 9.0 SVM_RKHS_linear_d3 8.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.26: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.27: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.28: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.29: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.30: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["simulace2.html", "Kapitola 2 Simulace 2 2.1 Simulace funkcionálních dat 2.2 Vyhlazení pozorovaných křivek 2.3 Klasifikace křivek 2.4 Tabulka výsledků 2.5 Simulační studie", " Kapitola 2 Simulace 2 2.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro tuto simulaci zvolíme následující postup: zvolíme vhodné body (pro každou klasifikační třídu jiné), které proložíme interpolačním splajnem, takto získané funkce využijeme ke generování náhodných křivek pro obě třídy, generujeme body ze zvoleného intervalu pomocí vyhlazených funkcí interpolačním splajnem, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(stats) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 12]\\). V tomto případě si definujeme v prvním kroku body, kterými má náš interpolační splajn procházet. Následně proložíme těmito body interpolační splajn, k čemuž využijeme funkci spline() z knihovny stats. Pro lepší interpretovatelnost simulovaných dat jsme definovali interval \\(I = [0, 12]\\) a funkce pro obě klasifikační třídy zvolíme tak, aby při troše fantazie mohli představovat například vývoj určité veličiny (teplota, tlak, srážky, nezaměstnanost, prodej nějaké komodity atd.) v průběhu roku. Budeme předpokládat, že se vývoj této veličiny periodicky opakuje v čase (s roční periodou), proto ve funkci spline() zvolíme parametr method = 'periodic'. Code # definujici body pro tridu 0 x.0 &lt;- c(0, 2, 2.96, 3.64, 4.2, 5.3, 6.22, 7.32, 8.3, 9.3, 10.6, 11.54, 12) y.0 &lt;- c(3, 4.66, 4.6, 3.48, 2.08, 1.5, 1.32, 2.78, 3.82, 4.48, 3.88, 3.16, 3) # definujici body pro tridu 1 x.1 &lt;- c(0, 1.3, 2, 2.7, 3.33, 3.68, 4.06, 4.72, 5.9, 6.9, 8, 9.02, 10.1, 11.25, 12) y.1 &lt;- c(2.8, 3.95, 4.82, 5.1, 4.8, 4, 2.73, 1.67, 1.53, 2.2, 3, 3.73, 3.69, 3.6, 2.8) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Jejich grafy jsou na obrázcích níže. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- function(n) { sp &lt;- spline(x.0, y.0, method = &#39;periodic&#39;, n = n) return(list(x = sp$x, y = sp$y)) } # pridat nahodny posun v zacatku nebo periode funkce_1 &lt;- function(n) { sp &lt;- spline(x.1, y.1, method = &#39;periodic&#39;, n = n) return(list(x = sp$x, y = sp$y)) } Code n_x &lt;- 501 x &lt;- funkce_0(n_x)$x y0 &lt;- funkce_0(n_x)$y y1 &lt;- funkce_1(n_x)$y df &lt;- data.frame(x = rep(x, 2), y = c(y0, y1), Y = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), each = length(x))) df |&gt; ggplot(aes(x = x, y = y, colour = Y)) + geom_line(linewidth = 1) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 1.2: Znázornění dvou funkcí na intervalu \\([0, 12]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce. Tento posun budeme generovat s normálního rozdělění s parametrem \\(\\sigma^2 = 3.75^2\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(fun(length(t))$y, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(X + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 12] t &lt;- seq(0, 12, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 3.75) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 3.75) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 1.3: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 2.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nyní jedná o periodické křivky na intervalu \\(I = [0, 12]\\), využijeme k vyhlazení fourierovu bázi. Za uzly bereme celý vektor t, budeme uvažovat tzv. harmonic acceleration penalties. Harmonic acceleration pro funkci \\(x(t)\\) je \\[ Lx = \\omega^2 Dx + D^3x, \\] kde \\(D^mx\\) značí \\(m\\)-tou derivaci funkce \\(x(t)\\) podle \\(t\\). Platí přitom \\(L\\sin(\\omega x) = 0 = L\\cos(\\omega x)\\). Potom jako penalizaci bereme hodnotu funkcionálu \\[ J_L(x) = \\int \\left[Lx(t)\\right]^2\\text dt. \\] Code rangeval &lt;- range(t) fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = length(t)) omega &lt;- 2 * pi / diff(rangeval) acvec &lt;- c(0, omega^2, 0) harmLfd &lt;- vec2Lfd(bwtvec = acvec, rangeval = rangeval) Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(fbasis, harmLfd, lambda.vect[index]) FSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(FSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;dashed&#39;, linewidth = 0.8) + geom_point(size = 2.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 2.1: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(fbasis, harmLfd, lambda.opt) FSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- FSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.2: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;black&#39;, linewidth = 1, linetype = &#39;twodash&#39;) Obrázek 2.3: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. 2.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 2.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) summary(neighb.model) # shrnuti modelu ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.7042254 0.7536232 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 50 21 ## 2 17 52 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.7286 0.6857 0.6929 0.6714 0.6714 0.6643 0.6429 0.6357 0.6571 0.6357 0.6357 ## 12 ## 0.5714 ## ## -Optimal number of neighbors: knn.opt= 1 ## with highest probability of correct classification max.prob= 0.7285714 ## ## -Probability of correct classification: 0.7286 Code plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K Code neighb.model$max.prob # maximalni presnost ## [1] 0.7285714 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 1 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results ## [1] 0.7187077 0.7055862 0.7421472 0.7036110 0.6815970 0.6948198 0.6662214 ## [8] 0.6688805 0.6825357 0.6620869 0.6573983 0.6029223 0.6140367 0.6280504 ## [15] 0.6012092 0.6057577 0.5720150 0.6038152 0.6191998 0.6205984 0.5961229 ## [22] 0.6115075 0.6033823 0.5972433 Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 3 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.7421. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 2.4: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) summary(neighb.model) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.7323944 0.6521739 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 52 19 ## 2 24 45 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 3 ## 0.6929 ## ## -Optimal number of neighbors: knn.opt= 3 ## with highest probability of correct classification max.prob= 0.6928571 ## ## -Probability of correct classification: 0.6929 Code # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost 1 - presnost ## [1] 0.3333333 Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 3, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.3071 a na testovacích datech 0.3333. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 2.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 99.52 % variability v datech. První hlavní komponenta potom vysvětluje 99.16 % a druhá 0.36 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 1.8: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (88.57 %), tak i na testovacích datech (85 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 2.5: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (87.14 %), tak i na testovacích datech (85 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 2.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. V tomto případě však funkce fregre.glm() hlásí chybu, že systém je výpočetně singulární, proto nyní uvažujme pouze druhou z možných implementací logistické regrese pro funkcionální data. 2.3.4.1 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy přesnost klasifikátoru na trénovacích (89.29 %) i na testovacích datech (83.33 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.12: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 2.3.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [0, 12]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 12, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost klasifikátoru na testovacích datech je tedy 46.67 % a na trénovacích datech 67.14 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 2.7: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 2.8: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 2 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na testovacích datech je tedy 83.33 % a na trénovacích datech 90 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.14: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 2.9: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí ve Fourierově bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 87.14 % a na testovacích datech 85 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.16: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 2.10: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Fbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 2.3.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 12]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost náhodného lesu na trénovacích datech je tedy 98.57 % a na testovacích datech 71.67 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 96.43 % a na testovacích datech 83.33 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí Fourierovské báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost tohoto klasifikátoru na trénovacích datech je 100 % a na testovacích datech 85 %. Code Res &lt;- data.frame(model = &#39;RForest - Fbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 2.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 2.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí Fourierovy báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 2.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými nebo Fourierovými, funkcemi (sekce 2.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 2.3.7.5. 2.3.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 12]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 91.43 % pro lineární jádro, 70 % pro polynomiální jádro a 68.57 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 88.33 % pro lineární jádro, 75 % pro polynomiální jádro a 51.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 2.3.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 90 % pro lineární jádro, 87.86 % pro polynomiální jádro a 87.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 81.67 % pro lineární jádro, 83.33 % pro polynomiální jádro a 81.67 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 2.11: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 2.3.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí Fourierovy báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 100 % pro lineární jádro, 100 % pro polynomiální jádro a 100 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 86.67 % pro lineární jádro, 75 % pro polynomiální jádro a 73.33 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 2.3.7.4 Projekce na Fourierovu bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na Fourierovu bázi, protože využití B-splinové báze není pro naše periodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Code k_cv &lt;- 10 # k-fold CV # hodnoty pro Fourierovu bazi rangeval &lt;- range(t) n_basis_min &lt;- 3 n_basis_max &lt;- 40 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na Fourierovu bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = fbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 34 0.09031136 ## poly 4 0.16588828 ## radial 18 0.12593407 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 34 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9097, 4 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8341 a 18 pro radiální jádro s hodnotou přesnosti 0.8741. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 2.12: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na Fourierovu bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0 % pro lineární jádro, 13.57 % pro polynomiální jádro a 2.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 18.33 % pro lineární jádro, 23.33 % pro polynomiální jádro a 15 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 2.3.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 1.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše uvedená v Lemmatu 1.1 se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 2.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 2.3.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 2.3.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku2 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 2.3.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). 2.3.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 2.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0571 0.2667 SVM poly - RKHS 0.0571 0.3000 SVM rbf - RKHS 0.0857 0.3000 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 1, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 2.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 29 0.5179 0.1219 linear poly 6 10.0000 0.1626 polynomial radial 8 0.8483 0.1276 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 29 a \\(\\gamma={}\\) 0.5179 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8781, \\(d={}\\) 6 a \\(\\gamma={}\\) 10 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8374 a \\(d={}\\) 8 a \\(\\gamma={}\\) 0.8483 pro radiální jádro s hodnotou přesnosti 0.8724. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 2.13: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 2.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0571 0.1833 SVM poly - RKHS - radial 0.1000 0.1000 SVM rbf - RKHS - radial 0.1000 0.1167 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 5.71 % pro lineární jádro, 10 % pro polynomiální jádro a 10 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 18.33 % pro lineární jádro, 10 % pro polynomiální jádro a 11.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 2.3.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 2.4: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 13 3 0.1133 linear poly 3 4 0.1283 polynomial radial 8 3 0.1216 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 13 a \\(p={}\\) 3 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8867, \\(d={}\\) 3 a \\(p={}\\) 4 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8717 a \\(d={}\\) 8 a \\(p={}\\) 3 pro radiální jádro s hodnotou přesnosti 0.8784. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.0929 0.2833 SVM poly - RKHS - poly 0.1429 0.1500 SVM rbf - RKHS - poly 0.1000 0.2167 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 9.29 % pro lineární jádro, 14.29 % pro polynomiální jádro a 10 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 28.33 % pro lineární jádro, 15 % pro polynomiální jádro a 21.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 2.3.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 2.5: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 9 0.1505 linear poly 17 0.1642 polynomial radial 31 0.1643 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 9 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8495, \\(d={}\\) 17 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8358 a \\(d={}\\) 31 pro radiální jádro s hodnotou přesnosti 0.8357. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.4: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.1286 0.2333 SVM poly - RKHS - linear 0.0929 0.2167 SVM rbf - RKHS - linear 0.1071 0.2333 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 12.86 % pro lineární jádro, 9.29 % pro polynomiální jádro a 10.71 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 23.33 % pro lineární jádro, 21.67 % pro polynomiální jádro a 23.33 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 2.4 Tabulka výsledků Tabulka 2.6: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3071 0.3333 LDA 0.1143 0.1500 QDA 0.1286 0.1500 LR score 0.1071 0.1667 Tree - diskr. 0.3286 0.5333 Tree - score 0.1000 0.1667 Tree - Fbasis 0.1286 0.1500 RForest - diskr 0.0143 0.2833 RForest - score 0.0357 0.1667 RForest - Fbasis 0.0000 0.1500 SVM linear - diskr 0.0857 0.1167 SVM poly - diskr 0.3000 0.2500 SVM rbf - diskr 0.3143 0.4833 SVM linear - PCA 0.1000 0.1833 SVM poly - PCA 0.1214 0.1667 SVM rbf - PCA 0.1214 0.1833 SVM linear - Fbasis 0.0000 0.1333 SVM poly - Fbasis 0.0000 0.2500 SVM rbf - Fbasis 0.0000 0.2667 SVM linear - projection 0.0000 0.1833 SVM poly - projection 0.1357 0.2333 SVM rbf - projection 0.0286 0.1500 SVM linear - RKHS - radial 0.0571 0.1833 SVM poly - RKHS - radial 0.1000 0.1000 SVM rbf - RKHS - radial 0.1000 0.1167 SVM linear - RKHS - poly 0.0929 0.2833 SVM poly - RKHS - poly 0.1429 0.1500 SVM rbf - RKHS - poly 0.1000 0.2167 SVM linear - RKHS - linear 0.1286 0.2333 SVM poly - RKHS - linear 0.0929 0.2167 SVM rbf - RKHS - linear 0.1071 0.2333 2.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Fbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Fbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část n.sim-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na Fourierovu bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 12] t &lt;- seq(0, 12, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 3.75) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 3.75) rangeval &lt;- range(t) fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = length(t)) omega &lt;- 2 * pi / diff(rangeval) acvec &lt;- c(0, omega^2, 0) harmLfd &lt;- vec2Lfd(bwtvec = acvec, rangeval = rangeval) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(fbasis, harmLfd, lambda.vect[index]) FSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(FSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(fbasis, harmLfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 12, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Fbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Fbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na Fourierovu bázi # hodnoty pro Fourierovu bazi rangeval &lt;- range(t) n_basis_min &lt;- 3 n_basis_max &lt;- 40 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = fbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = fbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by =2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_02.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/simulace_02_res.RData&#39;) 2.5.1 Výsledky Tabulka 2.7: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.2639 0.3015 0.0478 0.0642 LDA 0.1161 0.1222 0.0288 0.0443 QDA 0.1164 0.1218 0.0290 0.0454 LR_score 0.1154 0.1252 0.0279 0.0475 Tree_discr 0.2890 0.4555 0.0757 0.0703 Tree_score 0.1004 0.1315 0.0246 0.0442 Tree_Fbasis 0.1129 0.1557 0.0282 0.0492 RF_discr 0.0166 0.2907 0.0103 0.0692 RF_score 0.0412 0.1347 0.0153 0.0418 RF_Fbasis 0.0000 0.1145 0.0000 0.0427 SVM linear - diskr 0.0894 0.1107 0.0251 0.0463 SVM poly - diskr 0.3183 0.3732 0.0595 0.0994 SVM rbf - diskr 0.3233 0.4212 0.0520 0.0846 SVM linear - PCA 0.1157 0.1257 0.0283 0.0477 SVM poly - PCA 0.1369 0.1623 0.0332 0.0621 SVM rbf - PCA 0.1095 0.1307 0.0285 0.0462 SVM linear - Fbasis 0.0017 0.1750 0.0053 0.0592 SVM poly - Fbasis 0.0006 0.2545 0.0023 0.1116 SVM rbf - Fbasis 0.0030 0.1465 0.0048 0.0487 SVM linear - projection 0.0581 0.1177 0.0314 0.0513 SVM poly - projection 0.0320 0.1380 0.0314 0.0519 SVM rbf - projection 0.0291 0.1237 0.0240 0.0487 SVM linear - RKHS - radial 0.0508 0.1240 0.0277 0.0460 SVM poly - RKHS - radial 0.0463 0.1432 0.0272 0.0535 SVM rbf - RKHS - radial 0.0424 0.1347 0.0215 0.0468 SVM linear - RKHS - poly 0.1076 0.1723 0.0305 0.0585 SVM poly - RKHS - poly 0.1061 0.1833 0.0367 0.0664 SVM rbf - RKHS - poly 0.1016 0.1712 0.0306 0.0510 SVM linear - RKHS - linear 0.1151 0.1980 0.0363 0.0594 SVM poly - RKHS - linear 0.1175 0.2080 0.0341 0.0619 SVM rbf - RKHS - linear 0.1068 0.1892 0.0290 0.0550 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 2.14: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[test])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 2.15: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 2.8: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 3.0 nharm 1.0 LR_func_n_basis NA SVM_d_Linear 12.0 SVM_d_Poly 10.0 SVM_d_Radial 13.0 SVM_RKHS_radial_gamma1 6.8 SVM_RKHS_radial_gamma2 8.5 SVM_RKHS_radial_gamma3 8.5 SVM_RKHS_radial_d1 18.0 SVM_RKHS_radial_d2 15.0 SVM_RKHS_radial_d3 19.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 4.0 SVM_RKHS_poly_p3 4.0 SVM_RKHS_poly_d1 9.0 SVM_RKHS_poly_d2 17.0 SVM_RKHS_poly_d3 9.0 SVM_RKHS_linear_d1 17.0 SVM_RKHS_linear_d2 17.0 SVM_RKHS_linear_d3 24.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 2.16: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 2.17: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 2.18: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 2.19: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.26: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["simulace2sigma.html", "Kapitola 3 Závislost na parametru \\(\\sigma^2\\) 3.1 Simulace funkcionálních dat 3.2 Vyhlazení pozorovaných křivek 3.3 Klasifikace křivek 3.4 Tabulka výsledků 3.5 Simulační studie", " Kapitola 3 Závislost na parametru \\(\\sigma^2\\) V této části se budeme zabývat závislostí výsledků z předchozí sekce 2 na hodnotě \\(\\sigma^2\\), která definuje rozptyl normálního rozdělení, ze kterého generujeme náhodné chyby kolem generujících křivek (můžeme říci, že \\(\\sigma^2\\) nese informaci například o chybovosti měření určitého přístroje). Očekáváme, že s rostoucí hodnotou \\(\\sigma^2\\) se budou výsledky jednotlivých metod zhoršovat a tudíž klasifikace nebude tak úspěšná. V následující sekci 4 se pak podíváme na závislost výsledků na hodnotě \\(\\sigma^2_{shift}\\), tedy na rozptylu normálního rozdělení, ze kterého generujeme posun pro generované křivky. 3.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro tuto simulaci zvolíme následující postup: zvolíme vhodné body (pro každou klasifikační třídu jiné), které proložíme interpolačním splajnem, takto získané funkce využijeme ke generování náhodných křivek pro obě třídy, generujeme body ze zvoleného intervalu pomocí vyhlazených funkcí interpolačním splajnem, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(stats) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 12]\\). V tomto případě si definujeme v prvním kroku body, kterými má náš interpolační splajn procházet. Následně proložíme těmito body interpolační splajn, k čemuž využijeme funkci spline() z knihovny stats. Pro lepší interpretovatelnost simulovaných dat jsme definovali interval \\(I = [0, 12]\\) a funkce pro obě klasifikační třídy zvolíme tak, aby při troše fantazie mohli představovat například vývoj určité veličiny (teplota, tlak, srážky, nezaměstnanost, prodej nějaké komodity atd.) v průběhu roku. Budeme předpokládat, že se vývoj této veličiny periodicky opakuje v čase (s roční periodou), proto ve funkci spline() zvolíme parametr method = 'periodic'. Code # definujici body pro tridu 0 x.0 &lt;- c(0, 2, 2.96, 3.64, 4.2, 5.3, 6.22, 7.32, 8.3, 9.3, 10.6, 11.54, 12) y.0 &lt;- c(3, 4.66, 4.6, 3.48, 2.08, 1.5, 1.32, 2.78, 3.82, 4.48, 3.88, 3.16, 3) # definujici body pro tridu 1 x.1 &lt;- c(0, 1.3, 2, 2.7, 3.33, 3.68, 4.06, 4.72, 5.9, 6.9, 8, 9.02, 10.1, 11.25, 12) y.1 &lt;- c(2.8, 3.95, 4.82, 5.1, 4.8, 4, 2.73, 1.67, 1.53, 2.2, 3, 3.73, 3.69, 3.6, 2.8) # # definujici body pro tridu 0 # x.0 &lt;- c(0, 2, 2.96, 3.64, 4.2, 5.3, 6.22, 7.32, 8.3, 9.3, 10.6, 11.54, 12) # y.0 &lt;- c(3, 4.66, 4.6, 3.48, 2.08, 1.5, 1.32, 2.78, 3.82, 4.48, 3.88, 3.16, 3) # # # definujici body pro tridu 1 # x.1 &lt;- c(0, 1.3, 2, 2.7, 3.33, 3.68, 4.06, 4.72, 5.9, 6.9, 8, 9.02, 10.1, 11.25, 12) # y.1 &lt;- c(2.9, 3.95, 4.62, 4.6, 4.1, 3.6, 2.73, 1.67, 1.53, 2.2, 3, 3.73, 3.69, 3.6, 2.9) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Jejich grafy jsou na obrázcích níže. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- function(n) { sp &lt;- spline(x.0, y.0, method = &#39;periodic&#39;, n = n) return(list(x = sp$x, y = sp$y)) } # pridat nahodny posun v zacatku nebo periode funkce_1 &lt;- function(n) { sp &lt;- spline(x.1, y.1, method = &#39;periodic&#39;, n = n) return(list(x = sp$x, y = sp$y)) } Code n_x &lt;- 501 x &lt;- funkce_0(n_x)$x y0 &lt;- funkce_0(n_x)$y y1 &lt;- funkce_1(n_x)$y df &lt;- data.frame(x = rep(x, 2), y = c(y0, y1), Y = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), each = length(x))) df |&gt; ggplot(aes(x = x, y = y, colour = Y)) + geom_line(linewidth = 1) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 3.1: Znázornění dvou funkcí na intervalu \\([0, 1]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce. Tento posun budeme generovat s normálního rozdělění s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(fun(length(t))$y, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(X + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 12] t &lt;- seq(0, 12, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 3.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 3.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nyní jedná o periodické křivky na intervalu \\(I = [0, 12]\\), využijeme k vyhlazení fourierovu bázi. Za uzly bereme celý vektor t, budeme uvažovat tzv. harmonic acceleration penalties. Harmonic acceleration pro funkci \\(x(t)\\) je \\[ Lx = \\omega^2 Dx + D^3x, \\] kde \\(D^mx\\) značí \\(m\\)-tou derivaci funkce \\(x(t)\\) podle \\(t\\). Platí přitom \\(L\\sin(\\omega x) = 0 = L\\cos(\\omega x)\\). Potom jako penalizaci bereme hodnotu funkcionálu \\[ J_L(x) = \\int \\left[Lx(t)\\right]^2\\text dt. \\] Code rangeval &lt;- range(t) fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = length(t)) omega &lt;- 2 * pi / diff(rangeval) acvec &lt;- c(0, omega^2, 0) harmLfd &lt;- vec2Lfd(bwtvec = acvec, rangeval = rangeval) Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(fbasis, harmLfd, lambda.vect[index]) FSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(FSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;dashed&#39;, linewidth = 0.8) + geom_point(size = 2.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 1.5: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(fbasis, harmLfd, lambda.opt) FSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- FSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = &#39;Time&#39;, y = &#39;Function&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.1: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25) + theme_bw() + labs(x = &#39;Time&#39;, y = &#39;Function&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;black&#39;, linewidth = 1, linetype = &#39;twodash&#39;) Obrázek 2.2: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. 3.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 3.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) summary(neighb.model) # shrnuti modelu ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.7746479 0.7246377 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 55 16 ## 2 19 50 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.7286 0.7429 0.7357 0.7214 0.7500 0.7214 0.7000 0.6857 0.7429 0.7286 0.6929 ## 12 ## 0.6714 ## ## -Optimal number of neighbors: knn.opt= 5 ## with highest probability of correct classification max.prob= 0.75 ## ## -Probability of correct classification: 0.75 Code plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K Code neighb.model$max.prob # maximalni presnost ## [1] 0.75 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 5 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results ## [1] 0.6887997 0.7483437 0.7368816 0.7384620 0.7559592 0.7304458 0.7198493 ## [8] 0.7059212 0.7511321 0.7249606 0.6868431 0.6916845 0.6723656 0.6802993 ## [15] 0.6909588 0.6829584 0.6737427 0.6955291 0.6705774 0.6768249 0.6776039 ## [22] 0.6602983 0.6613755 0.6606879 Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 5 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.756. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 3.3: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) summary(neighb.model) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.7746479 0.7246377 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 55 16 ## 2 19 50 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 5 ## 0.75 ## ## -Optimal number of neighbors: knn.opt= 5 ## with highest probability of correct classification max.prob= 0.75 ## ## -Probability of correct classification: 0.75 Code # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost 1 - presnost ## [1] 0.2166667 Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 5, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.25 a na testovacích datech 0.2167. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 3.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 98.3 % variability v datech. První hlavní komponenta potom vysvětluje 97.06 % a druhá 1.24 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 3.4: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (88.57 %), tak i na testovacích datech (85 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.5: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 3.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (87.14 %), tak i na testovacích datech (85 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 3.3.4 Support Vector Machines Definujeme pro další metody data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 12, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 2.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 2.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí Fourierovy báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 2.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými nebo Fourierovými, funkcemi (sekce 2.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 2.3.7.5. 3.3.4.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 1]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 91.43 % pro lineární jádro, 67.86 % pro polynomiální jádro a 87.14 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 88.33 % pro lineární jádro, 70 % pro polynomiální jádro a 75 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 3.3.4.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 88.57 % pro lineární jádro, 83.57 % pro polynomiální jádro a 87.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 81.67 % pro lineární jádro, 83.33 % pro polynomiální jádro a 81.67 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 3.7: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 3.3.4.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí Fourierovy báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 100 % pro lineární jádro, 100 % pro polynomiální jádro a 100 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 86.67 % pro lineární jádro, 75 % pro polynomiální jádro a 73.33 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 3.3.4.4 Projekce na Fourierovu bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na Fourierovu bázi, protože využití B-splinové báze není pro naše periodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Code k_cv &lt;- 10 # k-fold CV # hodnoty pro Fourierovu bazi rangeval &lt;- range(t) n_basis_min &lt;- 3 n_basis_max &lt;- 40 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na Fourierovu bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = fbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 34 0.09638278 ## poly 10 0.14627747 ## radial 4 0.12418040 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 34 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9036, 10 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8537 a 4 pro radiální jádro s hodnotou přesnosti 0.8758. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 1.14: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na Fourierovu bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = fbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0 % pro lineární jádro, 5.71 % pro polynomiální jádro a 7.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 21.67 % pro lineární jádro, 16.67 % pro polynomiální jádro a 15 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 3.4 Tabulka výsledků Tabulka 3.1: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.2500 0.2167 LDA 0.1143 0.1500 QDA 0.1286 0.1500 SVM linear - diskr 0.0857 0.1167 SVM poly - diskr 0.3214 0.3000 SVM rbf - diskr 0.1286 0.2500 SVM linear - PCA 0.1143 0.1833 SVM poly - PCA 0.1643 0.1667 SVM rbf - PCA 0.1214 0.1833 SVM linear - Fbasis 0.0000 0.1333 SVM poly - Fbasis 0.0000 0.2500 SVM rbf - Fbasis 0.0000 0.2667 SVM linear - projection 0.0000 0.2167 SVM poly - projection 0.0571 0.1667 SVM rbf - projection 0.0786 0.1500 3.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 10\\). Nyní zopakujeme celou předchozí část n.sim-krát a hodnoty chybovostí si budeme ukládat to objektu SIMUL_params. Přitom budeme měnit hodnotu parametru \\(\\sigma\\) a podíváme se, jak se mění výsledky jednotlivých vybraných klasifikačních metod v závislosti na této hodnotě. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci pro kazdou hodnotu simulacniho parametru n.sim &lt;- 10 methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;) # vektor smerodatnych odchylek definujicich rozptyl kolem generujicich krivek sigma_vector &lt;- seq(0.1, 5, length = 20) # vysledny objekt, do nehoz ukladame vysledky simulaci SIMUL_params &lt;- array(data = NA, dim = c(length(methods), 4, length(sigma_vector)), dimnames = list( method = methods, metric = c(&#39;ERRtrain&#39;, &#39;Errtest&#39;, &#39;SDtrain&#39;, &#39;SDtest&#39;), sigma = paste0(sigma_vector))) for (n_sigma in 1:length(sigma_vector)) { ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 12] t &lt;- seq(0, 12, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, sigma_vector[n_sigma], 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, sigma_vector[n_sigma], 2) rangeval &lt;- range(t) fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = length(t)) omega &lt;- 2 * pi / diff(rangeval) acvec &lt;- c(0, omega^2, 0) harmLfd &lt;- vec2Lfd(bwtvec = acvec, rangeval = rangeval) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -5, to = 3, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(fbasis, harmLfd, lambda.vect[index]) FSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(FSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(fbasis, harmLfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 12, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na Fourierovu bázi # hodnoty pro Fourierovu bazi rangeval &lt;- range(t) n_basis_min &lt;- 3 n_basis_max &lt;- 40 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = fbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = fbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, paste0(n_sigma, &#39;: &#39;, sim)) } # Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) SIMUL_params[, , n_sigma] &lt;- as.matrix(SIMULACE.df) } # ulozime vysledne hodnoty save(SIMUL_params, file = &#39;RData/simulace_parametry_sigma_02a.RData&#39;) 3.5.1 Grafický výstup Podívejme se na závislost simulovaných výsledků na hodnotě parametru směrodatné odchylky vertikálního posunu. Code # pro testovaci data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(sigma_vector), names_to = &#39;sigma&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), sigma = as.numeric(sigma)) |&gt; filter(method %in% c(&#39;KNN&#39;, &#39;LDA&#39;,&#39;SVM linear - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Fbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM rbf - projection&#39;)) |&gt; ggplot(aes(x = sigma, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[x]), y = expression(widehat(Err)[test]), colour = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;bottom&#39;) Obrázek 3.8: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{x}\\), který definuje směrodatnou odchylku pro generování náhodných odchylek kolem generujících křivek. "],["simulace2shift.html", "Kapitola 4 Závislost na parametru \\(\\sigma_{shift}\\) 4.1 Simulace funkcionálních dat 4.2 Vyhlazení pozorovaných křivek 4.3 Klasifikace křivek 4.4 Tabulka výsledků 4.5 Simulační studie", " Kapitola 4 Závislost na parametru \\(\\sigma_{shift}\\) V této části se budeme zabývat závislostí výsledků ze sekce 2 na hodnotě \\(\\sigma^2_{shift}\\), která definuje rozptyl normálního rozdělení, ze kterého generujeme posun pro generované křivky. Očekáváme, že s rostoucí hodnotou \\(\\sigma^2_{shift}\\) se budou výsledky jednotlivých metod zhoršovat a tudíž klasifikace nebude tak úspěšná. Přitom předpokládáme, že metody, které využívají funkcionální podstatu dat, budou více úspěšné v porovnání s klasickými metodami při zvětšující se hodnotě \\(\\sigma^2_{shift}\\). V předchozí sekci 3 jsme se podívali na závislost výsledků na hodnotě \\(\\sigma^2\\), tedy na rozptylu normálního rozdělení, ze kterého generujeme náhodné chyby kolem generujících křivek. 4.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro tuto simulaci zvolíme následující postup: zvolíme vhodné body (pro každou klasifikační třídu jiné), které proložíme interpolačním splajnem, takto získané funkce využijeme ke generování náhodných křivek pro obě třídy, generujeme body ze zvoleného intervalu pomocí vyhlazených funkcí interpolačním splajnem, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(stats) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 12]\\). V tomto případě si definujeme v prvním kroku body, kterými má náš interpolační splajn procházet. Následně proložíme těmito body interpolační splajn, k čemuž využijeme funkci spline() z knihovny stats. Pro lepší interpretovatelnost simulovaných dat jsme definovali interval \\(I = [0, 12]\\) a funkce pro obě klasifikační třídy zvolíme tak, aby při troše fantazie mohli představovat například vývoj určité veličiny (teplota, tlak, srážky, nezaměstnanost, prodej nějaké komodity atd.) v průběhu roku. Budeme předpokládat, že se vývoj této veličiny periodicky opakuje v čase (s roční periodou), proto ve funkci spline() zvolíme parametr method = 'periodic'. Code # definujici body pro tridu 0 x.0 &lt;- c(0, 2, 2.96, 3.64, 4.2, 5.3, 6.22, 7.32, 8.3, 9.3, 10.6, 11.54, 12) y.0 &lt;- c(3, 4.66, 4.6, 3.48, 2.08, 1.5, 1.32, 2.78, 3.82, 4.48, 3.88, 3.16, 3) # definujici body pro tridu 1 x.1 &lt;- c(0, 1.3, 2, 2.7, 3.33, 3.68, 4.06, 4.72, 5.9, 6.9, 8, 9.02, 10.1, 11.25, 12) y.1 &lt;- c(2.8, 3.95, 4.82, 5.1, 4.8, 4, 2.73, 1.67, 1.53, 2.2, 3, 3.73, 3.69, 3.6, 2.8) # # definujici body pro tridu 0 # x.0 &lt;- c(0, 2, 2.96, 3.64, 4.2, 5.3, 6.22, 7.32, 8.3, 9.3, 10.6, 11.54, 12) # y.0 &lt;- c(3, 4.66, 4.6, 3.48, 2.08, 1.5, 1.32, 2.78, 3.82, 4.48, 3.88, 3.16, 3) # # # definujici body pro tridu 1 # x.1 &lt;- c(0, 1.3, 2, 2.7, 3.33, 3.68, 4.06, 4.72, 5.9, 6.9, 8, 9.02, 10.1, 11.25, 12) # y.1 &lt;- c(2.9, 3.95, 4.62, 4.6, 4.1, 3.6, 2.73, 1.67, 1.53, 2.2, 3, 3.73, 3.69, 3.6, 2.9) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Jejich grafy jsou na obrázcích níže. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- function(n) { sp &lt;- spline(x.0, y.0, method = &#39;periodic&#39;, n = n) return(list(x = sp$x, y = sp$y)) } # pridat nahodny posun v zacatku nebo periode funkce_1 &lt;- function(n) { sp &lt;- spline(x.1, y.1, method = &#39;periodic&#39;, n = n) return(list(x = sp$x, y = sp$y)) } Code n_x &lt;- 501 x &lt;- funkce_0(n_x)$x y0 &lt;- funkce_0(n_x)$y y1 &lt;- funkce_1(n_x)$y df &lt;- data.frame(x = rep(x, 2), y = c(y0, y1), Y = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), each = length(x))) df |&gt; ggplot(aes(x = x, y = y, colour = Y)) + geom_line(linewidth = 1) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 3.1: Znázornění dvou funkcí na intervalu \\([0, 1]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce. Tento posun budeme generovat s normálního rozdělění s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(fun(length(t))$y, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(X + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 12] t &lt;- seq(0, 12, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 3.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 4.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nyní jedná o periodické křivky na intervalu \\(I = [0, 12]\\), využijeme k vyhlazení fourierovu bázi. Za uzly bereme celý vektor t, budeme uvažovat tzv. harmonic acceleration penalties. Harmonic acceleration pro funkci \\(x(t)\\) je \\[ Lx = \\omega^2 Dx + D^3x, \\] kde \\(D^mx\\) značí \\(m\\)-tou derivaci funkce \\(x(t)\\) podle \\(t\\). Platí přitom \\(L\\sin(\\omega x) = 0 = L\\cos(\\omega x)\\). Potom jako penalizaci bereme hodnotu funkcionálu \\[ J_L(x) = \\int \\left[Lx(t)\\right]^2\\text dt. \\] Code rangeval &lt;- range(t) fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = length(t)) omega &lt;- 2 * pi / diff(rangeval) acvec &lt;- c(0, omega^2, 0) harmLfd &lt;- vec2Lfd(bwtvec = acvec, rangeval = rangeval) Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(fbasis, harmLfd, lambda.vect[index]) FSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(FSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;dashed&#39;, linewidth = 0.8) + geom_point(size = 2.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 1.5: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(fbasis, harmLfd, lambda.opt) FSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- FSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = &#39;Time&#39;, y = &#39;Function&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.1: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25) + theme_bw() + labs(x = &#39;Time&#39;, y = &#39;Function&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;black&#39;, linewidth = 1, linetype = &#39;twodash&#39;) Obrázek 2.2: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. 4.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 4.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) summary(neighb.model) # shrnuti modelu ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.7746479 0.7246377 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 55 16 ## 2 19 50 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.7286 0.7429 0.7357 0.7214 0.7500 0.7214 0.7000 0.6857 0.7429 0.7286 0.6929 ## 12 ## 0.6714 ## ## -Optimal number of neighbors: knn.opt= 5 ## with highest probability of correct classification max.prob= 0.75 ## ## -Probability of correct classification: 0.75 Code plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K Code neighb.model$max.prob # maximalni presnost ## [1] 0.75 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 5 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results ## [1] 0.6887997 0.7483437 0.7368816 0.7384620 0.7559592 0.7304458 0.7198493 ## [8] 0.7059212 0.7511321 0.7249606 0.6868431 0.6916845 0.6723656 0.6802993 ## [15] 0.6909588 0.6829584 0.6737427 0.6955291 0.6705774 0.6768249 0.6776039 ## [22] 0.6602983 0.6613755 0.6606879 Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 5 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.756. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 3.3: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) summary(neighb.model) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.7746479 0.7246377 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 55 16 ## 2 19 50 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 5 ## 0.75 ## ## -Optimal number of neighbors: knn.opt= 5 ## with highest probability of correct classification max.prob= 0.75 ## ## -Probability of correct classification: 0.75 Code # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost 1 - presnost ## [1] 0.2166667 Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 5, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.25 a na testovacích datech 0.2167. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 4.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 98.3 % variability v datech. První hlavní komponenta potom vysvětluje 97.06 % a druhá 1.24 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 3.4: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (88.57 %), tak i na testovacích datech (85 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.5: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (87.14 %), tak i na testovacích datech (85 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.3.4 Support Vector Machines Definujeme pro další metody data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 12, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 2.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 2.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí Fourierovy báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 2.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými nebo Fourierovými, funkcemi (sekce 2.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 2.3.7.5. 4.3.4.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 1]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 91.43 % pro lineární jádro, 67.86 % pro polynomiální jádro a 87.14 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 88.33 % pro lineární jádro, 70 % pro polynomiální jádro a 75 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 4.3.4.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 88.57 % pro lineární jádro, 83.57 % pro polynomiální jádro a 87.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 81.67 % pro lineární jádro, 83.33 % pro polynomiální jádro a 81.67 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 3.7: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 4.3.4.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí Fourierovy báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 100 % pro lineární jádro, 100 % pro polynomiální jádro a 100 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 86.67 % pro lineární jádro, 75 % pro polynomiální jádro a 73.33 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 4.3.4.4 Projekce na Fourierovu bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na Fourierovu bázi, protože využití B-splinové báze není pro naše periodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Code k_cv &lt;- 10 # k-fold CV # hodnoty pro Fourierovu bazi rangeval &lt;- range(t) n_basis_min &lt;- 3 n_basis_max &lt;- 40 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na Fourierovu bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = fbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 34 0.09638278 ## poly 10 0.14627747 ## radial 4 0.12418040 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 34 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9036, 10 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8537 a 4 pro radiální jádro s hodnotou přesnosti 0.8758. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 1.14: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na Fourierovu bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = fbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0 % pro lineární jádro, 5.71 % pro polynomiální jádro a 7.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 21.67 % pro lineární jádro, 16.67 % pro polynomiální jádro a 15 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 4.4 Tabulka výsledků Tabulka 3.1: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.2500 0.2167 LDA 0.1143 0.1500 QDA 0.1286 0.1500 SVM linear - diskr 0.0857 0.1167 SVM poly - diskr 0.3214 0.3000 SVM rbf - diskr 0.1286 0.2500 SVM linear - PCA 0.1143 0.1833 SVM poly - PCA 0.1643 0.1667 SVM rbf - PCA 0.1214 0.1833 SVM linear - Fbasis 0.0000 0.1333 SVM poly - Fbasis 0.0000 0.2500 SVM rbf - Fbasis 0.0000 0.2667 SVM linear - projection 0.0000 0.2167 SVM poly - projection 0.0571 0.1667 SVM rbf - projection 0.0786 0.1500 4.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 10\\). Nyní zopakujeme celou předchozí část n.sim-krát a hodnoty chybovostí si budeme ukládat to objektu SIMUL_params. Přitom budeme měnit hodnotu parametru \\(\\sigma_{shift}\\) a podíváme se, jak se mění výsledky jednotlivých vybraných klasifikačních metod v závislosti na této hodnotě. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci pro kazdou hodnotu simulacniho parametru n.sim &lt;- 10 methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;) # vektor smerodatnych odchylek definujicich posunuti generovanych krivek shift_vector &lt;- seq(0, 10, length = 21) # vysledny objekt, do nehoz ukladame vysledky simulaci SIMUL_params &lt;- array(data = NA, dim = c(length(methods), 4, length(shift_vector)), dimnames = list( method = methods, metric = c(&#39;ERRtrain&#39;, &#39;Errtest&#39;, &#39;SDtrain&#39;, &#39;SDtest&#39;), sigma = paste0(shift_vector))) for (n_shift in 1:length(shift_vector)) { ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 12] t &lt;- seq(0, 12, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, shift_vector[n_shift]) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, shift_vector[n_shift]) rangeval &lt;- range(t) fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = length(t)) omega &lt;- 2 * pi / diff(rangeval) acvec &lt;- c(0, omega^2, 0) harmLfd &lt;- vec2Lfd(bwtvec = acvec, rangeval = rangeval) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -5, to = 3, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(fbasis, harmLfd, lambda.vect[index]) FSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(FSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(fbasis, harmLfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 12, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na Fourierovu bázi # hodnoty pro Fourierovu bazi rangeval &lt;- range(t) n_basis_min &lt;- 3 n_basis_max &lt;- 40 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = fbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = fbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, paste0(n_shift, &#39;: &#39;, sim)) } # Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) SIMUL_params[, , n_shift] &lt;- as.matrix(SIMULACE.df) } # ulozime vysledne hodnoty save(SIMUL_params, file = &#39;RData/simulace_parametry_shift_02b.RData&#39;) 4.5.1 Grafický výstup Podívejme se na závislost simulovaných výsledků na hodnotě parametru směrodatné odchylky vertikálního posunu. Code # pro testovaci data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(shift_vector), names_to = &#39;shift&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), shift = as.numeric(shift)) |&gt; filter(method %in% c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Fbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM rbf - projection&#39;)) |&gt; ggplot(aes(x = shift, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[shift]), y = expression(widehat(Err)[test]), colour = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;bottom&#39;) Obrázek 3.8: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{shift}\\), který definuje směrodatnou odchylku pro generování vertikálního posunutí simulovaných křivek. "],["simulace3.html", "Kapitola 5 Simulace 3 5.1 Simulace funkcionálních dat 5.2 Vyhlazení pozorovaných křivek 5.3 Klasifikace křivek 5.4 Tabulka výsledků 5.5 Simulační studie", " Kapitola 5 Simulace 3 5.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro simulaci nejprve: zvolíme vhodné funkce, generujeme body ze zvoleného intervalu, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 6]\\). Nyní vytvoříme funkce pomocí interpolačních polynomů. Nejprve si definujeme body, kterými má procházet naše křivka, a následně jimi proložíme interpolační polynom, který použijeme pro generování křivek pro klasifikaci. Code # definujici body pro tridu 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # definujici body pro tridu 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) Pro výpočet interpolačních polynomů využijeme funkci poly.calc() z knihovny polynom. Dále definujeme funkce poly.0() a poly.1(), které budou počítat hodnoty polynomů v daném bodě intervalu. K jejich vytvoření použijeme funkci predict(), na jejíž vstup zadáme příslušný polynom a bod, ve kterám chceme polynom vyhodnotit. Code # vypocet polynomu polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # vykresleni polynomu xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) Obrázek 3.2: Znázornění dvou funkcí na intervalu \\(I = [0, 6]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce (parametr sigma_shift). Tento posun budeme generovat s normálního rozdělení s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 5.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Opět využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor t, standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) Obrázek 5.1: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 5.2: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Obrázek 1.6: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Obrázek 5.3: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 5.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěšnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 5.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) neighb.model$max.prob # maximalni presnost ## [1] 0.6428571 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 5 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results ## [1] 0.5623592 0.5655382 0.6374971 0.5902938 0.6570858 0.5980519 0.5758192 ## [8] 0.5792012 0.5993777 0.5699559 0.5507196 0.5505840 0.5720883 0.5730172 ## [15] 0.6172813 0.5921596 0.6093109 0.5973814 0.5977928 0.6116456 0.6028113 ## [22] 0.5856167 0.5885471 0.5717639 Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 5 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.6571. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 5.4: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 5, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.3571 a na testovacích datech 0.3833. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 5.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 98.72 % variability v datech. První hlavní komponenta potom vysvětluje 98.2 % a druhá 0.52 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 5.5: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (58.57 %), tak i na testovacích datech (60 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (64.29 %), tak i na testovacích datech (60 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 5.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 5.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 3.7: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 29, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 11 s validační chybovostí 0.072. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) Obrázek 5.7: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 11 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 3.57 %) i testovací chybovost (rovna 8.33 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Klasifikační\\n třída&#39;) Obrázek 1.14: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line() + theme_bw() + labs(x = expression(x[1]), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Obrázek 2.9: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 6]\\). Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro časy \\(t\\) z prostředka a začátku intervalu \\([0, 6]\\), zatímco pro pozdějsí časy jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd na začátku a konci intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy přesnost klasifikátoru na trénovacích (59.29 %) i na testovacích datech (60 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.17: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 5.3.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [0, 6]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost klasifikátoru na testovacích datech je tedy 53.33 % a na trénovacích datech 66.43 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 5.8: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.19: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 2 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na testovacích datech je tedy 60 % a na trénovacích datech 62.14 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 5.9: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 5.10: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 66.43 % a na testovacích datech 55 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 2.11: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 5.11: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 5.3.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 6]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost náhodného lesu na trénovacích datech je tedy 99.29 % a na testovacích datech 60 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních p = 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 95.71 % a na testovacích datech 60 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost tohoto klasifikátoru na trénovacích datech je 99.29 % a na testovacích datech 63.33 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 5.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 5.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 5.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 5.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 5.3.7.5. 5.3.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 6]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 89.29 % pro lineární jádro, 70.71 % pro polynomiální jádro a 63.57 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 78.33 % pro lineární jádro, 66.67 % pro polynomiální jádro a 51.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.3.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 60 % pro lineární jádro, 57.86 % pro polynomiální jádro a 62.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 60 % pro lineární jádro, 55 % pro polynomiální jádro a 58.33 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) # + Obrázek 5.12: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code # geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), # colour = &#39;black&#39;) + # geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), # colour = &#39;black&#39;) Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.3.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 85.71 % pro lineární jádro, 79.29 % pro polynomiální jádro a 62.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 75 % pro lineární jádro, 76.67 % pro polynomiální jádro a 51.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.3.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 10 0.06457875 ## poly 10 0.07736264 ## radial 10 0.07838828 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 10 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9354, 10 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9226 a 10 pro radiální jádro s hodnotou přesnosti 0.9216. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &#39;Jádro&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 5.13: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 3.57 % pro lineární jádro, 5 % pro polynomiální jádro a 5 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 8.33 % pro lineární jádro, 10 % pro polynomiální jádro a 10 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 5.3.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 5.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 5.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 5.3.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 5.3.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku3 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 5.3.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). 5.3.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 5.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.1000 0.4167 SVM poly - RKHS 0.0500 0.3167 SVM rbf - RKHS 0.0786 0.2500 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 5.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 11 8.4834 0.0727 linear poly 8 8.4834 0.1145 polynomial radial 40 0.4394 0.0993 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 11 a \\(\\gamma={}\\) 8.4834 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9273, \\(d={}\\) 8 a \\(\\gamma={}\\) 8.4834 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8855 a \\(d={}\\) 40 a \\(\\gamma={}\\) 0.4394 pro radiální jádro s hodnotou přesnosti 0.9007. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 5.14: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.4: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0571 0.1333 SVM poly - RKHS - radial 0.0643 0.0833 SVM rbf - RKHS - radial 0.0143 0.1500 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 5.71 % pro lineární jádro, 6.43 % pro polynomiální jádro a 1.43 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 13.33 % pro lineární jádro, 8.33 % pro polynomiální jádro a 15 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 5.3.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 1.5: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 9 2 0.2972 linear poly 14 4 0.2950 polynomial radial 8 2 0.2897 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 9 a \\(p={}\\) 2 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.7028, \\(d={}\\) 14 a \\(p={}\\) 4 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.705 a \\(d={}\\) 8 a \\(p={}\\) 2 pro radiální jádro s hodnotou přesnosti 0.7103. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.6: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.2929 0.3000 SVM poly - RKHS - poly 0.2429 0.3667 SVM rbf - RKHS - poly 0.2429 0.2333 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 29.29 % pro lineární jádro, 24.29 % pro polynomiální jádro a 24.29 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 30 % pro lineární jádro, 36.67 % pro polynomiální jádro a 23.33 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 5.3.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 5.3: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 29 0.3799 linear poly 4 0.3890 polynomial radial 37 0.3674 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 29 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.6201, \\(d={}\\) 4 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.611 a \\(d={}\\) 37 pro radiální jádro s hodnotou přesnosti 0.6326. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 5.4: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.2643 0.3000 SVM poly - RKHS - linear 0.3714 0.4333 SVM rbf - RKHS - linear 0.2714 0.3667 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 26.43 % pro lineární jádro, 37.14 % pro polynomiální jádro a 27.14 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 30 % pro lineární jádro, 43.33 % pro polynomiální jádro a 36.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 5.4 Tabulka výsledků Tabulka 5.5: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3571 0.3833 LDA 0.4143 0.4000 QDA 0.3571 0.4000 LR functional 0.0357 0.0833 LR score 0.4071 0.4000 Tree - diskr. 0.3357 0.4667 Tree - score 0.3786 0.4000 Tree - Bbasis 0.3357 0.4500 RForest - diskr 0.0071 0.4000 RForest - score 0.0429 0.4000 RForest - Bbasis 0.0071 0.3667 SVM linear - diskr 0.1071 0.2167 SVM poly - diskr 0.2929 0.3333 SVM rbf - diskr 0.3643 0.4833 SVM linear - PCA 0.4000 0.4000 SVM poly - PCA 0.4214 0.4500 SVM rbf - PCA 0.3714 0.4167 SVM linear - Bbasis 0.1429 0.2500 SVM poly - Bbasis 0.2071 0.2333 SVM rbf - Bbasis 0.3714 0.4833 SVM linear - projection 0.0357 0.0833 SVM poly - projection 0.0500 0.1000 SVM rbf - projection 0.0500 0.1000 SVM linear - RKHS - radial 0.0571 0.1333 SVM poly - RKHS - radial 0.0643 0.0833 SVM rbf - RKHS - radial 0.0143 0.1500 SVM linear - RKHS - poly 0.2929 0.3000 SVM poly - RKHS - poly 0.2429 0.3667 SVM rbf - RKHS - poly 0.2429 0.2333 SVM linear - RKHS - linear 0.2643 0.3000 SVM poly - RKHS - linear 0.3714 0.4333 SVM rbf - RKHS - linear 0.2714 0.3667 5.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -5, to = 3, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by =2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_03.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/simulace_03_res.RData&#39;) 5.5.1 Výsledky Tabulka 5.6: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.3487 0.3998 0.0403 0.0741 LDA 0.3654 0.4083 0.0672 0.0963 QDA 0.3607 0.4167 0.0660 0.0981 LR_functional 0.0377 0.1207 0.0294 0.0464 LR_score 0.3680 0.4027 0.0724 0.0894 Tree_discr 0.2857 0.4315 0.0760 0.0749 Tree_score 0.3334 0.4277 0.0780 0.1041 Tree_Bbasis 0.2884 0.4378 0.0702 0.0755 RF_discr 0.0114 0.3485 0.0093 0.0738 RF_score 0.0525 0.4452 0.0172 0.0955 RF_Bbasis 0.0105 0.3433 0.0081 0.0743 SVM linear - diskr 0.1152 0.1522 0.0296 0.0477 SVM poly - diskr 0.3486 0.4233 0.0541 0.0947 SVM rbf - diskr 0.3358 0.4443 0.0469 0.0881 SVM linear - PCA 0.3656 0.4158 0.0708 0.0996 SVM poly - PCA 0.3932 0.4740 0.0611 0.0841 SVM rbf - PCA 0.3292 0.4295 0.0543 0.1018 SVM linear - Bbasis 0.1249 0.1620 0.0315 0.0522 SVM poly - Bbasis 0.3396 0.4152 0.0516 0.0871 SVM rbf - Bbasis 0.2987 0.4053 0.0530 0.0907 SVM linear - projection 0.0434 0.0955 0.0200 0.0412 SVM poly - projection 0.1107 0.1743 0.0436 0.0770 SVM rbf - projection 0.0506 0.1175 0.0181 0.0521 SVM linear - RKHS - radial 0.0536 0.1445 0.0264 0.0456 SVM poly - RKHS - radial 0.0468 0.1675 0.0287 0.0542 SVM rbf - RKHS - radial 0.0458 0.1535 0.0225 0.0514 SVM linear - RKHS - poly 0.2006 0.3078 0.0459 0.0747 SVM poly - RKHS - poly 0.2194 0.3315 0.0418 0.0687 SVM rbf - RKHS - poly 0.1910 0.2878 0.0391 0.0701 SVM linear - RKHS - linear 0.2557 0.3897 0.0588 0.0618 SVM poly - RKHS - linear 0.3028 0.4485 0.0365 0.0735 SVM rbf - RKHS - linear 0.2491 0.3728 0.0371 0.0693 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # pro trenovaci data # tikz(file = &quot;figures/DP_sim_03_boxplot_train.tex&quot;, width = 10, height = 6) SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods2, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 1.28: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # dev.off() Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[test])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 1.29: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 5.7: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 3.0 nharm 1.0 LR_func_n_basis 12.0 SVM_d_Linear 13.0 SVM_d_Poly 11.0 SVM_d_Radial 11.0 SVM_RKHS_radial_gamma1 5.2 SVM_RKHS_radial_gamma2 1.9 SVM_RKHS_radial_gamma3 1.9 SVM_RKHS_radial_d1 17.0 SVM_RKHS_radial_d2 15.0 SVM_RKHS_radial_d3 17.0 SVM_RKHS_poly_p1 3.0 SVM_RKHS_poly_p2 4.0 SVM_RKHS_poly_p3 3.0 SVM_RKHS_poly_d1 17.0 SVM_RKHS_poly_d2 17.0 SVM_RKHS_poly_d3 16.0 SVM_RKHS_linear_d1 21.0 SVM_RKHS_linear_d2 21.0 SVM_RKHS_linear_d3 23.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.15: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.16: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.17: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.18: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.19: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["simulace3sigma.html", "Kapitola 6 Závislost na parametru \\(\\sigma^2\\) 6.1 Simulace funkcionálních dat 6.2 Vyhlazení pozorovaných křivek 6.3 Klasifikace křivek 6.4 Tabulka výsledků 6.5 Simulační studie", " Kapitola 6 Závislost na parametru \\(\\sigma^2\\) V této části se budeme zabývat závislostí výsledků z předchozí sekce 5 na hodnotě \\(\\sigma^2\\), která definuje rozptyl normálního rozdělení, ze kterého generujeme náhodné chyby kolem generujících křivek (můžeme říci, že \\(\\sigma^2\\) nese informaci například o chybovosti měření určitého přístroje). Očekáváme, že s rostoucí hodnotou \\(\\sigma^2\\) se budou výsledky jednotlivých metod zhoršovat a tudíž klasifikace nebude tak úspěšná. V následující sekci 7 se pak podíváme na závislost výsledků na hodnotě \\(\\sigma^2_{shift}\\), tedy na rozptylu normálního rozdělení, ze kterého generujeme posun pro generované křivky. 6.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro simulaci nejprve: zvolíme vhodné funkce, generujeme body ze zvoleného intervalu, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 6]\\). Nyní vytvoříme funkce pomocí interpolačních polynomů. Nejprve si definujeme body, kterými má procházet naše křivka, a následně jimi proložíme interpolační polynom, který použijeme pro generování křivek pro klasifikaci. Code # definujici body pro tridu 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # definujici body pro tridu 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Pro výpočet interpolačních polynomů využijeme funkci poly.calc() z knihovny polynom. Dále definujeme funkce poly.0() a poly.1(), které budou počítat hodnoty polynomů v daném bodě intervalu. K jejich vytvoření použijeme funkci predict(), na jejíž vstup zadáme příslušný polynom a bod, ve kterám chceme polynom vyhodnotit. Code # vypocet polynomu polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # vykresleni polynomu xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 3.2: Znázornění dvou funkcí na intervalu \\(I = [0, 6]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce (parametr sigma_shift). Tento posun budeme generovat s normálního rozdělení s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 6.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Opět využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor t, standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) Obrázek 5.1: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 5.2: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Obrázek 1.6: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Obrázek 5.3: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 6.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 6.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) summary(neighb.model) # shrnuti modelu ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.6338028 0.6521739 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 45 26 ## 2 24 45 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.5714 0.5714 0.6214 0.5500 0.6429 0.6143 0.5929 0.5643 0.5571 0.5786 0.5357 ## 12 ## 0.5429 ## ## -Optimal number of neighbors: knn.opt= 5 ## with highest probability of correct classification max.prob= 0.6428571 ## ## -Probability of correct classification: 0.6429 Code plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K Code neighb.model$max.prob # maximalni presnost ## [1] 0.6428571 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 5 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results ## [1] 0.5623592 0.5655382 0.6374971 0.5902938 0.6570858 0.5980519 0.5758192 ## [8] 0.5792012 0.5993777 0.5699559 0.5507196 0.5505840 0.5720883 0.5730172 ## [15] 0.6172813 0.5921596 0.6093109 0.5973814 0.5977928 0.6116456 0.6028113 ## [22] 0.5856167 0.5885471 0.5717639 Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 5 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.6571. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 5.4: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) summary(neighb.model) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.6338028 0.6521739 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 45 26 ## 2 24 45 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 5 ## 0.6429 ## ## -Optimal number of neighbors: knn.opt= 5 ## with highest probability of correct classification max.prob= 0.6428571 ## ## -Probability of correct classification: 0.6429 Code # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost 1 - presnost ## [1] 0.3833333 Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 5, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.3571 a na testovacích datech 0.3833. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 6.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 98.72 % variability v datech. První hlavní komponenta potom vysvětluje 98.2 % a druhá 0.52 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 5.5: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (58.57 %), tak i na testovacích datech (60 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (64.29 %), tak i na testovacích datech (60 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 5.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 6.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 3.7: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 29, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) CV.results ## 4 5 6 7 8 9 10 11 ## 0.5952478 0.6050643 0.8208611 0.8542745 0.8426443 0.8659019 0.9053366 0.9280022 ## 12 13 14 15 16 17 18 19 ## 0.9171198 0.9053677 0.9128187 0.9195076 0.8910355 0.8901079 0.8831144 0.8508246 ## 20 21 22 23 24 25 ## 0.8688230 0.8615599 0.8691765 0.8845426 0.8872714 0.8852691 Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 11 s validační chybovostí 0.072. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) Obrázek 5.7: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 11 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 3.57 %) i testovací chybovost (rovna 8.33 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Třída&#39;) Obrázek 1.14: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Obrázek 2.9: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 12]\\). Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro časy \\(t\\) z prostředka intervalu \\([0, 1]\\), zatímco pro krajní časy jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd na začátku a konci intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.5 Support Vector Machines Definujeme pro další metody data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci. Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi. Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme definovat novou jádrovou funkci, která vezme do úvahy funkcionální podstatu dat. To je možné zejména proto, že v definici metody SVM pracujeme s libovolným Hilbertovým prostorem (bez omezení na konečnou dimenzi), tedy připouštíme i Hilbertův prostor funkcionálních dat. 6.3.5.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 1]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 89.29 % pro lineární jádro, 70.71 % pro polynomiální jádro a 63.57 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 78.33 % pro lineární jádro, 66.67 % pro polynomiální jádro a 51.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.3.5.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních p = 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 60 % pro lineární jádro, 57.86 % pro polynomiální jádro a 62.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 60 % pro lineární jádro, 55 % pro polynomiální jádro a 58.33 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 6.1: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.3.5.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 85.71 % pro lineární jádro, 79.29 % pro polynomiální jádro a 62.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 75 % pro lineární jádro, 76.67 % pro polynomiální jádro a 51.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.3.5.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 11 0.05578755 ## poly 10 0.09166667 ## radial 10 0.06959707 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 11 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9442, 10 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9083 a 10 pro radiální jádro s hodnotou přesnosti 0.9304. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 5.9: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 4.29 % pro lineární jádro, 5 % pro polynomiální jádro a 5 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 10 % pro lineární jádro, 10 % pro polynomiální jádro a 10 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 6.3.5.5 RKHS + SVM 6.3.5.5.0.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 6.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.1000 0.4167 SVM poly - RKHS 0.0500 0.3167 SVM rbf - RKHS 0.0786 0.2500 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 6.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 9 8.4834 0.0716 linear poly 8 13.8950 0.1147 polynomial radial 26 0.7197 0.1218 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 9 a \\(\\gamma={}\\) 8.4834 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9284, \\(d={}\\) 8 a \\(\\gamma={}\\) 13.895 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8853 a \\(d={}\\) 26 a \\(\\gamma={}\\) 0.7197 pro radiální jádro s hodnotou přesnosti 0.8782. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 6.2: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 6.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0786 0.1500 SVM poly - RKHS - radial 0.0714 0.0833 SVM rbf - RKHS - radial 0.0214 0.1667 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 7.86 % pro lineární jádro, 7.14 % pro polynomiální jádro a 2.14 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 15 % pro lineární jádro, 8.33 % pro polynomiální jádro a 16.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 6.4 Tabulka výsledků Tabulka 6.4: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3571 0.3833 LDA 0.4143 0.4000 QDA 0.3571 0.4000 LR functional 0.0357 0.0833 SVM linear - diskr 0.1071 0.2167 SVM poly - diskr 0.2929 0.3333 SVM rbf - diskr 0.3643 0.4833 SVM linear - PCA 0.4000 0.4000 SVM poly - PCA 0.4214 0.4500 SVM rbf - PCA 0.3714 0.4167 SVM linear - Bbasis 0.1429 0.2500 SVM poly - Bbasis 0.2071 0.2333 SVM rbf - Bbasis 0.3714 0.4833 SVM linear - projection 0.0429 0.1000 SVM poly - projection 0.0500 0.1000 SVM rbf - projection 0.0500 0.1000 SVM linear - RKHS - radial 0.0786 0.1500 SVM poly - RKHS - radial 0.0714 0.0833 SVM rbf - RKHS - radial 0.0214 0.1667 6.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 10\\). Nyní zopakujeme celou předchozí část n.sim-krát a hodnoty chybovostí si budeme ukládat to objektu SIMUL_params. Přitom budeme měnit hodnotu parametru \\(\\sigma\\) a podíváme se, jak se mění výsledky jednotlivých vybraných klasifikačních metod v závislosti na této hodnotě. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci pro kazdou hodnotu simulacniho parametru n.sim &lt;- 10 methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;) # vektor smerodatnych odchylek definujicich rozptyl kolem generujicich krivek sigma_vector &lt;- seq(0.1, 5, length = 20) # vysledny objekt, do nehoz ukladame vysledky simulaci SIMUL_params &lt;- array(data = NA, dim = c(length(methods), 4, length(sigma_vector)), dimnames = list( method = methods, metric = c(&#39;ERRtrain&#39;, &#39;Errtest&#39;, &#39;SDtrain&#39;, &#39;SDtest&#39;), sigma = paste0(sigma_vector))) for (n_sigma in 1:length(sigma_vector)) { ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, sigma_vector[n_sigma], 4) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, sigma_vector[n_sigma], 4) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -5, to = 3, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by =2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS$SVM_RKHS_radial_gamma[sim] &lt;- gamma.opt CV_RESULTS$SVM_RKHS_radial_d[sim] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, paste0(n_sigma, &#39;: &#39;, sim)) } # Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) SIMUL_params[, , n_sigma] &lt;- as.matrix(SIMULACE.df) } # ulozime vysledne hodnoty save(SIMUL_params, file = &#39;RData/simulace_parametry_sigma_03a.RData&#39;) ### Grafický výstup Podívejme se na závislost simulovaných výsledků na hodnotě parametru směrodatné odchylky vertikálního posunu. Code # pro testovaci data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(sigma_vector), names_to = &#39;sigma&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), sigma = as.numeric(sigma)) |&gt; filter(method %in% c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;LR_functional&#39;,# &#39;SVM linear - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;)) |&gt; ggplot(aes(x = sigma, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[x]), y = expression(widehat(Err)[test]), colour = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;bottom&#39;) Obrázek 5.12: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{x}\\), který definuje směrodatnou odchylku pro generování náhodných odchylek kolem generujících křivek. "],["simulace3shift.html", "Kapitola 7 Závislost na parametru \\(\\sigma_{shift}\\) 7.1 Simulace funkcionálních dat 7.2 Vyhlazení pozorovaných křivek 7.3 Klasifikace křivek 7.4 Tabulka výsledků 7.5 Simulační studie", " Kapitola 7 Závislost na parametru \\(\\sigma_{shift}\\) V této části se budeme zabývat závislostí výsledků ze sekce 5 na hodnotě \\(\\sigma^2_{shift}\\), která definuje rozptyl normálního rozdělení, ze kterého generujeme posun pro generované křivky. Očekáváme, že s rostoucí hodnotou \\(\\sigma^2_{shift}\\) se budou výsledky jednotlivých metod zhoršovat a tudíž klasifikace nebude tak úspěšná. Přitom předpokládáme, že metody, které využívají funkcionální podstatu dat, budou více úspěšné v porovnání s klasickými metodami při zvětšující se hodnotě \\(\\sigma^2_{shift}\\). V předchozí sekci 6 jsme se podívali na závislost výsledků na hodnotě \\(\\sigma^2\\), tedy na rozptylu normálního rozdělení, ze kterého generujeme náhodné chyby kolem generujících křivek. 7.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro simulaci nejprve: zvolíme vhodné funkce, generujeme body ze zvoleného intervalu, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 6]\\). Nyní vytvoříme funkce pomocí interpolačních polynomů. Nejprve si definujeme body, kterými má procházet naše křivka, a následně jimi proložíme interpolační polynom, který použijeme pro generování křivek pro klasifikaci. Code # definujici body pro tridu 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # definujici body pro tridu 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Pro výpočet interpolačních polynomů využijeme funkci poly.calc() z knihovny polynom. Dále definujeme funkce poly.0() a poly.1(), které budou počítat hodnoty polynomů v daném bodě intervalu. K jejich vytvoření použijeme funkci predict(), na jejíž vstup zadáme příslušný polynom a bod, ve kterám chceme polynom vyhodnotit. Code # vypocet polynomu polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # vykresleni polynomu xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 3.2: Znázornění dvou funkcí na intervalu \\(I = [0, 6]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce (parametr sigma_shift). Tento posun budeme generovat s normálního rozdělení s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 7.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Opět využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor t, standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) Obrázek 5.1: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 5.2: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Obrázek 1.6: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Obrázek 5.3: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 7.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 7.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) summary(neighb.model) # shrnuti modelu ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.6338028 0.6521739 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 45 26 ## 2 24 45 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.5714 0.5714 0.6214 0.5500 0.6429 0.6143 0.5929 0.5643 0.5571 0.5786 0.5357 ## 12 ## 0.5429 ## ## -Optimal number of neighbors: knn.opt= 5 ## with highest probability of correct classification max.prob= 0.6428571 ## ## -Probability of correct classification: 0.6429 Code plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K Code neighb.model$max.prob # maximalni presnost ## [1] 0.6428571 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 5 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results ## [1] 0.5623592 0.5655382 0.6374971 0.5902938 0.6570858 0.5980519 0.5758192 ## [8] 0.5792012 0.5993777 0.5699559 0.5507196 0.5505840 0.5720883 0.5730172 ## [15] 0.6172813 0.5921596 0.6093109 0.5973814 0.5977928 0.6116456 0.6028113 ## [22] 0.5856167 0.5885471 0.5717639 Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 5 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.6571. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 5.4: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) summary(neighb.model) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.6338028 0.6521739 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 45 26 ## 2 24 45 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 5 ## 0.6429 ## ## -Optimal number of neighbors: knn.opt= 5 ## with highest probability of correct classification max.prob= 0.6428571 ## ## -Probability of correct classification: 0.6429 Code # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost 1 - presnost ## [1] 0.3833333 Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 5, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.3571 a na testovacích datech 0.3833. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 7.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 98.72 % variability v datech. První hlavní komponenta potom vysvětluje 98.2 % a druhá 0.52 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 5.5: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (58.57 %), tak i na testovacích datech (60 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 7.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (64.29 %), tak i na testovacích datech (60 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 5.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 7.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 7.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 3.7: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 29, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) CV.results ## 4 5 6 7 8 9 10 11 ## 0.5952478 0.6050643 0.8208611 0.8542745 0.8426443 0.8659019 0.9053366 0.9280022 ## 12 13 14 15 16 17 18 19 ## 0.9171198 0.9053677 0.9128187 0.9195076 0.8910355 0.8901079 0.8831144 0.8508246 ## 20 21 22 23 24 25 ## 0.8688230 0.8615599 0.8691765 0.8845426 0.8872714 0.8852691 Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 11 s validační chybovostí 0.072. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) Obrázek 5.7: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 11 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 3.57 %) i testovací chybovost (rovna 8.33 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Třída&#39;) Obrázek 1.14: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Obrázek 2.9: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 12]\\). Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro časy \\(t\\) z prostředka intervalu \\([0, 1]\\), zatímco pro krajní časy jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd na začátku a konci intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 7.3.5 Support Vector Machines Definujeme pro další metody data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci. Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi. Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme definovat novou jádrovou funkci, která vezme do úvahy funkcionální podstatu dat. To je možné zejména proto, že v definici metody SVM pracujeme s libovolným Hilbertovým prostorem (bez omezení na konečnou dimenzi), tedy připouštíme i Hilbertův prostor funkcionálních dat. 7.3.5.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 1]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 89.29 % pro lineární jádro, 70.71 % pro polynomiální jádro a 63.57 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 78.33 % pro lineární jádro, 66.67 % pro polynomiální jádro a 51.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 7.3.5.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních p = 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 60 % pro lineární jádro, 57.86 % pro polynomiální jádro a 62.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 60 % pro lineární jádro, 55 % pro polynomiální jádro a 58.33 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 6.1: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 7.3.5.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 85.71 % pro lineární jádro, 79.29 % pro polynomiální jádro a 62.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 75 % pro lineární jádro, 76.67 % pro polynomiální jádro a 51.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 7.3.5.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 11 0.05578755 ## poly 10 0.09166667 ## radial 10 0.06959707 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 11 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9442, 10 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9083 a 10 pro radiální jádro s hodnotou přesnosti 0.9304. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 5.9: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 4.29 % pro lineární jádro, 5 % pro polynomiální jádro a 5 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 10 % pro lineární jádro, 10 % pro polynomiální jádro a 10 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 7.3.5.5 RKHS + SVM 7.3.5.5.0.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 6.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.1000 0.4167 SVM poly - RKHS 0.0500 0.3167 SVM rbf - RKHS 0.0786 0.2500 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 6.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 9 8.4834 0.0716 linear poly 8 13.8950 0.1147 polynomial radial 26 0.7197 0.1218 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 9 a \\(\\gamma={}\\) 8.4834 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9284, \\(d={}\\) 8 a \\(\\gamma={}\\) 13.895 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8853 a \\(d={}\\) 26 a \\(\\gamma={}\\) 0.7197 pro radiální jádro s hodnotou přesnosti 0.8782. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 6.2: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 6.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0786 0.1500 SVM poly - RKHS - radial 0.0714 0.0833 SVM rbf - RKHS - radial 0.0214 0.1667 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 7.86 % pro lineární jádro, 7.14 % pro polynomiální jádro a 2.14 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 15 % pro lineární jádro, 8.33 % pro polynomiální jádro a 16.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 7.4 Tabulka výsledků Tabulka 6.4: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3571 0.3833 LDA 0.4143 0.4000 QDA 0.3571 0.4000 LR functional 0.0357 0.0833 SVM linear - diskr 0.1071 0.2167 SVM poly - diskr 0.2929 0.3333 SVM rbf - diskr 0.3643 0.4833 SVM linear - PCA 0.4000 0.4000 SVM poly - PCA 0.4214 0.4500 SVM rbf - PCA 0.3714 0.4167 SVM linear - Bbasis 0.1429 0.2500 SVM poly - Bbasis 0.2071 0.2333 SVM rbf - Bbasis 0.3714 0.4833 SVM linear - projection 0.0429 0.1000 SVM poly - projection 0.0500 0.1000 SVM rbf - projection 0.0500 0.1000 SVM linear - RKHS - radial 0.0786 0.1500 SVM poly - RKHS - radial 0.0714 0.0833 SVM rbf - RKHS - radial 0.0214 0.1667 7.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 10\\). Nyní zopakujeme celou předchozí část n.sim-krát a hodnoty chybovostí si budeme ukládat to objektu SIMUL_params. Přitom budeme měnit hodnotu parametru \\(\\sigma_{shift}\\) a podíváme se, jak se mění výsledky jednotlivých vybraných klasifikačních metod v závislosti na této hodnotě. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci pro kazdou hodnotu simulacniho parametru n.sim &lt;- 10 methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;) # vektor smerodatnych odchylek definujicich posunuti generovanych krivek shift_vector &lt;- seq(0, 10, length = 21) # vysledny objekt, do nehoz ukladame vysledky simulaci SIMUL_params &lt;- array(data = NA, dim = c(length(methods), 4, length(shift_vector)), dimnames = list( method = methods, metric = c(&#39;ERRtrain&#39;, &#39;Errtest&#39;, &#39;SDtrain&#39;, &#39;SDtest&#39;), shift = paste0(shift_vector))) for (n_shift in 1:length(shift_vector)) { ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, shift_vector[n_shift]) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, shift_vector[n_shift]) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -5, to = 3, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by =2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS$SVM_RKHS_radial_gamma[sim] &lt;- gamma.opt CV_RESULTS$SVM_RKHS_radial_d[sim] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, paste0(n_shift, &#39;: &#39;, sim)) } # Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) SIMUL_params[, , n_shift] &lt;- as.matrix(SIMULACE.df) } # ulozime vysledne hodnoty save(SIMUL_params, file = &#39;RData/simulace_parametry_shift_03b.RData&#39;) 7.5.1 Grafický výstup Podívejme se na závislost simulovaných výsledků na hodnotě parametru směrodatné odchylky vertikálního posunu. Code # pro testovaci data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(shift_vector), names_to = &#39;shift&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), shift = as.numeric(shift)) |&gt; filter(method %in% c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;LR_functional&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;)) |&gt; ggplot(aes(x = shift, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[shift]), y = expression(widehat(Err)[test]), colour = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;bottom&#39;) Obrázek 5.12: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{shift}\\), který definuje směrodatnou odchylku pro generování vertikálního posunutí simulovaných křivek. "],["simulace3diskr.html", "Kapitola 8 Závislost na diskretizaci 8.1 Simulace funkcionálních dat 8.2 Vyhlazení pozorovaných křivek 8.3 Klasifikace křivek", " Kapitola 8 Závislost na diskretizaci V této části se budeme zabývat závislostí výsledků z předchozí sekce 5 na hodnotě \\(p\\), která definuje délku ekvidistantní posloupnosti bodů, které jsou použity k diskretizaci pozorovaných funkcionálních objektů. Bude nás zajímat, jak se mění chybovosti jednotlivých klasifikačních metod při zjemňování dělení intervalu, tedy při zvětšující se dimenzi diskretizovaných vektorů. 8.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro simulaci nejprve: zvolíme vhodné funkce, generujeme body ze zvoleného intervalu, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 6]\\). Nyní vytvoříme funkce pomocí interpolačních polynomů. Nejprve si definujeme body, kterými má procházet naše křivka, a následně jimi proložíme interpolační polynom, který použijeme pro generování křivek pro klasifikaci. Code # definujici body pro tridu 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # definujici body pro tridu 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Pro výpočet interpolačních polynomů využijeme funkci poly.calc() z knihovny polynom. Dále definujeme funkce poly.0() a poly.1(), které budou počítat hodnoty polynomů v daném bodě intervalu. K jejich vytvoření použijeme funkci predict(), na jejíž vstup zadáme příslušný polynom a bod, ve kterám chceme polynom vyhodnotit. Code # vypocet polynomu polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # vykresleni polynomu xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 3.2: Znázornění dvou funkcí na intervalu \\(I = [0, 6]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce (parametr sigma_shift). Tento posun budeme generovat s normálního rozdělení s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 8.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Opět využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor t, standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) Obrázek 5.1: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 5.2: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Obrázek 1.6: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Obrázek 5.3: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 8.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les "],["simulace4.html", "Kapitola 9 Simulace 4 9.1 Simulace funkcionálních dat 9.2 Vyhlazení pozorovaných křivek 9.3 Výpočet derivací 9.4 Klasifikace křivek 9.5 Tabulka výsledků 9.6 Simulační studie", " Kapitola 9 Simulace 4 V této poslední sekci věnující se simulovaným datům se budeme zabývat stejnými daty jako v sekci 5 (případně také v sekcích 6 a 7), tedy půjde o generování funkcionálních dat z funkcí vypočtených pomocí interpolačních polynomů. Jelikož jsme v sekci 5 generovali data s náhodným vertikálním posunem s parametrem směrodatné odchylky \\(\\sigma_{shift}\\), mohli bychom se pokusit odstranit tento posun a klasifikovat data po odstranění tohoto posunu. Viděli jsme totiž v sekci 7, že se zvětšující se hodnotou parametru směrodatné odchylky \\(\\sigma_{shift}\\) se úspěšnost, zejména klasických klasifikačních metod, poměrně dramaticky zhoršuje. Naopak klasifikační metody beroucí do úvahy funkcionální podstatu dat se zpravidla i se zvětšující se hodnotou \\(\\sigma_{shift}\\) chovají poměrně stabilně. Jednou z možností k odstranění vertikálního posunutí, kterou využijeme v následující části, je klasifikovat data na základě odhadu první derivace dané vygenerované a vyhlazené křivky, neboť jak známo \\[ \\frac{\\text d}{\\text d t} \\big( x(t) + c \\big) = \\frac{\\text d}{\\text d t} x(t)= x&#39;(t). \\] 9.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro simulaci nejprve: zvolíme vhodné funkce, generujeme body ze zvoleného intervalu, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 6]\\). Nyní vytvoříme funkce pomocí interpolačních polynomů. Nejprve si definujeme body, kterými má procházet naše křivka, a následně jimi proložíme interpolační polynom, který použijeme pro generování křivek pro klasifikaci. Code # definujici body pro tridu 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # definujici body pro tridu 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Pro výpočet interpolačních polynomů využijeme funkci poly.calc() z knihovny polynom. Dále definujeme funkce poly.0() a poly.1(), které budou počítat hodnoty polynomů v daném bodě intervalu. K jejich vytvoření použijeme funkci predict(), na jejíž vstup zadáme příslušný polynom a bod, ve kterám chceme polynom vyhodnotit. Code # vypocet polynomu polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # vykresleni polynomu xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 3.2: Znázornění dvou funkcí na intervalu \\(I = [0, 6]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce (parametr sigma_shift). Tento posun budeme generovat s normálního rozdělení s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 9.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Opět využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor t, jelikož uvažujeme první derivaci, volíme norder = 5. Budeme penalizovat třetí derivaci funkcí, neboť nyní požadujeme hladké i první derivace. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 5 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(3) # penalizujeme 3. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) Obrázek 5.1: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 5.2: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Obrázek 1.6: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Obrázek 5.3: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 9.3 Výpočet derivací K výpočtu derivace pro funkcionální objekt využijeme v R funkci deriv.fd() z balíčku fda. Jelikož chceme klasifikovat na základě první derivace, volíme argument Lfdobj = 1. Code XXder &lt;- deriv.fd(XXfd, 1) Nyní si vykresleme prvních několik prvních derivací pro obě klasifikační třídy. Všimněme si z obrázku níže, že se opravdu vertikální posun pomocí derivování opravdu podařilo odstranit. Ztratili jsme tím ale do jisté míry rozdílnost mezi křivkami, protože jak z obrázku vyplývá, křivky derivací pro obě třídy se liší primárně až ke konci intervalu, tedy pro argument v rozmezí přibližně \\([5, 6]\\). Code fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;grey3&#39;, linewidth = 0.7, linetype = &#39;dashed&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Obrázek 1.7: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;grey3&#39;, linewidth = 0.7, linetype = &#39;dashed&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1.5, 2)) Obrázek 9.1: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 9.4 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 9.4.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) summary(neighb.model) # shrnuti modelu ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.7464789 0.6376812 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 53 18 ## 2 25 44 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.6143 0.6357 0.6357 0.6000 0.6429 0.6286 0.6357 0.6643 0.6786 0.6857 0.6857 ## 12 ## 0.6929 ## ## -Optimal number of neighbors: knn.opt= 12 ## with highest probability of correct classification max.prob= 0.6928571 ## ## -Probability of correct classification: 0.6929 Code plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K Code neighb.model$max.prob # maximalni presnost ## [1] 0.6928571 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 12 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results ## [1] 0.6527850 0.6398870 0.6691107 0.6308232 0.6556309 0.6512990 0.6556309 ## [8] 0.6772193 0.6875181 0.6799221 0.7086563 0.7174691 0.7338576 0.7406408 ## [15] 0.7239741 0.7266985 0.7220261 0.7106165 0.7129975 0.7183088 0.6819286 ## [22] 0.7020751 0.6656948 0.6732976 Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 14 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.7406. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linestyle = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) ## Warning in geom_line(linestyle = &quot;dashed&quot;, colour = &quot;grey&quot;): Ignoring unknown ## parameters: `linestyle` Obrázek 5.5: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) summary(neighb.model) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.7183099 0.6666667 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 51 20 ## 2 23 46 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 14 ## 0.6929 ## ## -Optimal number of neighbors: knn.opt= 14 ## with highest probability of correct classification max.prob= 0.6928571 ## ## -Probability of correct classification: 0.6929 Code # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost 1 - presnost ## [1] 0.1833333 Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 14, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.3071 a na testovacích datech 0.1833. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 9.4.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 3, které dohromady vysvětlují 93.96 % variability v datech. První hlavní komponenta potom vysvětluje 50.6 % a druhá 33.44 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 2.6: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 3 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (68.57 %), tak i na testovacích datech (76.67 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 9.2: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.4.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (65 %), tak i na testovacích datech (80 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.7: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.4.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 9.4.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 9.3: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 10, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) CV.results ## 4 5 6 7 8 9 10 11 ## 0.6490101 0.8110996 0.8217872 0.8241638 0.8574350 0.9140240 0.9127461 0.9251521 ## 12 13 14 15 16 17 18 19 ## 0.9152450 0.9043956 0.9315667 0.9155411 0.8812271 0.8873666 0.8741551 0.8754722 ## 20 21 22 23 24 25 ## 0.8862554 0.8714900 0.8847026 0.8727723 0.8679267 0.9137971 Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 14 s validační chybovostí 0.0684. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) Obrázek 9.4: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 14 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) ## Warning in Minverse(t(B) %*% B): System is computationally singular (rank 54) ## ## The matrix inverse is computed by svd (effective rank 52) Code # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) ## Warning in Minverse(t(B) %*% B): System is computationally singular (rank 54) ## ## The matrix inverse is computed by svd (effective rank 52) Code predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) ## Warning in Minverse(t(B) %*% B): System is computationally singular (rank 54) ## ## The matrix inverse is computed by svd (effective rank 52) Code predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 5 %) i testovací chybovost (rovna 11.67 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Třída&#39;) Obrázek 1.15: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 1, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Obrázek 1.16: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 1]\\). Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.4.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy přesnost klasifikátoru na trénovacích (68.57 %) i na testovacích datech (76.67 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 5.8: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.4.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 9.4.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [0, 6]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost klasifikátoru na testovacích datech je tedy 81.67 % a na trénovacích datech 73.57 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 5.9: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 5.10: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.4.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 3 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na testovacích datech je tedy 76.67 % a na trénovacích datech 70 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.5: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 2.11: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.4.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 73.57 % a na testovacích datech 81.67 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.6: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 2.12: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.4.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 9.4.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 6]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost náhodného lesu na trénovacích datech je tedy 100 % a na testovacích datech 80 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.4.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 3 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 98.57 % a na testovacích datech 70 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.4.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost tohoto klasifikátoru na trénovacích datech je 100 % a na testovacích datech 81.67 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.4.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 5.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 5.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 5.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 5.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 5.3.7.5. 9.4.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 6]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 86.43 % pro lineární jádro, 78.57 % pro polynomiální jádro a 85 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 81.67 % pro lineární jádro, 81.67 % pro polynomiální jádro a 76.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.4.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 3 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 67.86 % pro lineární jádro, 70.71 % pro polynomiální jádro a 70 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 76.67 % pro lineární jádro, 81.67 % pro polynomiální jádro a 80 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 9.7: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.4.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 85 % pro lineární jádro, 79.29 % pro polynomiální jádro a 84.29 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 81.67 % pro lineární jádro, 81.67 % pro polynomiální jádro a 78.33 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.4.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), # matice diskretnich dat argvals = t.seq, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- t(Projection) # subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 12 0.1638919 ## poly 15 0.2286630 ## radial 10 0.2241529 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 12 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8361, 15 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.7713 a 10 pro radiální jádro s hodnotou přesnosti 0.7758. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 1.23: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), # matice diskretnich dat argvals = t.seq, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 11.43 % pro lineární jádro, 20 % pro polynomiální jádro a 12.86 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 20 % pro lineární jádro, 18.33 % pro polynomiální jádro a 21.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.4.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 9.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 9.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 9.4.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 9.4.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku4 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 9.4.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). 9.4.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 2.5: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0000 0.4833 SVM poly - RKHS 0.0000 0.4000 SVM rbf - RKHS 0.0214 0.3000 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 1, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 1.4: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 17 10.0000 0.1570 linear poly 27 0.3162 0.2073 polynomial radial 16 0.3162 0.1837 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 17 a \\(\\gamma={}\\) 10 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.843, \\(d={}\\) 27 a \\(\\gamma={}\\) 0.3162 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.7927 a \\(d={}\\) 16 a \\(\\gamma={}\\) 0.3162 pro radiální jádro s hodnotou přesnosti 0.8163. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 9.8: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.1500 0.2167 SVM poly - RKHS - radial 0.0786 0.3333 SVM rbf - RKHS - radial 0.1214 0.2833 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 15 % pro lineární jádro, 7.86 % pro polynomiální jádro a 12.14 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 21.67 % pro lineární jádro, 33.33 % pro polynomiální jádro a 28.33 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.4.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 9.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 26 2 0.2780 linear poly 39 2 0.3491 polynomial radial 29 2 0.2979 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 26 a \\(p={}\\) 2 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.722, \\(d={}\\) 39 a \\(p={}\\) 2 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.6509 a \\(d={}\\) 29 a \\(p={}\\) 2 pro radiální jádro s hodnotou přesnosti 0.7021. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 2.8: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.2429 0.4 SVM poly - RKHS - poly 0.1214 0.3 SVM rbf - RKHS - poly 0.1714 0.3 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 24.29 % pro lineární jádro, 12.14 % pro polynomiální jádro a 17.14 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 40 % pro lineární jádro, 30 % pro polynomiální jádro a 30 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.4.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 9.3: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 9 0.3417 linear poly 6 0.3721 polynomial radial 19 0.3257 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 9 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.6583, \\(d={}\\) 6 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.6279 a \\(d={}\\) 19 pro radiální jádro s hodnotou přesnosti 0.6743. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.9: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.3000 0.2833 SVM poly - RKHS - linear 0.3571 0.4833 SVM rbf - RKHS - linear 0.3071 0.2667 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 30 % pro lineární jádro, 35.71 % pro polynomiální jádro a 30.71 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 28.33 % pro lineární jádro, 48.33 % pro polynomiální jádro a 26.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.5 Tabulka výsledků Tabulka 9.4: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3071 0.1833 LDA 0.3143 0.2333 QDA 0.3500 0.2000 LR functional 0.0500 0.1167 LR score 0.3143 0.2333 Tree - diskr. 0.2643 0.1833 Tree - score 0.3000 0.2333 Tree - Bbasis 0.2643 0.1833 RForest - diskr 0.0000 0.2000 RForest - score 0.0143 0.3000 RForest - Bbasis 0.0000 0.1833 SVM linear - diskr 0.1357 0.1833 SVM poly - diskr 0.2143 0.1833 SVM rbf - diskr 0.1500 0.2333 SVM linear - PCA 0.3214 0.2333 SVM poly - PCA 0.2929 0.1833 SVM rbf - PCA 0.3000 0.2000 SVM linear - Bbasis 0.1500 0.1833 SVM poly - Bbasis 0.2071 0.1833 SVM rbf - Bbasis 0.1571 0.2167 SVM linear - projection 0.1143 0.2000 SVM poly - projection 0.2000 0.1833 SVM rbf - projection 0.1286 0.2167 SVM linear - RKHS - radial 0.1500 0.2167 SVM poly - RKHS - radial 0.0786 0.3333 SVM rbf - RKHS - radial 0.1214 0.2833 SVM linear - RKHS - poly 0.2429 0.4000 SVM poly - RKHS - poly 0.1214 0.3000 SVM rbf - RKHS - poly 0.1714 0.3000 SVM linear - RKHS - linear 0.3000 0.2833 SVM poly - RKHS - linear 0.3571 0.4833 SVM rbf - RKHS - linear 0.3071 0.2667 9.6 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 5 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(3) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd # vypocet derivace XXder &lt;- deriv.fd(XXfd, 1) fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- t(Projection) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by =2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_04.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/simulace_04_res.RData&#39;) 9.6.1 Výsledky Tabulka 9.5: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.2277 0.2495 0.0421 0.0616 LDA 0.2295 0.2383 0.0415 0.0645 QDA 0.2234 0.2485 0.0440 0.0620 LR_functional 0.0384 0.1067 0.0273 0.0433 LR_score 0.2284 0.2400 0.0429 0.0635 Tree_discr 0.1823 0.2493 0.0477 0.0701 Tree_score 0.2092 0.2848 0.0461 0.0693 Tree_Bbasis 0.1854 0.2447 0.0542 0.0723 RF_discr 0.0106 0.2323 0.0085 0.0706 RF_score 0.0359 0.2693 0.0179 0.0694 RF_Bbasis 0.0111 0.2315 0.0082 0.0676 SVM linear - diskr 0.1056 0.1378 0.0433 0.0502 SVM poly - diskr 0.1578 0.2488 0.0559 0.0794 SVM rbf - diskr 0.1298 0.2253 0.0489 0.0676 SVM linear - PCA 0.2282 0.2428 0.0435 0.0605 SVM poly - PCA 0.2210 0.2840 0.0505 0.0688 SVM rbf - PCA 0.1916 0.2617 0.0446 0.0661 SVM linear - Bbasis 0.1135 0.1475 0.0465 0.0534 SVM poly - Bbasis 0.1581 0.2475 0.0530 0.0771 SVM rbf - Bbasis 0.1299 0.2207 0.0483 0.0678 SVM linear - projection 0.1107 0.1442 0.0430 0.0525 SVM poly - projection 0.1396 0.2192 0.0548 0.0697 SVM rbf - projection 0.1161 0.1962 0.0485 0.0706 SVM linear - RKHS - radial 0.1034 0.1737 0.0421 0.0601 SVM poly - RKHS - radial 0.1021 0.2300 0.0570 0.0750 SVM rbf - RKHS - radial 0.1039 0.1915 0.0501 0.0665 SVM linear - RKHS - poly 0.2187 0.3337 0.0769 0.0931 SVM poly - RKHS - poly 0.1786 0.3458 0.0767 0.0915 SVM rbf - RKHS - poly 0.2006 0.3260 0.0723 0.0985 SVM linear - RKHS - linear 0.2876 0.3675 0.0874 0.0966 SVM poly - RKHS - linear 0.3162 0.3940 0.0880 0.1052 SVM rbf - RKHS - linear 0.2774 0.3423 0.0757 0.0876 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 9.9: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[test])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 5.15: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 9.6: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 13.0 nharm 3.0 LR_func_n_basis 12.0 SVM_d_Linear 11.0 SVM_d_Poly 8.0 SVM_d_Radial 8.0 SVM_RKHS_radial_gamma1 3.2 SVM_RKHS_radial_gamma2 0.4 SVM_RKHS_radial_gamma3 3.2 SVM_RKHS_radial_d1 19.0 SVM_RKHS_radial_d2 15.0 SVM_RKHS_radial_d3 13.0 SVM_RKHS_poly_p1 3.0 SVM_RKHS_poly_p2 2.0 SVM_RKHS_poly_p3 3.0 SVM_RKHS_poly_d1 20.0 SVM_RKHS_poly_d2 29.0 SVM_RKHS_poly_d3 27.0 SVM_RKHS_linear_d1 12.0 SVM_RKHS_linear_d2 21.0 SVM_RKHS_linear_d3 11.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.10: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.11: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.12: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.13: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.14: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["aplikace1.html", "Kapitola 10 Aplikace na reálných datech 1 10.1 Vyhlazení pozorovaných křivek 10.2 Klasifikace křivek 10.3 Tabulka výsledků 10.4 Simulační studie", " Kapitola 10 Aplikace na reálných datech 1 Ve druhé části tohoto dokumentu se budeme zabývat aplikací dříve popsaných metod (pro více podrobností viz například sekci 1) na reálná data. Budeme pracovat s daty growth, která jsou dostupná v knihovně fda. Jedná se o datový soubor obsahující růstové křivky 39 chlapců a 54 děvčat, přičemž subjekty sledujeme v intervalu jednoho až 18. roku života, a to v celkem 31 časových bodech. Naší úlohou bude klasifikovat růstové křivky podle pohlaví, tedy bude nás zajímat predikce pro novou růstovou křivku, zda se jedná o chlapce, či dívku. Nejprve si data načteme a vykreslíme. Code # nacteni dat library(fda) library(ggplot2) library(dplyr) library(tidyr) data &lt;- growth data.gr &lt;- cbind(data.frame(age = data$age), data$hgtf, data$hgtm) n_girls &lt;- 54 n_boys &lt;- 39 Code gender.labs &lt;- c(&quot;dívky&quot;, &quot;chlapci&quot;) names(gender.labs) &lt;- c(&#39;girl&#39;, &#39;boy&#39;) pivot_longer(data.gr, cols = girl01:boy39, names_to = &#39;sample&#39;, values_to = &#39;height&#39;, cols_vary = &#39;slowest&#39;) |&gt; mutate(Individual = as.factor(sample), Gender = factor(rep(rep(c(&#39;girl&#39;, &#39;boy&#39;), c(n_girls, n_boys)), each = length(data.gr$age)), levels = c(&#39;girl&#39;, &#39;boy&#39;))) |&gt; ggplot(aes(x = age, y = height, colour = Gender, group = Individual)) + geom_line() + theme_bw() + facet_wrap(~Gender, labeller = labeller(Gender = gender.labs)) + labs(x = &quot;Věk [v letech]&quot;, y = &quot;Výška [v cm]&quot;, colour = &quot;Pohlaví&quot;) + theme(legend.position = &#39;none&#39;) + scale_color_discrete(labels = c(&quot;dívky&quot;, &quot;chlapci&quot;)) Obrázek 10.1: Růstové křivky zlášť pro dívky a chlapce. Vidíme, že růstové křivky dívek i chlapců začínají přibližně ve srovnatelných hodnotách, počáteční průběh je podobný, liší se zejména po 13. roku, kde u dívek začínají růstové křivky konvergovat ke konstantní funkci, zatímco u chlapců růst pokračuje dále, přičemž se zastavuje kolem 15. roku. Navíc mezi 13. a 15. rokem u chlapců můžeme vidět, jak křivky ještě zrychlí svůj růst předtím, než dojde k ukončení růstu. Čekali bychom, že tyto znaky budou hrát klíčovou roli při klasifikaci podle pohlaví, neboť v nich se dívky a chlapci liší nejvíce. 10.1 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nejedná o periodické křivky na intervalu \\(I = [1, 18]\\), využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor age, standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code t &lt;- data.gr$age rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě pohlaví stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\) máme v případě rozdílné volby pro každou třídu volit. Code # spojeni pozorovani do jedne matice XX &lt;- data.gr[, -1] |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -5, to = -0.5, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.7) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 3.1: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále při základu 10. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code n &lt;- n_girls + n_boys names(gender.labs) &lt;- c(&#39;girls&#39;, &#39;boys&#39;) DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n_girls], 1, mean), n_girls), rep(apply(fdobjSmootheval[ , (n_girls + 1) : n], 1, mean), n_boys)), Gender = factor(rep(c(&#39;girls&#39;, &#39;boys&#39;), c(n_girls, n_boys) * length(t)), levels = c(&#39;girls&#39;, &#39;boys&#39;)) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n_girls], 1, mean), apply(fdobjSmootheval[ , (n_girls + 1) : n], 1, mean)), group = factor(rep(c(&#39;girls&#39;, &#39;boys&#39;), each = length(t)), levels = c(&#39;girls&#39;, &#39;boys&#39;)) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, color = Gender, group = time)) + geom_line(linewidth = 0.25) + theme_bw() + facet_wrap(~Gender, labeller = labeller(Gender = gender.labs)) + labs(x = &quot;Věk [v letech]&quot;, y = &quot;Výška [v cm]&quot;, colour = &quot;Pohlaví&quot;) + theme(legend.position = &#39;none&#39;) + scale_color_discrete(labels = c(&quot;dívky&quot;, &quot;chlapci&quot;)) + geom_line(aes(x = t, y = Mean), colour = &#39;black&#39;, linewidth = 1, linetype = &#39;twodash&#39;) Obrázek 10.2: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle pohlaví. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. 10.2 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les set.seed(42) Rozdělíme data v poměru 30:70 na testovací a trénovací část, abychom mohli stanovit úspěšnost klasifikace jednotlivých metod. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěšnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast set.seed(42) split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro divky a 1 pro chlapce Y &lt;- rep(c(0, 1), c(n_girls, n_boys)) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 33 32 Code table(Y.test) ## Y.test ## 0 1 ## 21 7 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5076923 0.4923077 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.75 0.25 Vidíme, že v obou množinách (testovací a trénovací) jsou chlapci i dívky zastoupeni přibližně ve stejném poměru, přičemž bereme do úvahy, že chlapců je méně než dívek. Konkrétně dívek je přibližně 58.06 \\(\\%\\) a chlapců 41.94 \\(\\%\\). 10.2.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) neighb.model$max.prob # maximalni presnost ## [1] 0.9538462 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 3 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; t() ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## K 1.0000000 2.0000000 3.0000000 4.0000000 5.0000000 6.0000000 7.0000000 ## CV 0.9071429 0.8946429 0.9514286 0.9314286 0.9514286 0.9028571 0.9414286 ## [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## K 8.0000000 9.0000000 10.0000000 11.0000000 12.0000000 13.0000000 14.0000000 ## CV 0.9071429 0.9271429 0.9371429 0.9389286 0.9246429 0.9289286 0.9289286 ## [,15] [,16] [,17] [,18] ## K 15.0000000 16.0000000 17.0000000 18.0000000 ## CV 0.9246429 0.9271429 0.8946429 0.8971429 Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 3 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9514. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 2.3: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 3, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.0462 a na testovacích datech 0.0357. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 10.2.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p=\\) 2, které dohromady vysvětlují 94.92 \\(\\%\\) variability v datech. První hlavní komponenta potom vysvětluje 81.34 % a druhá 13.58 \\(\\%\\) variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Pohlaví&#39;) + scale_color_discrete(labels = c(&quot;dívky&quot;, &quot;chlapci&quot;)) + theme_bw() Obrázek 1.6: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (98.46 %), tak i na testovacích datech (96.43 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Pohlaví&#39;) + scale_color_discrete(labels = c(&quot;dívky&quot;, &quot;chlapci&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 2.4: Skóre prvních dvou hlavních komponent, barevně odlišené podle pohlaví. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (98.46 %), tak i na testovacích datech (96.43 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Pohlaví&#39;) + scale_color_discrete(labels = c(&quot;dívky&quot;, &quot;chlapci&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.5: Skóre prvních dvou hlavních komponent, barevně odlišené podle pohlaví. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola, avšak se jen (alespoň opticky) velmi málo liší od přímky. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 10.2.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; nbasis.x &lt;- 15 # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 1.9: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 4, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 4 s validační chybovostí 0.0688. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) Obrázek 2.6: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 4 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b, maxit = 1000, epsilon = 1e-2) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 3.08 %) i testovací chybovost (rovna 10.71 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_color_discrete(labels = c(&quot;dívky&quot;, &quot;chlapci&quot;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Pohlaví&#39;) Obrázek 10.3: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(min(t), max(t), length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line() + theme_bw() + labs(x = expression(x[1]), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Obrázek 9.2: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 6]\\). Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro časy \\(t\\) z prostředka a začátku intervalu \\([1, 18]\\), zatímco pro pozdějsí časy jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd na začátku a konci intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy přesnost klasifikátoru na trénovacích (98.46 %) i na testovacích datech (96.43 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.7: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 10.2.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [1, 18]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost klasifikátoru na testovacích datech je tedy 92.86 % a na trénovacích datech 86.15 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.3: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.14: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 2 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na testovacích datech je tedy 89.29 % a na trénovacích datech 90.77 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 10.4: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.15: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 86.15 % a na testovacích datech 92.86 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.17: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.18: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 10.2.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [1, 18]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost náhodného lesu na trénovacích datech je tedy 98.46 % a na testovacích datech 96.43 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 98.46 % a na testovacích datech 96.43 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost tohoto klasifikátoru na trénovacích datech je 98.46 % a na testovacích datech 96.43 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 10.2.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 10.2.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 10.2.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 10.2.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 10.2.7.5. 10.2.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [1, 18]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 98.46 % pro lineární jádro, 87.69 % pro polynomiální jádro a 98.46 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 96.43 % pro lineární jádro, 92.86 % pro polynomiální jádro a 96.43 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 10.2.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 98.46 % pro lineární jádro, 95.38 % pro polynomiální jádro a 98.46 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 96.43 % pro lineární jádro, 100 % pro polynomiální jádro a 96.43 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) # + Obrázek 10.5: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code # geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), # colour = &#39;black&#39;) + # geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), # colour = &#39;black&#39;) Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 10.2.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 98.46 % pro lineární jádro, 86.15 % pro polynomiální jádro a 98.46 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 96.43 % pro lineární jádro, 82.14 % pro polynomiální jádro a 96.43 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 10.2.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 8 0.01428571 ## poly 7 0.02857143 ## radial 23 0.03428571 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 8 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9857, 7 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9714 a 23 pro radiální jádro s hodnotou přesnosti 0.9657. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &#39;Jádro&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 1.21: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 1.54 % pro lineární jádro, 1.54 % pro polynomiální jádro a 1.54 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 3.57 % pro lineární jádro, 3.57 % pro polynomiální jádro a 3.57 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 10.2.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 10.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 10.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 10.2.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 10.2.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku5 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 10.2.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). Code # hodnoty z clanku # A. Muñoz, J. González / Pattern Recognition Letters 31 (2010) 511–516 eps &lt;- 0.01 C &lt;- 1 # 100 dava spatne vysledky 10.2.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, cost = C, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 10.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0000 0.2500 SVM poly - RKHS 0.0000 0.5000 SVM rbf - RKHS 0.0154 0.2143 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:30 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 2.3: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 21 3.7276 0.0668 linear poly 3 3.7276 0.0486 polynomial radial 3 0.0373 0.0611 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 21 a \\(\\gamma={}\\) 3.7276 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9332, \\(d={}\\) 3 a \\(\\gamma={}\\) 3.7276 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9514 a \\(d={}\\) 3 a \\(\\gamma={}\\) 0.0373 pro radiální jádro s hodnotou přesnosti 0.9389. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) ## Warning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Spectral is 11 ## Returning the palette you asked for with that many colors Obrázek 5.13: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.2: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0000 0.2500 SVM poly - RKHS - radial 0.0615 0.0714 SVM rbf - RKHS - radial 0.0615 0.1071 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 0 % pro lineární jádro, 6.15 % pro polynomiální jádro a 6.15 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 25 % pro lineární jádro, 7.14 % pro polynomiální jádro a 10.71 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 10.2.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 2:30 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, coef0 = 1, cost = C, epsilon = eps, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, gamma = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 5.1: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 13 3 0.0577 linear poly 9 2 0.0595 polynomial radial 2 2 0.0863 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 13 a \\(p={}\\) 3 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9423, \\(d={}\\) 9 a \\(p={}\\) 2 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9405 a \\(d={}\\) 2 a \\(p={}\\) 2 pro radiální jádro s hodnotou přesnosti 0.9137. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, cost = C, gamma = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = C, gamma = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 5.2: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.0615 0.1071 SVM poly - RKHS - poly 0.0000 0.0357 SVM rbf - RKHS - poly 0.0769 0.0714 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 6.15 % pro lineární jádro, 0 % pro polynomiální jádro a 7.69 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 10.71 % pro lineární jádro, 3.57 % pro polynomiální jádro a 7.14 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 10.2.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 2:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 10.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 16 0.1087 linear poly 19 0.0736 polynomial radial 9 0.1079 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 16 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8913, \\(d={}\\) 19 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9264 a \\(d={}\\) 9 pro radiální jádro s hodnotou přesnosti 0.8921. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.0615 0.0000 SVM poly - RKHS - linear 0.0000 0.0357 SVM rbf - RKHS - linear 0.0000 0.2500 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 6.15 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 0 % pro lineární jádro, 3.57 % pro polynomiální jádro a 25 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 10.3 Tabulka výsledků Tabulka 10.3: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.0462 0.0357 LDA 0.0154 0.0357 QDA 0.0154 0.0357 LR functional 0.0308 0.1071 LR score 0.0154 0.0357 Tree - diskr. 0.1385 0.0714 Tree - score 0.0923 0.1071 Tree - Bbasis 0.1385 0.0714 RForest - diskr 0.0154 0.0357 RForest - score 0.0154 0.0357 RForest - Bbasis 0.0154 0.0357 SVM linear - diskr 0.0154 0.0357 SVM poly - diskr 0.1231 0.0714 SVM rbf - diskr 0.0154 0.0357 SVM linear - PCA 0.0154 0.0357 SVM poly - PCA 0.0462 0.0000 SVM rbf - PCA 0.0154 0.0357 SVM linear - Bbasis 0.0154 0.0357 SVM poly - Bbasis 0.1385 0.1786 SVM rbf - Bbasis 0.0154 0.0357 SVM linear - projection 0.0154 0.0357 SVM poly - projection 0.0154 0.0357 SVM rbf - projection 0.0154 0.0357 SVM linear - RKHS - radial 0.0000 0.2500 SVM poly - RKHS - radial 0.0615 0.0714 SVM rbf - RKHS - radial 0.0615 0.1071 SVM linear - RKHS - poly 0.0615 0.1071 SVM poly - RKHS - poly 0.0000 0.0357 SVM rbf - RKHS - poly 0.0769 0.0714 SVM linear - RKHS - linear 0.0615 0.0000 SVM poly - RKHS - linear 0.0000 0.0357 SVM rbf - RKHS - linear 0.0000 0.2500 10.4 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), c(n_girls, n_boys)) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze nbasis.x &lt;- 15 basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(1, 18, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS C &lt;- 1 eps &lt;- 0.01 ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 30, by =2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 10, by = 1) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/aplikace_01.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/aplikace_01_res.RData&#39;) 10.4.1 Výsledky Tabulka 10.4: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.0633 0.0545 0.0246 0.0344 LDA 0.0267 0.0121 0.0086 0.0156 QDA 0.0383 0.0152 0.0158 0.0214 LR_functional 0.0350 0.0727 0.0200 0.0383 LR_score 0.0383 0.0152 0.0112 0.0214 Tree_discr 0.1083 0.1364 0.0239 0.0674 Tree_score 0.0800 0.1485 0.0367 0.0579 Tree_Bbasis 0.1083 0.1364 0.0239 0.0674 RF_discr 0.0200 0.0879 0.0131 0.0614 RF_score 0.0233 0.0455 0.0141 0.0410 RF_Bbasis 0.0183 0.0818 0.0095 0.0590 SVM linear - diskr 0.0300 0.0333 0.0131 0.0265 SVM poly - diskr 0.2200 0.2636 0.0367 0.0881 SVM rbf - diskr 0.0317 0.0424 0.0095 0.0326 SVM linear - PCA 0.0300 0.0182 0.0131 0.0156 SVM poly - PCA 0.0733 0.0667 0.0196 0.0424 SVM rbf - PCA 0.0283 0.0121 0.0112 0.0156 SVM linear - Bbasis 0.0267 0.0212 0.0086 0.0205 SVM poly - Bbasis 0.1783 0.2091 0.0561 0.1035 SVM rbf - Bbasis 0.0250 0.0242 0.0088 0.0192 SVM linear - projection 0.0300 0.0273 0.0131 0.0224 SVM poly - projection 0.0617 0.0667 0.0343 0.0511 SVM rbf - projection 0.0300 0.0364 0.0131 0.0192 SVM linear - RKHS - radial 0.0400 0.1030 0.0251 0.0717 SVM poly - RKHS - radial 0.0500 0.0606 0.0222 0.0623 SVM rbf - RKHS - radial 0.0433 0.0848 0.0161 0.0636 SVM linear - RKHS - poly 0.0767 0.0848 0.0211 0.0682 SVM poly - RKHS - poly 0.0350 0.1030 0.0266 0.0499 SVM rbf - RKHS - poly 0.0467 0.1182 0.0367 0.0614 SVM linear - RKHS - linear 0.0400 0.1333 0.0353 0.0703 SVM poly - RKHS - linear 0.0350 0.1303 0.0200 0.0893 SVM rbf - RKHS - linear 0.0350 0.1000 0.0200 0.0554 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 10.6: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[test])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 10.7: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 5.4: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 4 nharm 2 LR_func_n_basis 6 SVM_d_Linear 6 SVM_d_Poly 6 SVM_d_Radial 6 SVM_RKHS_radial_gamma1 0 SVM_RKHS_radial_gamma2 0 SVM_RKHS_radial_gamma3 0 SVM_RKHS_radial_d1 10 SVM_RKHS_radial_d2 12 SVM_RKHS_radial_d3 10 SVM_RKHS_poly_p1 3 SVM_RKHS_poly_p2 3 SVM_RKHS_poly_p3 2 SVM_RKHS_poly_d1 5 SVM_RKHS_poly_d2 5 SVM_RKHS_poly_d3 4 SVM_RKHS_linear_d1 23 SVM_RKHS_linear_d2 23 SVM_RKHS_linear_d3 25 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 2.18: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 2.19: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.26: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.27: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.28: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["aplikace2.html", "Kapitola 11 Aplikace na reálných datech 2 11.1 Vyhlazení pozorovaných křivek 11.2 Klasifikace křivek 11.3 Tabulka výsledků 11.4 Klasifikace dalších fonémů", " Kapitola 11 Aplikace na reálných datech 2 V této části dokumentu se budeme zabývat aplikací dříve popsaných metod (pro více podrobností viz například sekci 1) na reálná data phoneme, která jsou dostupná například na této adrese. Podrobný popis dat pak můžeme nalézt zde. Jedná se o datový soubor obsahující log-periodogramy (zvukový záznam řeči) celkem 50 mužů, přičemž od každého jedince máme k dispozici několik zvukových záznamů. V datovém souboru rozlišujeme celkem pět různých hlásek – aa, ao, dcl, iy, sh. Naší úlohou bude klasifikovat log-periodogramy do těchto skupin, tedy bude nás zajímat predikce pro nový log-periodogram. Nejprve si data načteme z textového souboru phoneme.txt a upravíme datový soubor tak, aby byl vhodný pro další analýzu. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(tidyverse) set.seed(42) # nacteni dat data &lt;- read.delim2(&#39;phoneme.txt&#39;, header = T, sep = &#39;,&#39;) # zmenime dve promenne na typ factor data &lt;- data |&gt; mutate(g = factor(g), speaker = factor(speaker)) # numericke promenne prevedeme opravdu na numericke data[, 2:257] &lt;- as.numeric(data[, 2:257] |&gt; as.matrix()) Podíváme se na popisné statistiky faktorových proměnných, tedy zejména nás zajímají absolutní a relativní četnosti hlásek v datovém souboru. Code data.frame(phoneme = table(data$g) |&gt; names(), count = as.numeric(table(data$g)), proportion = as.numeric(table(data$g) / sum(table(data$g)))) ## phoneme count proportion ## 1 aa 695 0.1541362 ## 2 ao 1022 0.2266578 ## 3 dcl 757 0.1678864 ## 4 iy 1163 0.2579286 ## 5 sh 872 0.1933910 V posledním sloupci jsou uvedeny názvy jednotlivých záznamů, přičemž kromě dalších obsahují i informaci o tom, zda se jedná o trénovací (označení train) nebo testovací (označení test) pozorování. To má pro nás klíčovou roli, neboť podle tohoto označení rozdělíme záznamy na testovací a trénovací množinu. Code tr_vs_test &lt;- str_split(data$speaker, &#39;\\\\.&#39;) |&gt; unlist() tr_vs_test &lt;- tr_vs_test[seq(1, length(tr_vs_test), by = 4)] data$train &lt;- ifelse(tr_vs_test == &#39;train&#39;, TRUE, FALSE) data.frame(count = tr_vs_test |&gt; factor() |&gt; summary(), proportion = tr_vs_test |&gt; factor() |&gt; summary() / length(tr_vs_test)) ## count proportion ## test 1169 0.2592593 ## train 3340 0.7407407 Vykresleme si pro lepší představu log-periodogramy jednotlivých fonémů. Z obrázku níže můžeme vidět, že nejvíce si podobné co do průběhu jsou log-periodogramy pro fonémy aa a ao. Vybereme si pro klasifikaci tyto dva fonémy a naším cílem bude porovnat metody popsané v předchozích sekcích z hlediska úspěšnosti klasifikace. Code n_plot &lt;- 5 data[c(1, 2, 3, 5, 9), ] |&gt; pivot_longer(cols = x.1:x.256, names_to = &#39;time&#39;, values_to = &#39;value&#39;) |&gt; mutate(time = rep(1:256, n_plot)) |&gt; ggplot(aes(x = time, y = value, colour = g, group = interaction(speaker, g))) + geom_line() + theme_bw() + facet_grid(~ g) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) + theme(aspect.ratio = 1, legend.position = &quot;none&quot;) Obrázek 3.1: Log-periodogramy fonémů pro vybrané záznamy. Code # ggsave(&#39;figures/app_02_log_periodogramy.pdf&#39;, width = 12, height = 5) Vytvořme si ještě vhodné proměnné v R, do kterých uložíme trénovací a testovací záznamy a také informaci o druhu fonému. Code # nastaveni generatoru cisel set.seed(42) # pocet trenovacich a testovacich dat # n_train &lt;- 500 # n_test &lt;- 300 # vybrane fonemy ke klasifikaci phoneme_subset &lt;- c(&#39;aa&#39;, &#39;ao&#39;) # testovaci a trenovaci data data_train &lt;- data |&gt; filter(train) |&gt; filter(g %in% phoneme_subset) data_test &lt;- data |&gt; filter(!train) |&gt; filter(g %in% phoneme_subset) # omezime se pouze na nejakou podmnozinu # data_train &lt;- data_train[sample(1:dim(data_train)[1], n_train), ] # data_test &lt;- data_test[sample(1:dim(data_test)[1], n_test), ] # odstranime sloupce, ktere nenesou informaci o frekvenci a # transponujeme tak, aby ve sloupcich byly jednotlive zaznamy X_train &lt;- data_train[, -c(1, 258, 259, 260)] |&gt; t() X_test &lt;- data_test[, -c(1, 258, 259, 260)] |&gt; t() # prejmenujeme radky a sloupce rownames(X_train) &lt;- 1:256 colnames(X_train) &lt;- paste0(&#39;train&#39;, data_train$row.names) rownames(X_test) &lt;- 1:256 colnames(X_test) &lt;- paste0(&#39;test&#39;, data_test$row.names) # definujeme vektor fonemu y_train &lt;- data_train[, 258] |&gt; factor(levels = phoneme_subset) y_test &lt;- data_test[, 258] |&gt; factor(levels = phoneme_subset) # absolutni cetnosti data.frame(phoneme = phoneme_subset, train = table(y_train) |&gt; as.numeric(), test = table(y_test) |&gt; as.numeric()) ## phoneme train test ## 1 aa 519 176 ## 2 ao 759 263 11.1 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nejedná o periodické křivky, využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor frekvencí (1 až 256 Hz), standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = 1, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 1.3: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Ještě znázorněme křivky včetně průměru zvlášť pro každou třídu. Code library(tikzDevice) n &lt;- dim(XX)[2] y &lt;- c(y_train, y_test) DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Phoneme = rep(y, each = length(t))) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(eval.fd(fdobj = mean.fd(XXfd[y == phoneme_subset[1]]), evalarg = t), eval.fd(fdobj = mean.fd(XXfd[y == phoneme_subset[2]]), evalarg = t)), Phoneme = factor(rep(phoneme_subset, each = length(t)), levels = levels(y)) ) DFsmooth |&gt; filter(time %in% as.character(1:50)) |&gt; ggplot(aes(x = t, y = Smooth, color = Phoneme)) + geom_line(aes(group = time), linewidth = 0.05, alpha = 0.7) + theme_bw() + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Phoneme&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean, group = Phoneme), linewidth = 1.25, linetype = &#39;solid&#39;, colour = &#39;grey2&#39;) + facet_wrap(~Phoneme) + theme(legend.position = &#39;none&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), labels = phoneme_subset) Obrázek 1.5: Vykreslení prvních 100 vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čarou je zakreslen průměr pro každou třídu. Code # ggsave(&quot;figures/kap7_phoneme_curves_mean.tex&quot;, device = tikz, width = 9, height = 4.5) Nakonec této podkapitoly si ještě vykresleme vyhlazené křivky pro vybrané záznamy z grafu pro všechny fonémy výše. Code n_plot &lt;- 5 data[c(1, 2, 3, 5, 9), ] |&gt; pivot_longer(cols = x.1:x.256, names_to = &#39;time&#39;, values_to = &#39;value&#39;) |&gt; mutate(time = rep(1:256, n_plot)) |&gt; filter(g %in% phoneme_subset) |&gt; ggplot(aes(x = time, y = value, colour = g, group = interaction(speaker, g))) + geom_line(alpha = 0.4) + theme_bw() + facet_grid(~ g) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) + theme(aspect.ratio = 1, legend.position = &quot;none&quot;) + geom_line( data = DFsmooth |&gt; filter(time %in% as.character(c(1, 3))) |&gt; mutate(g = Phoneme), aes(x = t, y = Smooth, color = g, group = time), linewidth = 0.8 ) Obrázek 2.1: Log-periodogramy fonémů pro vybrané záznamy. Code ggsave(&#39;figures/app_02_log_periodogramy_plus_smooth.pdf&#39;, width = 10, height = 5) 11.2 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Code # rozdeleni na testovaci a trenovaci cast split &lt;- ifelse(substr(XXfd$fdnames$reps, 1, 2) == &#39;tr&#39;, TRUE, FALSE) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(y, split == TRUE) Y.test &lt;- subset(y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## aa ao ## 519 759 Code table(Y.test) ## Y.test ## aa ao ## 176 263 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## aa ao ## 0.4061033 0.5938967 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## aa ao ## 0.4009112 0.5990888 11.2.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme hodnotu \\(k = {5}\\), neboť výpočet je nyní velmi časově náročný. Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) summary(neighb.model) # shrnuti modelu ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.6782274 0.8853755 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 352 167 ## 2 87 672 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.7363 0.7113 0.7574 0.7480 0.7668 0.7746 0.7833 0.7786 0.7879 0.7895 0.7895 ## 12 13 14 15 16 17 18 19 20 21 22 ## 0.7926 0.7934 0.7864 0.7848 0.7903 0.7887 0.7848 0.7911 0.7919 0.7950 0.8013 ## 23 24 25 26 27 28 29 30 31 32 33 ## 0.7981 0.7958 0.7934 0.7919 0.7926 0.7950 0.7950 0.7911 0.7887 0.7903 0.7903 ## 34 35 36 ## 0.7911 0.7934 0.7911 ## ## -Optimal number of neighbors: knn.opt= 22 ## with highest probability of correct classification max.prob= 0.801252 ## ## -Probability of correct classification: 0.8013 Code plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K Code neighb.model$max.prob # maximalni presnost ## [1] 0.801252 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 22 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 5 # k-fold CV neighbours &lt;- seq(2, 35, by = 5) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[(1:length(neighbours))[neighbour == neighbours], index] &lt;- presnost #cat(&#39;\\r&#39;, paste0(index, &#39;: &#39;, neighbour)) } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- neighbours[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 22 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.798. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 5.3: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) summary(neighb.model) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.6782274 0.8853755 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 352 167 ## 2 87 672 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 22 ## 0.8013 ## ## -Optimal number of neighbors: knn.opt= 22 ## with highest probability of correct classification max.prob= 0.801252 ## ## -Probability of correct classification: 0.8013 Code # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 22, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.1987 a na testovacích datech 0.2073. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 11.2.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 9, které dohromady vysvětlují 90.26 % variability v datech. První hlavní komponenta potom vysvětluje 44.16 % a druhá 13.79 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() Obrázek 9.1: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 9 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (80.05 %), tak i na testovacích datech (79.04 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pokud p = 9 if(dim(data.PCA.train)[2] == 10) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1], V6 = data.PCA.train$V5[1], V7 = data.PCA.train$V5[1], V8 = data.PCA.train$V5[1], V9 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 5.4: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{\\texttt{aa}, \\texttt{ao}\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (79.81 %), tak i na testovacích datech (79.04 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 11.1: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 11.2.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) - 1 colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze nbasis.x &lt;- 100 basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 10.3: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 14, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() - 1 x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() - 1 dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 20 s validační chybovostí 0.1719. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) Obrázek 5.6: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 20 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train - 1) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 16.2 %) i testovací chybovost (rovna 19.36 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;, linewidth = 0.3) + geom_point(aes(x = linear.predictor, y = response + rnorm(length(response), sd = 0.02), colour = Y), size = 0.5, alpha = 0.75) + geom_line(aes(x = linear.predictor, y = response), colour = &#39;grey2&#39;) + # scale_colour_discrete(labels = phoneme_subset) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Foném&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), labels = phoneme_subset) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = c(0.85, 0.18)) + scale_y_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1)#, # labels = c(&#39;0&#39;, &#39;0.25&#39;, &#39;0.5&#39;, &#39;0.75&#39;, &#39;1&#39;) ) + scale_x_continuous(breaks = c(-6, -3, 0, 3, 6)) Obrázek 1.13: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Code # library(tikzDevice) # ggsave(&quot;figures/DP_kap3_linearpredictor.tex&quot;, width = 4, height = 3.5, device = tikz) Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(min(tt), max(tt), length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line(colour = &#39;deepskyblue2&#39;, linewidth = 0.8) + theme_bw() + labs(x = expression(t), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Obrázek 11.2: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [1, 256]\\). Code # ggsave(&quot;figures/DP_kap3_betahat.tex&quot;, width = 4, height = 3.5, # device = tikz) Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro frekvence \\(t\\) z prostředka a konce intervalu \\([1, 256]\\), zatímco pro počáteční frekvence jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd zejména na začátku intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy přesnost klasifikátoru na trénovacích (79.97 %) i na testovacích datech (78.82 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.14: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 11.2.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [1, 256]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(1, 256, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost klasifikátoru na testovacích datech je tedy 78.59 % a na trénovacích datech 82.24 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.15: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.16: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 9 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na testovacích datech je tedy 68.79 % a na trénovacích datech 74.73 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.17: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.18: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 84.12 % a na testovacích datech 77.9 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.19: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.20: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 11.2.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [1, 256]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost náhodného lesu na trénovacích datech je tedy 99.77 % a na testovacích datech 80.18 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 9 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 99.69 % a na testovacích datech 77.68 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost tohoto klasifikátoru na trénovacích datech je 99.92 % a na testovacích datech 81.09 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 11.2.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 11.2.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 11.2.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 11.2.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 11.2.7.5. 11.2.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [1, 256]\\)), přičemž budeme uvažovat všechny tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(BSmooth$fd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast split_norm &lt;- ifelse(substr(XXfd_norm$fdnames$reps, 1, 2) == &#39;tr&#39;, TRUE, FALSE) X.train_norm &lt;- subset(XXfd_norm, split_norm == TRUE) X.test_norm &lt;- subset(XXfd_norm, split_norm == FALSE) Y.train_norm &lt;- subset(y, split_norm == TRUE) Y.test_norm &lt;- subset(y, split_norm == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Nyní se pokusme, na rozdíl od postupu v předchozích kapitolách, hyperparametry klasifikátorů odhadnout z dat pomocí 10-násobné cross-validace. Vzhledem k tomu, že každé jádro má ve své definici jiné hyperparametry, budeme ke každé jádrové funkci přistupovat zvlášť. Nicméně hyperparametr \\(C\\) vystupuje u všech jádrových funkcí, přičemž ale připouštíme, že se může jeho optimální hodnota mezi jádry lišit. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split_norm), k = k_cv, time = 1) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 10) C.cv &lt;- 10^seq(-3, 3, length = 10) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.1, pro polynomiální jádro je \\(C\\) rovno 0.1 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 2.1544 a pro \\(\\gamma\\) je to 0.001. Validační přesnosti jsou postupně 0.8348794 pro lineární, 0.8153297 pro polynomiální a 0.8270669 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 84.3505 % pro lineární jádro, 86.072 % pro polynomiální jádro a 83.5681 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 79.9544 % pro lineární jádro, 81.0934 % pro polynomiální jádro a 81.0934 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 11.2.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 9 hlavních komponent. Nyní se pokusme, na rozdíl od postupu v předchozích kapitolách, hyperparametry klasifikátorů odhadnout z dat pomocí 10-násobné cross-validace. Vzhledem k tomu, že každé jádro má ve své definici jiné hyperparametry, budeme ke každé jádrové funkci přistupovat zvlášť. Nicméně hyperparametr \\(C\\) vystupuje u všech jádrových funkcí, přičemž ale připouštíme, že se může jeho optimální hodnota mezi jádry lišit. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 10) C.cv &lt;- 10^seq(-3, 3, length = 10) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.0046, pro polynomiální jádro je \\(C\\) rovno 0.1 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 1000 a pro \\(\\gamma\\) je to 0.001. Validační přesnosti jsou postupně 0.7965367 pro lineární, 0.7934178 pro polynomiální a 0.8012365 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 80.05 % pro lineární jádro, 82.86 % pro polynomiální jádro a 81.61 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 79.2711 % pro lineární jádro, 81.3212 % pro polynomiální jádro a 80.1822 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 11.3: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 11.2.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 10) C.cv &lt;- 10^seq(-3, 3, length = 10) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.1, pro polynomiální jádro je \\(C\\) rovno 0.1 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 0.4642 a pro \\(\\gamma\\) je to 0.001. Validační přesnosti jsou postupně 0.8372109 pro lineární, 0.8113989 pro polynomiální a 0.8168861 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 84.74 % pro lineární jádro, 84.35 % pro polynomiální jádro a 82.79 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 80.41 % pro lineární jádro, 80.8656 % pro polynomiální jádro a 80.8656 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 11.2.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 30. Code set.seed(42) k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- 32 # norder n_basis_max &lt;- 50 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # hodnoty hyperparametru pro jadrove funkce C &lt;- 1 p &lt;- 3 gamma &lt;- 0.001 # 1/ncol(data.projection.train) # # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 40 0.1643270 ## poly 43 0.1925320 ## radial 38 0.1690453 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 40 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8357, 43 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8075 a 38 pro radiální jádro s hodnotou přesnosti 0.831. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &#39;Jádro&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 2.13: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, degree = p, coef0 = coef0, gamma = gamma, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 15.1 % pro lineární jádro, 16.12 % pro polynomiální jádro a 16.12 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 19.59 % pro lineární jádro, 18.45 % pro polynomiální jádro a 17.54 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 11.2.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 11.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 11.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 11.2.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 11.2.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku6 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 11.2.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). Code C &lt;- 1 eps &lt;- 0.1 11.2.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.001 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 11.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.2872 0.3713 SVM poly - RKHS 0.0344 0.3622 SVM rbf - RKHS 0.1354 0.3417 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 25, by = 4) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 1, length = 7) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), (1:length(dimensions))[dimensions == d.RKHS], index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), (1:length(dimensions))[dimensions == d.RKHS], index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), (1:length(dimensions))[dimensions == d.RKHS], index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 5.1: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 15 0.3162 0.1737 linear poly 19 3.1623 0.1924 polynomial radial 23 10.0000 0.1862 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 15 a \\(\\gamma={}\\) 0.3162 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8263, \\(d={}\\) 19 a \\(\\gamma={}\\) 3.1623 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8076 a \\(d={}\\) 23 a \\(\\gamma={}\\) 10 pro radiální jádro s hodnotou přesnosti 0.8138. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 11.4: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 2.5: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.1714 0.2187 SVM poly - RKHS - radial 0.0736 0.2255 SVM rbf - RKHS - radial 0.1150 0.2073 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 17.14 % pro lineární jádro, 7.36 % pro polynomiální jádro a 11.5 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 21.87 % pro lineární jádro, 22.55 % pro polynomiální jádro a 20.73 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 11.2.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 5) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, cost = C, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), (1:length(dimensions))[dimensions == d.RKHS], index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), (1:length(dimensions))[dimensions == d.RKHS], index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), (1:length(dimensions))[dimensions == d.RKHS], index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 2.6: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 33 2 0.1674 linear poly 13 3 0.1956 polynomial radial 28 2 0.1792 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 33 a \\(p={}\\) 2 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8326, \\(d={}\\) 13 a \\(p={}\\) 3 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8044 a \\(d={}\\) 28 a \\(p={}\\) 2 pro radiální jádro s hodnotou přesnosti 0.8208. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, cost = C, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.5: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.1612 0.2323 SVM poly - RKHS - poly 0.1432 0.2392 SVM rbf - RKHS - poly 0.1236 0.2164 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 16.12 % pro lineární jádro, 14.32 % pro polynomiální jádro a 12.36 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 23.23 % pro lineární jádro, 23.92 % pro polynomiální jádro a 21.64 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 11.2.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 1.7: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 39 0.1870 linear poly 40 0.2066 polynomial radial 37 0.1941 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 39 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.813, \\(d={}\\) 40 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.7934 a \\(d={}\\) 37 pro radiální jádro s hodnotou přesnosti 0.8059. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 5.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.1784 0.2255 SVM poly - RKHS - linear 0.0696 0.2346 SVM rbf - RKHS - linear 0.1291 0.2255 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 17.84 % pro lineární jádro, 6.96 % pro polynomiální jádro a 12.91 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 22.55 % pro lineární jádro, 23.46 % pro polynomiální jádro a 22.55 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 11.3 Tabulka výsledků Nakonec se ještě podívejme na celkové výsledky na trénovacích a zejména na testovacích datech. Vidíme z tabulky níže, že nejlepší metodou pro klasifikaci je projekce na B-splinovou bázi v kombinaci s metodou SVM s radiálním jádrem. Dobře si také vede klasifikátor Random Forest na diskretizovaných datech a také funkcionální logistická regrese. Také SVM aplikované na diskretizovaná data se chová velmi úspěšně, a to bez ohledu na zvolenou jádrovou funkci. Naopak klasifikátor kombinující projekci na RKHS a SVM pro klasifikaci není na tato data nejlepší volbou, nicméně při volbě radiálního jádra jak pro projekci, tak pro klasifikaci vychází stejné výsledky jako pro LDA a QDA na skórech hlavních komponent. Doplňme, že v tomto případě využijí analýzy hlavních komponent pro klasifikaci není optimální volbou, neboť k vysvětlení 90 % variability v datech je potřebných alespoň 9 hlavních komponent, což je již poměrně hodně. Tabulka 1.8: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.1987 0.2073 LDA 0.1995 0.2096 QDA 0.2019 0.2096 LR functional 0.1620 0.1936 LR score 0.2003 0.2118 Tree - diskr. 0.1776 0.2141 Tree - score 0.2527 0.3121 Tree - Bbasis 0.1588 0.2210 RForest - diskr 0.0023 0.1982 RForest - score 0.0031 0.2232 RForest - Bbasis 0.0008 0.1891 SVM linear - diskr 0.1565 0.2005 SVM poly - diskr 0.1393 0.1891 SVM rbf - diskr 0.1643 0.1891 SVM linear - PCA 0.1995 0.2073 SVM poly - PCA 0.1714 0.1868 SVM rbf - PCA 0.1839 0.1982 SVM linear - Bbasis 0.1526 0.1959 SVM poly - Bbasis 0.1565 0.1913 SVM rbf - Bbasis 0.1721 0.1913 SVM linear - projection 0.1510 0.1959 SVM poly - projection 0.1612 0.1845 SVM rbf - projection 0.1612 0.1754 SVM linear - RKHS - radial 0.1714 0.2187 SVM poly - RKHS - radial 0.0736 0.2255 SVM rbf - RKHS - radial 0.1150 0.2073 SVM linear - RKHS - poly 0.1612 0.2323 SVM poly - RKHS - poly 0.1432 0.2392 SVM rbf - RKHS - poly 0.1236 0.2164 SVM linear - RKHS - linear 0.1784 0.2255 SVM poly - RKHS - linear 0.0696 0.2346 SVM rbf - RKHS - linear 0.1291 0.2255 Doplňme, že jelikož nyní máme dataset již rozdělený na testovací a trénovací data, nemá velký význam provádět celý postup několikrát, tak jako jsme to dělali například v předchozí Kapitole 10.4. Proto se spokojíme s výsledky uvedenými v tabulce výše. 11.4 Klasifikace dalších fonémů Doposud jsme se zabývali klasifikací dvou fonémů, a to aa a ao, neboť ty si byly nejpodobnější a tudíž výsledky klasifikace nejzajímavější. Podívejme se nyní na jinou možnou volbu dvou fonémů a zjistěme, jak si jednotlivé klasifikační metody povedou mezi sebou. Vzhledem k výpočetní náročnosti některých metod budeme uvažovat pouze vybrané metody. Vyjdeme z předchozí situace a zvolíme ty metody, které pro předchozí volbu fonémů dávaly nejlepší výsledky. 11.4.1 ao proti dcl Code # nastaveni generatoru cisel set.seed(42) # pocet trenovacich a testovacich dat # n_train &lt;- 500 # n_test &lt;- 300 # vybrane fonemy ke klasifikaci phoneme_subset &lt;- c(&#39;dcl&#39;, &#39;ao&#39;) # testovaci a trenovaci data data_train &lt;- data |&gt; filter(train) |&gt; filter(g %in% phoneme_subset) data_test &lt;- data |&gt; filter(!train) |&gt; filter(g %in% phoneme_subset) # omezime se pouze na nejakou podmnozinu # data_train &lt;- data_train[sample(1:dim(data_train)[1], n_train), ] # data_test &lt;- data_test[sample(1:dim(data_test)[1], n_test), ] # odstranime sloupce, ktere nenesou informaci o frekvenci a # transponujeme tak, aby ve sloupcich byly jednotlive zaznamy X_train &lt;- data_train[, -c(1, 258, 259, 260)] |&gt; t() X_test &lt;- data_test[, -c(1, 258, 259, 260)] |&gt; t() # prejmenujeme radky a sloupce rownames(X_train) &lt;- 1:256 colnames(X_train) &lt;- paste0(&#39;train&#39;, data_train$row.names) rownames(X_test) &lt;- 1:256 colnames(X_test) &lt;- paste0(&#39;test&#39;, data_test$row.names) # definujeme vektor fonemu y_train &lt;- data_train[, 258] |&gt; factor(levels = phoneme_subset) y_test &lt;- data_test[, 258] |&gt; factor(levels = phoneme_subset) # absolutni cetnosti data.frame(phoneme = phoneme_subset, train = table(y_train) |&gt; as.numeric(), test = table(y_test) |&gt; as.numeric()) ## phoneme train test ## 1 dcl 562 195 ## 2 ao 759 263 11.4.1.0.1 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nejedná o periodické křivky, využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor frekvencí (1 až 256 Hz), standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -2, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 2.19: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Ještě znázorněme křivky včetně průměru zvlášť pro každou třídu. Code n &lt;- dim(XX)[2] y &lt;- c(y_train, y_test) DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Phoneme = rep(y, each = length(t))) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , y == phoneme_subset[1]], 1, mean), apply(fdobjSmootheval[ , y == phoneme_subset[2]], 1, mean)), Phoneme = factor(rep(phoneme_subset, each = length(t)), levels = levels(y)) ) DFsmooth |&gt; filter(time %in% as.character(1:100)) |&gt; ggplot(aes(x = t, y = Smooth, color = Phoneme)) + geom_line(aes(group = time), linewidth = 0.2) + theme_bw() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;log-periodogram&#39;, colour = &#39;Phoneme&#39;) + scale_colour_discrete(labels = phoneme_subset) + geom_line(data = DFmean, aes(x = t, y = Mean, group = Phoneme), linewidth = 1, linetype = &#39;solid&#39;, colour = &#39;grey2&#39;) + facet_wrap(~Phoneme) + theme(legend.position = &#39;none&#39;) Obrázek 1.26: Vykreslení prvních 100 vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čarou je zakreslen průměr pro každou třídu. Nakonec této podkapitoly si ještě vykresleme vyhlazené křivky pro vybrané záznamy z grafu pro všechny fonémy výše. Code n_plot &lt;- 5 data[c(1, 2, 3, 5, 9), ] |&gt; pivot_longer(cols = x.1:x.256, names_to = &#39;time&#39;, values_to = &#39;value&#39;) |&gt; mutate(time = rep(1:256, n_plot)) |&gt; filter(g %in% phoneme_subset) |&gt; ggplot(aes(x = time, y = value, colour = g, group = interaction(speaker, g))) + geom_line(alpha = 0.4) + theme_bw() + facet_grid(~ g) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) + theme(aspect.ratio = 1, legend.position = &quot;none&quot;) + geom_line( data = DFsmooth |&gt; filter(time %in% as.character(c(1, 3))) |&gt; mutate(g = Phoneme), aes(x = t, y = Smooth, color = g, group = time), linewidth = 0.8 ) Obrázek 11.5: Log-periodogramy fonémů pro vybrané záznamy. Code ggsave(&#39;figures/app_02_log_periodogramy_plus_smooth.pdf&#39;, width = 10, height = 5) 11.4.1.0.2 Klasifikace křivek Code # rozdeleni na testovaci a trenovaci cast split &lt;- ifelse(substr(XXfd$fdnames$reps, 1, 2) == &#39;tr&#39;, TRUE, FALSE) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(y, split == TRUE) Y.test &lt;- subset(y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## dcl ao ## 562 759 Code table(Y.test) ## Y.test ## dcl ao ## 195 263 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## dcl ao ## 0.4254353 0.5745647 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## dcl ao ## 0.4257642 0.5742358 11.4.1.0.2.1 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 4, které dohromady vysvětlují 90.93 % variability v datech. První hlavní komponenta potom vysvětluje 80.98 % a druhá 5.06 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() Obrázek 11.6: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 4 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (100 %), tak i na testovacích datech (99.78 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pokud p = 9 if(dim(data.PCA.train)[2] == 10) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1], V6 = data.PCA.train$V5[1], V7 = data.PCA.train$V5[1], V8 = data.PCA.train$V5[1], V9 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.30: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code RESULTS &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) 11.4.1.0.2.2 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 11.4.1.0.2.2.1 Funkcionální logistická regrese Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) - 1 colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze nbasis.x &lt;- 100 basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 9.10: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 9, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() - 1 x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() - 1 dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 10 s validační chybovostí 0. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) Obrázek 5.18: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 10 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train - 1) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred Code # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 0 %) i testovací chybovost (rovna 0 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = .5) + scale_colour_discrete(labels = phoneme_subset) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Foném&#39;) Obrázek 5.19: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(min(tt), max(tt), length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line() + theme_bw() + labs(x = expression(t), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Obrázek 9.13: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [1, 256]\\). Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro frekvence \\(t\\) z prostředka a konce intervalu \\([1, 256]\\), zatímco pro počáteční frekvence jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd zejména na začátku intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.4.1.0.2.3 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. 11.4.1.0.2.3.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [1, 256]\\)), přičemž budeme uvažovat všechny tři výše zmíněné jádrové funkce. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 100 % pro lineární jádro, 100 % pro polynomiální jádro a 100 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 100 % pro lineární jádro, 99.78 % pro polynomiální jádro a 99.13 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 11.4.1.0.2.3.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 4 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 99.92 % pro lineární jádro, 100 % pro polynomiální jádro a 100 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 100 % pro lineární jádro, 100 % pro polynomiální jádro a 100 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 11.7: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 11.4.1.0.2.3.3 Bázové koeficienty Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 100 % pro lineární jádro, 100 % pro polynomiální jádro a 100 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 100 % pro lineární jádro, 99.78 % pro polynomiální jádro a 99.13 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 11.4.1.0.2.3.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Code set.seed(42) k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- 32 # norder n_basis_max &lt;- 50 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # hodnoty hyperparametru pro jadrove funkce C &lt;- 1 p &lt;- 3 gamma &lt;- 1/ncol(data.projection.train) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, degree = p, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 33 0.0000000000 ## poly 36 0.0000000000 ## radial 45 0.0007518797 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 33 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 1, 36 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 1 a 45 pro radiální jádro s hodnotou přesnosti 0.9992. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &#39;Jádro&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 11.8: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, degree = p, coef0 = coef0, gamma = gamma, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 0.22 % pro lineární jádro, 0.22 % pro polynomiální jádro a 0.87 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 11.4.1.0.3 Tabulka výsledků Tabulka 11.2: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) LDA 0e+00 0.0022 LR functional 0e+00 0.0000 SVM linear - diskr 0e+00 0.0000 SVM poly - diskr 0e+00 0.0022 SVM rbf - diskr 0e+00 0.0087 SVM linear - PCA 8e-04 0.0000 SVM poly - PCA 0e+00 0.0000 SVM rbf - PCA 0e+00 0.0000 SVM linear - Bbasis 0e+00 0.0000 SVM poly - Bbasis 0e+00 0.0022 SVM rbf - Bbasis 0e+00 0.0087 SVM linear - projection 0e+00 0.0022 SVM poly - projection 0e+00 0.0022 SVM rbf - projection 0e+00 0.0087 Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["aplikace3.html", "Kapitola 12 Aplikace na reálných datech 3 12.1 Vyhlazení pozorovaných křivek 12.2 Výpočet derivací 12.3 Klasifikace křivek 12.4 Tabulka výsledků 12.5 Klasifikace pomocí druhé derivace 12.6 Tabulka výsledků 12.7 Simulační studie", " Kapitola 12 Aplikace na reálných datech 3 Stejně jako v předchozí kapitole se budeme v této části dokumentu zabývat aplikací dříve popsaných metod (pro více podrobností viz například sekci 1) na reálná data tecator, která jsou dostupná například v balíčku ddalpha. Podrobný popis dat pak můžeme nalézt zde. Jedná se o datový soubor obsahující spektrometrické křivky (absorbanční křivky měřené ve 100 vlnových délkách). Pro každý kus jemně nasekaného masa pozorujeme jednu spektrometrickou křivku, která odpovídá absorbanci naměřené při 100 vlnových délkách. Kusy jsou rozděleny podle Ferratyho a Vieu (2006) do dvou tříd: s malým (\\(&lt; 20\\,\\%\\)) a velkým (\\(\\geq 20\\,\\%\\)) obsahem tuku získaným analytickým chemickým zpracováním. Naším cílem bude klasifikovat spektrometrické křivky na intervalu \\(I = [850 \\text{ nm}, 1050 \\text{ nm}]\\) na základě obsahu tuku. Jak uvidíme z výsledků v části 12.5, je výhodné uvažovat druhou derivaci křivek. Začněme nejprve s načtením a vykreslením dat. Data jsou uložena poněkud složitě, proto pro lepší budou práci s nimi si je uložíme do praktičtějšího formátu. Pojmenujeme si také příslušné sloupce podle toho, zda obsah tuku je malý (small) nebo velký (large). Code # nacteni dat library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) data &lt;- ddalpha::dataf.tecator() data.gr &lt;- data$dataf[[1]]$vals for(i in 2:length(data$labels)) { data.gr &lt;- rbind(data.gr, data$dataf[[i]]$vals) } data.gr &lt;- cbind(data.frame(wave = data$dataf[[1]]$args), t(data.gr)) # vektor trid labels &lt;- data$labels |&gt; unlist() # prejmenovani podle tridy colnames(data.gr) &lt;- c(&#39;wavelength&#39;, paste0(labels, 1:length(data$labels))) Spektrometrické křivky si vykresleme podle skupiny. Code abs.labs &lt;- c(&quot;Obsah tuku &lt; 20 %&quot;, &quot;Obsah tuku &gt; 20 %&quot;) names(abs.labs) &lt;- c(&#39;small&#39;, &#39;large&#39;) pivot_longer(data.gr, cols = large1:large215, names_to = &#39;sample&#39;, values_to = &#39;absorbance&#39;, cols_vary = &#39;slowest&#39;) |&gt; mutate(sample = as.factor(sample), Abs = factor(rep(labels, each = length(data.gr$wavelength)), levels = c(&#39;small&#39;, &#39;large&#39;))) |&gt; ggplot(aes(x = wavelength, y = absorbance, colour = Abs, group = sample)) + geom_line(linewidth = 0.5) + theme_bw() + facet_wrap(~Abs, labeller = labeller(Abs = abs.labs)) + labs(x = &quot;Vlnová délka [v nm]&quot;, y = &quot;Absorbance&quot;, colour = &quot;Obsah tuku&quot;) + theme(legend.position = &#39;none&#39;) + scale_color_discrete(labels = abs.labs) Obrázek 10.1: Absorpční křivky podle skupiny. 12.1 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nejedná o periodické křivky na intervalu \\(I = [850, 1050]\\), využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor wavelength, standardně bychom uvažovali kubické spliny, protože ale budeme chtít pracovat s druhou derivací, volíme norder = 6. Ze stejného důvodu budeme penalizovat čtvrtou derivaci funkcí. Code t &lt;- data.gr$wavelength rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # penalizujeme 4. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě pohlaví stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\) máme v případě rozdílné volby pro každou třídu volit. Code # spojeni pozorovani do jedne matice XX &lt;- data.gr[, -1] |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -1, to = 0.5, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.7) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 3.1: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále při základu 10. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code library(tikzDevice) n &lt;- dim(XX)[2] DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Fat = factor(rep(labels, each = length(t)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(eval.fd(fdobj = mean.fd(XXfd[labels == &#39;small&#39;]), evalarg = t), eval.fd(fdobj = mean.fd(XXfd[labels == &#39;large&#39;]), evalarg = t)), # c(apply(fdobjSmootheval[ , labels == &#39;small&#39;], 1, mean), # apply(fdobjSmootheval[ , labels == &#39;large&#39;], 1, mean)), Fat = factor(rep(c(&#39;small&#39;, &#39;large&#39;), each = length(t)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, color = Fat)) + geom_line(linewidth = 0.05, aes(group = time), alpha = 0.5) + theme_bw() + facet_wrap(~Fat, labeller = labeller(Fat = abs.labs) ) + labs(x = &quot;Vlnová délka&quot;, y = &quot;Absorbance&quot;, colour = &quot;Obsah tuku&quot;) + theme(legend.position = &#39;none&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Obrázek 10.2: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle pohlaví. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Code # ggsave(&quot;figures/kap7_tecator_curves_mean.tex&quot;, device = tikz, width = 9, height = 4.5) Vidíme, že křivky pro obě skupiny (podle obsahu tuku) jsou poměrně podobné, černou čerchovanou čarou je znázorněn průměr. Křivky se liší zejména uprostřed intervalu, kde u tučnějších vzorků nastává o jeden lokální extrém více, naopak u méně tučných vzorků vypadají křivky jednodušeji pouze s jedním globálním extrémem. 12.2 Výpočet derivací Jak jsme již zmínili výše, bude výhodné klasifikovat křivky na základě druhé derivace. K výpočtu derivace pro funkcionální objekt využijeme v R funkci deriv.fd() z balíčku fda. Jelikož chceme klasifikovat na základě druhé derivace, volíme argument Lfdobj = 2. Využití těchto dat bude ukázáno v sekci 12.5. Code XXder &lt;- deriv.fd(XXfd, 2) ttt &lt;- seq(min(t), max(t), length = 501) fdobjSmootheval_der2 &lt;- eval.fd(fdobj = XXder, evalarg = ttt) Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(ttt, n), time = factor(rep(1:n, each = length(ttt))), Smooth = c(fdobjSmootheval_der2), Fat = factor(rep(labels, each = length(ttt)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFmean &lt;- data.frame( t = rep(ttt, 2), Mean = c(eval.fd(fdobj = mean.fd(XXder[labels == &#39;small&#39;]), evalarg = ttt), eval.fd(fdobj = mean.fd(XXder[labels == &#39;large&#39;]), evalarg = ttt)), Fat = factor(rep(c(&#39;small&#39;, &#39;large&#39;), each = length(ttt)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, color = Fat)) + geom_line(linewidth = 0.05, aes(group = time), alpha = 0.5) + theme_bw() + facet_wrap(~Fat#, #labeller = labeller(Fat = abs.labs) ) + labs(x = &quot;Vlnová délka&quot;, y = &quot;Absorbance&quot;, colour = &quot;Obsah tuku&quot;) + theme(legend.position = &#39;none&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Obrázek 1.3: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. Code # ggsave(&quot;figures/kap7_tecator_curves_derivatives.tex&quot;, device = tikz, width = 9, height = 4.5) Vidíme z obrázku výše, že nyní se průměrné křivky mezi oběma skupinami vzorků liší mnohem výrazněji než v případě původních nederivovaných křivek. 12.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les set.seed(42) Rozdělíme data v poměru 30:70 na testovací a trénovací část, abychom mohli stanovit úspěšnost klasifikace jednotlivých metod. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěšnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast set.seed(42) split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 91 59 Code table(Y.test) ## Y.test ## 0 1 ## 47 18 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.6066667 0.3933333 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.7230769 0.2769231 12.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) neighb.model$max.prob # maximalni presnost ## [1] 0.8533333 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 1 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 1 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.8522. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 5.1: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 1, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.1467 a na testovacích datech 0.1692. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 12.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p=\\) 2, které dohromady vysvětlují 99.57 \\(\\%\\) variability v datech. První hlavní komponenta potom vysvětluje 98.47 % a druhá 1.09 \\(\\%\\) variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() Obrázek 3.3: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (68 %), tak i na testovacích datech (70.77 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 9.1: Skóre prvních dvou hlavních komponent, barevně odlišené podle pohlaví. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (68 %), tak i na testovacích datech (69.23 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 12.1: Skóre prvních dvou hlavních komponent, barevně odlišené podle pohlaví. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola, avšak se jen (alespoň opticky) velmi málo liší od přímky. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 12.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; nbasis.x &lt;- 7 # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 3.6: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 6, tedy výrazně menší hodnota než 30. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 24 s validační chybovostí 0.0488. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) Obrázek 10.3: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 24 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b, maxit = 1000, epsilon = 1e-2) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 0 %) i testovací chybovost (rovna 6.15 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Obsah tuku&#39;) Obrázek 1.11: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(min(t), max(t), length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line() + theme_bw() + labs(x = expression(x[1]), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Obrázek 5.6: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [850, 1050]\\). Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro časy \\(t\\) z prostředka a konce intervalu \\([850, 1050]\\), zatímco pro dřívější časy jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd na začátku intervalu, zatímco uprostřed a na konci intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sestrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy přesnost klasifikátoru na trénovacích (69.33 %) i na testovacích datech (70.77 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_colour_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 2.8: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 12.3.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [850, 1050]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost klasifikátoru na testovacích datech je tedy 64.62 % a na trénovacích datech 70.67 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 2.9: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 9.4: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 2 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na testovacích datech je tedy 61.54 % a na trénovacích datech 67.33 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.16: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 2.10: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 77.33 % a na testovacích datech 75.38 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 12.2: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 6.1: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 12.3.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [850, 1050]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost náhodného lesu na trénovacích datech je tedy 98 % a na testovacích datech 87.69 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 95.33 % a na testovacích datech 69.23 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost tohoto klasifikátoru na trénovacích datech je 98.67 % a na testovacích datech 87.69 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 12.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 12.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 12.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 12.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 12.3.7.5. 12.3.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [850, 1050]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(BSmooth$fd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Nyní se pokusme, na rozdíl od postupu v předchozích kapitolách, hyperparametry klasifikátorů odhadnout z dat pomocí 10-násobné cross-validace. Vzhledem k tomu, že každé jádro má ve své definici jiné hyperparametry, budeme ke každé jádrové funkci přistupovat zvlášť. Nicméně hyperparametr \\(C\\) vystupuje u všech jádrových funkcí, přičemž ale připouštíme, že se může jeho optimální hodnota mezi jádry lišit. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.0379, pro polynomiální jádro je \\(C\\) rovno 1.4384 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 12.7427 a pro \\(\\gamma\\) je to 0.0052. Validační přesnosti jsou postupně 0.9933333 pro lineární, 0.9804167 pro polynomiální a 0.9866667 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 99.3333 % pro lineární jádro, 98.6667 % pro polynomiální jádro a 98.6667 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 92.3077 % pro lineární jádro, 92.3077 % pro polynomiální jádro a 95.3846 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 12.3.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Nyní se pokusme, na rozdíl od postupu v předchozích kapitolách, hyperparametry klasifikátorů odhadnout z dat pomocí 10-násobné cross-validace. Vzhledem k tomu, že každé jádro má ve své definici jiné hyperparametry, budeme ke každé jádrové funkci přistupovat zvlášť. Nicméně hyperparametr \\(C\\) vystupuje u všech jádrových funkcí, přičemž ale připouštíme, že se může jeho optimální hodnota mezi jádry lišit. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.1624, pro polynomiální jádro je \\(C\\) rovno 0.0785 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 26.3665 a pro \\(\\gamma\\) je to 1.6379. Validační přesnosti jsou postupně 0.671131 pro lineární, 0.671131 pro polynomiální a 0.7040476 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 68 % pro lineární jádro, 66.67 % pro polynomiální jádro a 84 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 70.7692 % pro lineární jádro, 72.3077 % pro polynomiální jádro a 63.0769 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 6.2: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 12.3.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 6.1585, pro polynomiální jádro je \\(C\\) rovno 54.5559 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 26.3665 a pro \\(\\gamma\\) je to 0.0118. Validační přesnosti jsou postupně 0.9804167 pro lineární, 0.9675 pro polynomiální a 0.9741667 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 99.33 % pro lineární jádro, 99.33 % pro polynomiální jádro a 98.67 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 93.8462 % pro lineární jádro, 90.7692 % pro polynomiální jádro a 93.8462 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 12.3.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 20. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 9 0.01958333 ## poly 7 0.03297619 ## radial 6 0.14125000 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 9 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9804, 7 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.967 a 6 pro radiální jádro s hodnotou přesnosti 0.8588. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &#39;Jádro&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 12.3: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 2 % pro lineární jádro, 2.67 % pro polynomiální jádro a 9.33 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 6.15 % pro lineární jádro, 6.15 % pro polynomiální jádro a 10.77 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.3.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 12.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 12.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 12.3.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 12.3.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku7 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 12.3.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). Jako dobrá volba hyperparametrů se po vyzkoušení zdají být hodnoty \\(\\varepsilon = 0.01\\) a \\(C = 1\\). Vzhledem k výpočetní náročnosti nebudeme tyto hyperparametry odhadovat pomocí CV. Code eps &lt;- 0.01 C &lt;- 1 12.3.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, cost = C, coef0 = coef0, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 12.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0133 0.1231 SVM poly - RKHS 0.0267 0.0308 SVM rbf - RKHS 0.0400 0.0308 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:30 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 11.1: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 11 1.0000 0 linear poly 27 1.0000 0 polynomial radial 30 3.7276 0 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 11 a \\(\\gamma={}\\) 1 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 1, \\(d={}\\) 27 a \\(\\gamma={}\\) 1 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 1 a \\(d={}\\) 30 a \\(\\gamma={}\\) 3.7276 pro radiální jádro s hodnotou přesnosti 1. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 12.4: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 12.2: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0 0.0308 SVM poly - RKHS - radial 0 0.0308 SVM rbf - RKHS - radial 0 0.0154 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 0 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 3.08 % pro lineární jádro, 3.08 % pro polynomiální jádro a 1.54 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.3.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 2:30 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, coef0 = 1, cost = C, epsilon = eps, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, gamma = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 12.3: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 20 5 0.0474 linear poly 10 3 0.0461 polynomial radial 7 5 0.0403 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 20 a \\(p={}\\) 5 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9526, \\(d={}\\) 10 a \\(p={}\\) 3 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9539 a \\(d={}\\) 7 a \\(p={}\\) 5 pro radiální jádro s hodnotou přesnosti 0.9597. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, cost = C, gamma = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = C, gamma = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 2.6: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.0333 0.0615 SVM poly - RKHS - poly 0.0267 0.1077 SVM rbf - RKHS - poly 0.0333 0.1077 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 3.33 % pro lineární jádro, 2.67 % pro polynomiální jádro a 3.33 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 6.15 % pro lineární jádro, 10.77 % pro polynomiální jádro a 10.77 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.3.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 2:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 2.7: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 15 0.0667 linear poly 16 0.0454 polynomial radial 25 0.0526 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 15 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9333, \\(d={}\\) 16 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9546 a \\(d={}\\) 25 pro radiální jádro s hodnotou přesnosti 0.9474. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.7: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.0400 0.0923 SVM poly - RKHS - linear 0.0133 0.0308 SVM rbf - RKHS - linear 0.0200 0.0308 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 4 % pro lineární jádro, 1.33 % pro polynomiální jádro a 2 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 9.23 % pro lineární jádro, 3.08 % pro polynomiální jádro a 3.08 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.4 Tabulka výsledků Vidíme z tabulky níže, že jednotlivé klasifikační metody mají mezi sebou výrazné rozdíly co do úspěšnosti klasifikace. Zejména klasické metody, jako je KNN, LDA nebo QDA si vedou velmi bídně. Můžeme si všimnout, že všechny metody postavené na funkcionální analýze hlavních komponent nedosahují zdaleka podobných výsledků jako některé jiné metody. Naopak nyní se vymyká svou dobrou klasifikační schopností metoda RKHS společně s SVM. Poznamenejme, že také klasická SVM s lineárním jádrem si vede velmi obstojně. Obecně je lineární jádro dobrou volbou (jak jsme se ostatně mohli přesvědčit již dříve), neboť pro dostatečně hustou síť bodů dobře aproximuje určitý integrál na uvažovaném intervalu \\(I\\). Tabulka 2.8: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.1467 0.1692 LDA 0.3200 0.2923 QDA 0.3200 0.3077 LR functional 0.0000 0.0615 LR score 0.3067 0.2923 Tree - diskr. 0.2933 0.3538 Tree - score 0.3267 0.3846 Tree - Bbasis 0.2267 0.2462 RForest - diskr 0.0200 0.1231 RForest - score 0.0467 0.3077 RForest - Bbasis 0.0133 0.1231 SVM linear - diskr 0.0067 0.0769 SVM poly - diskr 0.0133 0.0769 SVM rbf - diskr 0.0133 0.0462 SVM linear - PCA 0.3200 0.2923 SVM poly - PCA 0.3333 0.2769 SVM rbf - PCA 0.1600 0.3692 SVM linear - Bbasis 0.0067 0.0615 SVM poly - Bbasis 0.0067 0.0923 SVM rbf - Bbasis 0.0133 0.0615 SVM linear - projection 0.0200 0.0615 SVM poly - projection 0.0267 0.0615 SVM rbf - projection 0.0933 0.1077 SVM linear - RKHS - radial 0.0000 0.0308 SVM poly - RKHS - radial 0.0000 0.0308 SVM rbf - RKHS - radial 0.0000 0.0154 SVM linear - RKHS - poly 0.0333 0.0615 SVM poly - RKHS - poly 0.0267 0.1077 SVM rbf - RKHS - poly 0.0333 0.1077 SVM linear - RKHS - linear 0.0400 0.0923 SVM poly - RKHS - linear 0.0133 0.0308 SVM rbf - RKHS - linear 0.0200 0.0308 12.5 Klasifikace pomocí druhé derivace Jak jsme již avizovali dříve, pro tato data je vhodné ke klasifikaci uvažovat jejich druhou derivaci. Tu jsme si již spočetli výše, proto už se nyní můžeme pustit rovnou do konstrukce modelů. Proveďme obdobnou analýzu jako v situaci výše, následně (jelikož data náhodně rozdělujeme na testovací a trénovací část), provedeme simulační studii, pomocí které budeme jednotlivé klasifikační metody schopni lépe a s mnohem větší silou porovnat. Code # rozdeleni na testovaci a trenovaci cast set.seed(42) split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 91 59 Code table(Y.test) ## Y.test ## 0 1 ## 47 18 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.6066667 0.3933333 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.7230769 0.2769231 12.5.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) neighb.model$max.prob # maximalni presnost ## [1] 0.9866667 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 3 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 3 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9897. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 1.25: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 3, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.0133 a na testovacích datech 0.0769. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 12.5.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p=\\) 2, které dohromady vysvětlují 93.12 \\(\\%\\) variability v datech. První hlavní komponenta potom vysvětluje 77.7 % a druhá 15.42 \\(\\%\\) variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() Obrázek 11.5: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (96 %), tak i na testovacích datech (90.77 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.28: Skóre prvních dvou hlavních komponent, barevně odlišené podle pohlaví. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (99.33 %), tak i na testovacích datech (98.46 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 12.5: Skóre prvních dvou hlavních komponent, barevně odlišené podle pohlaví. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola, avšak se jen (alespoň opticky) velmi málo liší od přímky. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 12.5.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; nbasis.x &lt;- 7 # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 9.11: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 5, tedy výrazně menší hodnota než 30. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 7 s validační chybovostí 0.0574. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) Obrázek 5.19: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 7 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b, maxit = 1000, epsilon = 1e-2) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred Code # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 0 %) i testovací chybovost (rovna 7.69 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Obsah tuku&#39;) Obrázek 12.6: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(min(t), max(t), length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line() + theme_bw() + labs(x = expression(x[1]), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Obrázek 9.14: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [850, 1050]\\). Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro časy \\(t\\) z prostředka a začátku intervalu \\([850, 1050]\\), zatímco pro pozdějsí časy jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd na začátku a konci intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred Code # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy přesnost klasifikátoru na trénovacích (99.33 %) i na testovacích datech (95.38 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_colour_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 12.7: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 12.5.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [850, 1050]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost klasifikátoru na testovacích datech je tedy 98.46 % a na trénovacích datech 99.33 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 11.8: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 12.8: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 2 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na testovacích datech je tedy 93.85 % a na trénovacích datech 99.33 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 12.9: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 12.10: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 99.33 % a na testovacích datech 98.46 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 12.11: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 12.12: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 12.5.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [850, 1050]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost náhodného lesu na trénovacích datech je tedy 100 % a na testovacích datech 95.38 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 99.33 % a na testovacích datech 95.38 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost tohoto klasifikátoru na trénovacích datech je 100 % a na testovacích datech 96.92 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 12.5.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 12.5.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 12.5.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 12.5.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 12.5.7.5. 12.5.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [850, 1050]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm_der &lt;- XXder XXfd_norm_der$coefs &lt;- XXfd_norm_der$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm_der, split == TRUE) X.test_norm &lt;- subset(XXfd_norm_der, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Nyní se pokusme, na rozdíl od postupu v předchozích kapitolách, hyperparametry klasifikátorů odhadnout z dat pomocí 10-násobné cross-validace. Vzhledem k tomu, že každé jádro má ve své definici jiné hyperparametry, budeme ke každé jádrové funkci přistupovat zvlášť. Nicméně hyperparametr \\(C\\) vystupuje u všech jádrových funkcí, přičemž ale připouštíme, že se může jeho optimální hodnota mezi jádry lišit. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.001, pro polynomiální jádro je \\(C\\) rovno 0.0785 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 0.336 a pro \\(\\gamma\\) je to 0.0052. Validační přesnosti jsou postupně 0.9933333 pro lineární, 0.9933333 pro polynomiální a 0.9933333 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 99.3333 % pro lineární jádro, 99.3333 % pro polynomiální jádro a 99.3333 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 98.4615 % pro lineární jádro, 93.8462 % pro polynomiální jádro a 98.4615 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 12.5.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Nyní se pokusme, na rozdíl od postupu v předchozích kapitolách, hyperparametry klasifikátorů odhadnout z dat pomocí 10-násobné cross-validace. Vzhledem k tomu, že každé jádro má ve své definici jiné hyperparametry, budeme ke každé jádrové funkci přistupovat zvlášť. Nicméně hyperparametr \\(C\\) vystupuje u všech jádrových funkcí, přičemž ale připouštíme, že se může jeho optimální hodnota mezi jádry lišit. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 2.9764, pro polynomiální jádro je \\(C\\) rovno 0.6952 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 1000 a pro \\(\\gamma\\) je to 1.6379. Validační přesnosti jsou postupně 0.9933333 pro lineární, 0.9933333 pro polynomiální a 0.9933333 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 99.33 % pro lineární jádro, 99.33 % pro polynomiální jádro a 99.33 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 95.3846 % pro lineární jádro, 98.4615 % pro polynomiální jádro a 96.9231 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 12.13: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 12.5.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.0089, pro polynomiální jádro je \\(C\\) rovno 0.1624 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 2.9764 a pro \\(\\gamma\\) je to 0.0118. Validační přesnosti jsou postupně 0.9933333 pro lineární, 0.9933333 pro polynomiální a 0.9933333 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 99.33 % pro lineární jádro, 99.33 % pro polynomiální jádro a 99.33 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 92.3077 % pro lineární jádro, 92.3077 % pro polynomiální jádro a 90.7692 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 12.5.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 20. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 9 0.01958333 ## poly 7 0.03297619 ## radial 6 0.14125000 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 9 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9804, 7 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.967 a 6 pro radiální jádro s hodnotou přesnosti 0.8588. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &#39;Jádro&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 12.14: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 2 % pro lineární jádro, 2.67 % pro polynomiální jádro a 9.33 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 6.15 % pro lineární jádro, 6.15 % pro polynomiální jádro a 10.77 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.5.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). Code # hodnoty hypermarametru stejne jako v minule casti eps &lt;- 0.01 C &lt;- 1 12.5.7.5.0.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, cost = C, coef0 = coef0, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 12.4: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0 0 SVM poly - RKHS 0 0 SVM rbf - RKHS 0 0 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:30 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 12.5: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 12 1.0000 0 linear poly 18 1.0000 0 polynomial radial 17 1.9307 0 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 12 a \\(\\gamma={}\\) 1 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 1, \\(d={}\\) 18 a \\(\\gamma={}\\) 1 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 1 a \\(d={}\\) 17 a \\(\\gamma={}\\) 1.9307 pro radiální jádro s hodnotou přesnosti 1. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 12.15: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 12.6: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0 0.0308 SVM poly - RKHS - radial 0 0.0769 SVM rbf - RKHS - radial 0 0.0308 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 0 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 3.08 % pro lineární jádro, 7.69 % pro polynomiální jádro a 3.08 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.5.7.5.0.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 2:30 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, coef0 = 1, cost = C, epsilon = eps, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, gamma = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 12.7: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 30 3 0.0077 linear poly 17 2 0.0077 polynomial radial 3 4 0.0254 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 30 a \\(p={}\\) 3 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9923, \\(d={}\\) 17 a \\(p={}\\) 2 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9923 a \\(d={}\\) 3 a \\(p={}\\) 4 pro radiální jádro s hodnotou přesnosti 0.9746. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, cost = C, gamma = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = C, gamma = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 12.8: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.00 0.0308 SVM poly - RKHS - poly 0.00 0.0154 SVM rbf - RKHS - poly 0.02 0.0308 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 0 % pro lineární jádro, 0 % pro polynomiální jádro a 2 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 3.08 % pro lineární jádro, 1.54 % pro polynomiální jádro a 3.08 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.5.7.5.0.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 2:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 12.9: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 31 0.0267 linear poly 21 0.0267 polynomial radial 6 0.0592 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 31 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9733, \\(d={}\\) 21 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9733 a \\(d={}\\) 6 pro radiální jádro s hodnotou přesnosti 0.9408. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 12.10: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.00 0.0462 SVM poly - RKHS - linear 0.00 0.0308 SVM rbf - RKHS - linear 0.02 0.1231 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 0 % pro lineární jádro, 0 % pro polynomiální jádro a 2 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 4.62 % pro lineární jádro, 3.08 % pro polynomiální jádro a 12.31 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.6 Tabulka výsledků Z tabulky níže si všimněme zejména dvou podstatných věcí. Tou první je, že metody klasifikují data podstatně lépe než v situaci původních nederivovaných dat. U některých metod je zlepšení i v řádech desítek procent. Druhou podstatnou věcí je fakt, že nyní není takový výrazný rozdíl mezi výsledky jednotlivých metod. Tabulka 12.11: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.0133 0.0769 LDA 0.0400 0.0923 QDA 0.0067 0.0154 LR functional 0.0000 0.0769 LR score 0.0067 0.0462 Tree - diskr. 0.0067 0.0154 Tree - score 0.0067 0.0615 Tree - Bbasis 0.0067 0.0154 RForest - diskr 0.0000 0.0462 RForest - score 0.0067 0.0462 RForest - Bbasis 0.0000 0.0308 SVM linear - diskr 0.0067 0.0154 SVM poly - diskr 0.0067 0.0615 SVM rbf - diskr 0.0067 0.0154 SVM linear - PCA 0.0067 0.0462 SVM poly - PCA 0.0067 0.0154 SVM rbf - PCA 0.0067 0.0308 SVM linear - Bbasis 0.0067 0.0769 SVM poly - Bbasis 0.0067 0.0769 SVM rbf - Bbasis 0.0067 0.0923 SVM linear - projection 0.0200 0.0615 SVM poly - projection 0.0267 0.0615 SVM rbf - projection 0.0933 0.1077 SVM linear - RKHS - radial 0.0000 0.0308 SVM poly - RKHS - radial 0.0000 0.0769 SVM rbf - RKHS - radial 0.0000 0.0308 SVM linear - RKHS - poly 0.0000 0.0308 SVM poly - RKHS - poly 0.0000 0.0154 SVM rbf - RKHS - poly 0.0200 0.0308 SVM linear - RKHS - linear 0.0000 0.0462 SVM poly - RKHS - linear 0.0000 0.0308 SVM rbf - RKHS - linear 0.0200 0.1231 12.7 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). 12.7.1 Simulace pro nederivovaná data Nejprve se podívejme na simulaci původních, tedy nederivovaných, dat. Vzhledem k vysokému počtu uvažovaných klasifikačních metod proveďme simulace zvlášť se stejným nastavením generátoru pseudonáhodných čísel (set.seed(42)), poté můžeme výsledky simulací porovnat mezi sebou. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:10) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze nbasis.x &lt;- 7 basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) } # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-2, 2, length = 5) C.cv &lt;- 10^seq(-3, 2, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], coef0 = 1, degree = p.opt, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], coef0 = 1, degree = p.opt, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS C &lt;- 1 eps &lt;- 0.01 ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 30, by = 2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, coef0 = 1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 10, by = 1) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/aplikace_03neder.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/aplikace_03neder_res.RData&#39;) 12.7.1.1 Výsledky Tabulka 12.12: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.1730 0.1777 0.0265 0.0504 LDA 0.2996 0.3163 0.0207 0.0446 QDA 0.3017 0.3182 0.0217 0.0441 LR_functional 0.0105 0.0482 0.0278 0.0467 LR_score 0.2919 0.3103 0.0207 0.0447 Tree_discr 0.1857 0.2937 0.0449 0.0583 Tree_score 0.2470 0.3360 0.0451 0.0574 Tree_Bbasis 0.1855 0.2918 0.0456 0.0565 RF_discr 0.0121 0.2072 0.0091 0.0502 RF_score 0.0360 0.3100 0.0110 0.0507 RF_Bbasis 0.0121 0.2066 0.0087 0.0453 SVM linear - diskr 0.0036 0.0162 0.0058 0.0227 SVM poly - diskr 0.0113 0.0455 0.0129 0.0234 SVM rbf - diskr 0.0053 0.0346 0.0063 0.0229 SVM linear - PCA 0.2989 0.3285 0.0234 0.0495 SVM poly - PCA 0.2833 0.3515 0.0329 0.0452 SVM rbf - PCA 0.1553 0.3469 0.1128 0.0479 SVM linear - Bbasis 0.0113 0.0277 0.0074 0.0194 SVM poly - Bbasis 0.0137 0.0495 0.0104 0.0287 SVM rbf - Bbasis 0.0153 0.0532 0.0063 0.0252 SVM linear - projection 0.0312 0.0425 0.0103 0.0244 SVM poly - projection 0.0339 0.0577 0.0135 0.0331 SVM rbf - projection 0.1395 0.2034 0.0289 0.0554 SVM linear - RKHS - radial 0.0008 0.0197 0.0024 0.0188 SVM poly - RKHS - radial 0.0009 0.0132 0.0023 0.0178 SVM rbf - RKHS - radial 0.0036 0.0195 0.0047 0.0163 SVM linear - RKHS - poly 0.0543 0.0832 0.0138 0.0321 SVM poly - RKHS - poly 0.0305 0.0889 0.0152 0.0305 SVM rbf - RKHS - poly 0.0302 0.0642 0.0094 0.0294 SVM linear - RKHS - linear 0.0448 0.0725 0.0149 0.0349 SVM poly - RKHS - linear 0.0433 0.0754 0.0134 0.0358 SVM rbf - RKHS - linear 0.0803 0.1188 0.0217 0.0434 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Code wilcox.test(SIMULACE$test[, &#39;SVM poly - RKHS - radial&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.3874749 Code wilcox.test(SIMULACE$test[, &#39;SVM rbf - RKHS - radial&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.201061 Code wilcox.test(SIMULACE$test[, &#39;SVM linear - RKHS - radial&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.1067453 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;greater&#39;, paired = T)$p.value ## [1] 2.09865e-11 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM poly - RKHS - radial&#39;], alternative = &#39;greater&#39;, paired = T)$p.value ## [1] 2.355944e-13 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM rbf - RKHS - radial&#39;], alternative = &#39;greater&#39;, paired = T)$p.value ## [1] 1.332949e-10 Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # nastavime jinak nazvy klasifikacnich metod methods_names &lt;- c( &#39;$K$ nejbližších sousedů&#39;, &#39;Lineární diskriminační analýza&#39;, &#39;Kvadratická diskriminační analýza&#39;, &#39;Funkcionální logistická regrese&#39;, &#39;Logistické regrese s fPCA&#39;, &#39;Rozhodovací strom -- diskretizace&#39;, &#39;Rozhodovací strom -- fPCA&#39;, &#39;Rozhodovací strom -- bázové koeficienty&#39;, &#39;Náhodný les -- diskretizace&#39;, &#39;Náhodný les -- fPCA&#39;, &#39;Náhodný les -- bázové koeficienty&#39;, &#39;SVM (linear) -- diskretizace&#39;, &#39;SVM (poly) -- diskretizace&#39;, &#39;SVM (radial) -- diskretizace&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (poly) -- fPCA&#39;, &#39;SVM (radial) -- fPCA&#39;, &#39;SVM (linear) -- bázové koeficienty&#39;, &#39;SVM (poly) -- bázové koeficienty&#39;, &#39;SVM (radial) -- bázové koeficienty&#39;, &#39;SVM (linear) -- projekce&#39;, &#39;SVM (poly) -- projekce&#39;, &#39;SVM (radial) -- projekce&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39;, &#39;RKHS (radial SVR) $+$ SVM (poly)&#39;, &#39;RKHS (radial SVR) $+$ SVM (radial)&#39;, &#39;RKHS (poly SVR) $+$ SVM (linear)&#39;, &#39;RKHS (poly SVR) $+$ SVM (poly)&#39;, &#39;RKHS (poly SVR) $+$ SVM (radial)&#39;, &#39;RKHS (linear SVR) $+$ SVM (linear)&#39;, &#39;RKHS (linear SVR) $+$ SVM (poly)&#39;, &#39;RKHS (linear SVR) $+$ SVM (radial)&#39; ) # barvy pro boxploty box_col &lt;- c(&#39;#4dd2ff&#39;, &#39;#0099cc&#39;, &#39;#00ace6&#39;, &#39;#00bfff&#39;, &#39;#1ac5ff&#39;, rep(&#39;#33ccff&#39;, 3), rep(&#39;#0086b3&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff6347&#39;, 3), rep(&#39;#ff7961&#39;, 3), rep(&#39;#ff4d2e&#39;, 3), rep(&#39;#fa2600&#39;, 9)) # box_col &lt;- c(&#39;#CA0A0A&#39;, &#39;#fa2600&#39;, &#39;#fa2600&#39;, &#39;#D15804&#39;, # &#39;#D15804&#39;, rep(&#39;#D3006D&#39;, 3), rep(&#39;#BE090F&#39;, 3), c(&quot;#12DEE8&quot;, &quot;#4ECBF3&quot;, &quot;#127DE8&quot;, &quot;#4C3CD3&quot;, &quot;#4E65F3&quot;, &quot;#4E9EF3&quot;, &quot;#081D58&quot;) |&gt; rep(each = 3)) # alpha pro boxploty box_alpha &lt;- c(0.9, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, seq(0.9, 0.6, length = 9)) - 0.3 Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 60, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 12.16: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, # y = &quot;$\\\\widehat{\\\\textnormal{Err}}_{test}$&quot; y = expression(widehat(Err)[test]) ) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 50, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 3, color = &quot;black&quot;, alpha = 0.9)# + Obrázek 12.17: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # scale_x_discrete(labels = methods_names) + # theme(plot.margin = unit(c(0.5, 0.5, 2, 2), &quot;cm&quot;)) + # scale_fill_manual(values = box_col) + # scale_alpha_manual(values = box_alpha) # ggsave(&quot;figures/kap7_tecator_box_test_neder.tex&quot;, device = tikz, width = 9, height = 7) Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 12.13: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 1 nharm 1 LR_func_n_basis 19 SVM_d_Linear 6 SVM_d_Poly 6 SVM_d_Radial 6 SVM_RKHS_radial_gamma1 1 SVM_RKHS_radial_gamma2 1 SVM_RKHS_radial_gamma3 1 SVM_RKHS_radial_d1 17 SVM_RKHS_radial_d2 16 SVM_RKHS_radial_d3 20 SVM_RKHS_poly_p1 4 SVM_RKHS_poly_p2 4 SVM_RKHS_poly_p3 5 SVM_RKHS_poly_d1 8 SVM_RKHS_poly_d2 7 SVM_RKHS_poly_d3 7 SVM_RKHS_linear_d1 20 SVM_RKHS_linear_d2 17 SVM_RKHS_linear_d3 13 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.18: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.19: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.20: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.21: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.22: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. 12.7.2 Simulace pro derivovaná data Nyní se konečně podívejme na simulaci derivovaných dat (určili jsme druhou derivaci křivek). Vzhledem k vysokému počtu uvažovaných klasifikačních metod proveďme simulace zvlášť se stejným nastavením generátoru pseudonáhodných čísel (set.seed(42)), abychom mohli výsledky simulací porovnat mezi sebou. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:10) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze nbasis.x &lt;- 7 basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm_der, split == TRUE) X.test_norm &lt;- subset(XXfd_norm_der, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) } # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-2, 2, length = 5) C.cv &lt;- 10^seq(-3, 2, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], coef0 = coef0, degree = p.opt, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS C &lt;- 1 eps &lt;- 0.01 ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 30, by = 2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 10, by = 1) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/aplikace_03der.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/aplikace_03der_res.RData&#39;) 12.7.2.1 Výsledky Tabulka 12.14: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.0147 0.0212 0.0066 0.0170 LDA 0.0558 0.0626 0.0089 0.0287 QDA 0.0105 0.0145 0.0064 0.0129 LR_functional 0.0009 0.0405 0.0032 0.0327 LR_score 0.0081 0.0145 0.0059 0.0157 Tree_discr 0.0109 0.0263 0.0454 0.0603 Tree_score 0.0171 0.0229 0.0063 0.0176 Tree_Bbasis 0.0107 0.0251 0.0455 0.0595 RF_discr 0.0003 0.0117 0.0015 0.0122 RF_score 0.0057 0.0168 0.0032 0.0164 RF_Bbasis 0.0001 0.0089 0.0007 0.0103 SVM linear - diskr 0.0033 0.0091 0.0052 0.0137 SVM poly - diskr 0.0013 0.0152 0.0032 0.0164 SVM rbf - diskr 0.0025 0.0148 0.0040 0.0135 SVM linear - PCA 0.0097 0.0197 0.0062 0.0203 SVM poly - PCA 0.0080 0.0174 0.0060 0.0167 SVM rbf - PCA 0.0073 0.0174 0.0058 0.0144 SVM linear - Bbasis 0.0033 0.0249 0.0065 0.0216 SVM poly - Bbasis 0.0031 0.0234 0.0044 0.0174 SVM rbf - Bbasis 0.0033 0.0220 0.0068 0.0197 SVM linear - projection 0.0297 0.0449 0.0086 0.0274 SVM poly - projection 0.0339 0.0560 0.0142 0.0393 SVM rbf - projection 0.1454 0.1954 0.0306 0.0605 SVM linear - RKHS - radial 0.0007 0.0238 0.0020 0.0152 SVM poly - RKHS - radial 0.0024 0.0238 0.0037 0.0170 SVM rbf - RKHS - radial 0.0038 0.0203 0.0046 0.0148 SVM linear - RKHS - poly 0.0142 0.0386 0.0071 0.0215 SVM poly - RKHS - poly 0.0070 0.0488 0.0094 0.0254 SVM rbf - RKHS - poly 0.0127 0.0535 0.0102 0.0232 SVM linear - RKHS - linear 0.0063 0.0442 0.0089 0.0219 SVM poly - RKHS - linear 0.0035 0.0397 0.0058 0.0250 SVM rbf - RKHS - linear 0.0061 0.0426 0.0075 0.0233 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Code wilcox.test(SIMULACE$test[, &#39;RF_Bbasis&#39;], SIMULACE$test[, &#39;RF_discr&#39;], alternative = &#39;less&#39;, paired = T)$p.value ## [1] 0.0005059073 Code wilcox.test(SIMULACE$test[, &#39;RF_Bbasis&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.8449667 Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 12.23: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, # y = &quot;$\\\\widehat{\\\\textnormal{Err}}_{test}$&quot; y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 50, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 3, color = &quot;black&quot;, alpha = 0.9) #+ Obrázek 12.24: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # scale_x_discrete(labels = methods_names) + # theme(plot.margin = unit(c(0.5, 0.5, 2, 2), &quot;cm&quot;)) + # coord_cartesian(ylim = c(0, 0.15)) + # scale_fill_manual(values = box_col) + # scale_alpha_manual(values = box_alpha) # ggsave(&quot;figures/kap7_tecator_box_test_der.tex&quot;, device = tikz, width = 9, height = 7) Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 12.15: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 5.0 nharm 2.0 LR_func_n_basis 6.0 SVM_d_Linear 6.0 SVM_d_Poly 6.0 SVM_d_Radial 6.0 SVM_RKHS_radial_gamma1 0.5 SVM_RKHS_radial_gamma2 0.3 SVM_RKHS_radial_gamma3 0.3 SVM_RKHS_radial_d1 14.0 SVM_RKHS_radial_d2 12.0 SVM_RKHS_radial_d3 10.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 4.0 SVM_RKHS_poly_p3 4.0 SVM_RKHS_poly_d1 5.0 SVM_RKHS_poly_d2 5.5 SVM_RKHS_poly_d3 4.0 SVM_RKHS_linear_d1 21.0 SVM_RKHS_linear_d2 21.0 SVM_RKHS_linear_d3 24.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.25: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.26: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.27: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.28: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.29: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["aplikace-na-reálných-datech-4.html", "Kapitola 13 Aplikace na reálných datech 4 13.1 handwrit data 13.2 growth data", " Kapitola 13 Aplikace na reálných datech 4 V této části se podíváme podrobněji na klasifikaci pomocí postupu, při kterém nejprve data projektujeme na Reproducing Kernel Hilbert Space definovaný jádrem \\(K\\) a následně pomocí koeficientů této projekce klasifikujeme data pomocí metody SVM. K získání koeficientů projekce se také využívá metoda SVM, avšak její analogie pro řešení regresních preblémů (SVM for regression). Tento postup je podrobněji popsán ve článku8, jehož autory jsou Muñoz a González. Autoři ve svém článku popisují mimo jiné i chování koeficientů z vyjádření pomocí jádrových funkcí (kernel expansion) a zlepšení stability koeficientů pomocí RKHS. Tuto vlastnost si nyní ilustrujme na analogickém příkladě, jako uvádějí autoři. Code library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(patchwork) library(e1071) library(caret) library(kernlab) 13.1 handwrit data Nejprve si načteme data, která představují souřadnice ručně psaných slov fda. Pro naše účely, kdy chceme porovnat dvě různá pozorování, si načtěme pouze první dva záznamy. Code # nacteni dat samples &lt;- c(4, 5) + 10 hand &lt;- handwrit[, samples, ] Pro lepší orientaci si data nejprve vykresleme. Nejprve celkový záznam nápisu fda, následně jednotlivé souřadnice \\(x\\) a \\(y\\) proti času. Code p1 &lt;- ggplot(data = data.frame(x = c(hand[, 1, 1], hand[, 2, 1]), y = c(hand[, 1, 2], hand[, 2, 2]), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = 1401)), aes(x = x, y = y, colour = Sample)) + geom_path() + theme_bw() + labs(x = &#39;X&#39;, y = &#39;Y&#39;) + theme(aspect.ratio = 1, legend.position = &#39;none&#39;) p2 &lt;- ggplot(data = data.frame(x = c(handwritTime, handwritTime), y = c(hand[, 1, 1], hand[, 2, 1]), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = 1401)), aes(x = x, y = y, colour = Sample)) + geom_path() + theme_bw() + labs(x = &#39;Time&#39;, y = &#39;X&#39;) + theme(aspect.ratio = 1, legend.position = &#39;none&#39;) p3 &lt;- ggplot(data = data.frame(x = c(handwritTime, handwritTime), y = c(hand[, 1, 2], hand[, 2, 2]), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = 1401)), aes(x = x, y = y, colour = Sample)) + geom_path() + theme_bw() + labs(x = &#39;Time&#39;, y = &#39;Y&#39;) + theme(aspect.ratio = 1, legend.position = &#39;none&#39;) (p1 | p2 | p3) Nyní se podívejme, jak dopadnou vektory koeficientů \\(\\boldsymbol \\alpha_1\\), \\(\\boldsymbol \\alpha_2\\) z jádrového vyjádření získaného pomocí SVM pro regresi a také jak vypadají \\(\\boldsymbol \\lambda_1\\), \\(\\boldsymbol \\lambda_2\\) pro RKHS reprezentaci. Vektory \\(\\boldsymbol \\alpha_1\\) a \\(\\boldsymbol \\alpha_2\\) definují reprezentaci křivek \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R, l= 1, 2.\\). Podobně vektory \\(\\boldsymbol \\lambda_1\\) a \\(\\boldsymbol \\lambda_2\\) definují vyjádření křivek v bázi RKHS \\(\\mathcal H_K\\) \\[ c_l^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_{jl}^* \\phi_j(\\boldsymbol x), \\] které můžeme odhadnout z dat pomocí \\[ \\hat\\lambda_{jl}^* = \\hat\\lambda_{jl} \\sum_{i = 1}^m \\alpha_{il}\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d. \\] Uvažme Gaussovské jádro s parametrem \\(\\gamma = 0.5\\) (analogicky jako ve výše zmíněném článku). Code gamma &lt;- 0.5 # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;)^2)) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Nyní si spočítejme pomocí funkce svm() z knihovny e1071 vektory koeficientů \\(\\boldsymbol \\alpha_1\\) a \\(\\boldsymbol \\alpha_2\\). Uvědomme si, že funkce svm() vrací koeficienty pouze pro podpůrné vektory, pro ostatní jsou tyto koeficienty nulové. Další možností pro získání koeficientů je využít funkci train() z balíčku caret s volbou method = 'svmRadial'. Syntaxe této funkce je poněkud složitější, proto jsme v celém předchozím postupu uvažovali první zmíněnou funkci. Code # urceni koeficientu alpha z SVM alpha &lt;- matrix(0, nrow = length(handwritTime), ncol = length(samples)) # prazdny objekt # model for(i in 1:length(samples)) { df.svm &lt;- data.frame(x = handwritTime, y = hand[, i, 1]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.03, gamma = gamma, cost = 1, tolerance = 0.001, shrinking = TRUE, scale = TRUE) # alpha[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty svm.RKHS &lt;- train(y ~ x, data = df.svm, method = &#39;svmRadial&#39;, metric = &quot;RMSE&quot;, preProcess = c(&#39;center&#39;, &#39;scale&#39;), # trControl = trainControl( # method = &quot;repeatedcv&quot;, # number = 5, # repeats = 2, # verboseIter = FALSE # ) trControl = trainControl(method = &quot;none&quot;), # Telling caret not to re-tune tuneGrid = data.frame(sigma = 19, C = 1000) # Specifying the parameters ) alpha[svm.RKHS$finalModel@alphaindex, i] &lt;- svm.RKHS$finalModel@alpha * svm.RKHS$finalModel@scaling$y.scale$`scaled:scale` } Vykresleme si pro lepší představu odhad křivky (červeně) společně s pozorovanou křivkou (modře). Code data.frame(x = handwritTime, y = svm.RKHS$finalModel@fitted * svm.RKHS$finalModel@scaling$y.scale$`scaled:scale` + svm.RKHS$finalModel@scaling$y.scale$`scaled:center`, line = &#39;estimate&#39;) |&gt; rbind(data.frame( x = handwritTime, y = hand[, i, 1], line = &#39;sample&#39; )) |&gt; ggplot(aes(x, y, col = line)) + geom_line() + theme_bw() + theme(legend.position = &#39;bottom&#39;) + labs(x = &#39;Time&#39;, y = &#39;X&#39;, col = &#39;Curve&#39;) Obrázek 3.2: Porovnání pozorované a odhadnuté křivky. Podívejme se nyní konečně na hodnoty \\(\\boldsymbol \\alpha_1\\) a \\(\\boldsymbol \\alpha_2\\) pro dvě pozorování. Code data.frame(alpha = c(alpha[, 1], alpha[, 2]), Time = c(handwritTime, handwritTime), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = length(handwritTime))) |&gt; ggplot(aes(xend = Time, x = Time, y = 0, yend = alpha, colour = Sample)) + geom_segment() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(bold(alpha))[i])) Podívejme se nyní na hodnoty \\(\\boldsymbol \\lambda_1\\) a \\(\\boldsymbol \\lambda_2\\) pro dvě pozorování. Code # spocitame matici K K &lt;- Kernel.RKHS(handwritTime, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors Code # d d.RKHS &lt;- rankMM(K) # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(hand)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace Lambda.RKHS[, 1] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha[, 1]) * eig.vals[1:d.RKHS] Lambda.RKHS[, 2] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha[, 2]) * eig.vals[1:d.RKHS] Code data.frame(lambda = c(Lambda.RKHS[, 1], Lambda.RKHS[, 2]), Time = c(handwritTime, handwritTime), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = length(handwritTime))) |&gt; ggplot(aes(xend = Time, x = Time, y = 0, yend = lambda, colour = Sample)) + geom_segment() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(bold(lambda))[i])) + xlim(handwritTime[1:20] |&gt; range()) + ylim(c(-0.01, 0.01)) + facet_wrap(~Sample) Nakonec si vykresleme rozdíly. Code data.frame(lambda = c(Lambda.RKHS[, 1] - Lambda.RKHS[, 2]), Time = c(handwritTime)) |&gt; ggplot(aes(xend = Time, x = Time, y = 0, yend = lambda)) + geom_segment() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(bold(lambda))[1] - widehat(bold(lambda))[2])) + ylim(c(-0.01, 0.01)) + xlim(handwritTime[1:20] |&gt; range()) 13.2 growth data Podívejme se nyní ještě na tutéž analýzu, ale z pohledu dat growth. Code # nacteni dat samples &lt;- c(54, 55) - 4 dataf &lt;- dataf.growth() t1 &lt;- dataf$dataf[[samples[1]]]$args t2 &lt;- dataf$dataf[[samples[2]]]$args y1 &lt;- dataf$dataf[[samples[1]]]$vals y2 &lt;- dataf$dataf[[samples[2]]]$vals data.gr &lt;- data.frame(t = t1, H1 = y1, H2 = y2) Code gamma &lt;- 0.1 # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;)^2)) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Nyní si spočítejme pomocí funkce svm() z knihovny e1071 vektory koeficientů \\(\\boldsymbol \\alpha_1\\) a \\(\\boldsymbol \\alpha_2\\). Uvědomme si, že funkce svm() vrací koeficienty pouze pro podpůrné vektory, pro ostatní jsou tyto koeficienty nulové. Code # urceni koeficientu alpha z SVM alpha &lt;- matrix(0, nrow = length(data.gr$t), ncol = 2) # prazdny objekt # model for(i in 1:2) { df.svm &lt;- data.frame(x = data.gr$t, y = data.gr[, i + 1]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.5, gamma = gamma, cost = 100000, tolerance = 0.001, shrinking = FALSE, scale = TRUE) # svm.RKHS &lt;- train(y ~ x, data = df.svm, # method = &#39;svmLinear&#39;) # urceni alpha # b &lt;- - svm.RKHS$rho # rho ... the negative intercept # betas &lt;- svm.RKHS$coefs # alphas &lt;- betas + b #/ Gauss.kernel() alpha[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Code data.frame(alpha = c(alpha[, 1], alpha[, 2]), Time = c(data.gr$t, data.gr$t), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = length(data.gr$t))) |&gt; ggplot(aes(xend = Time, x = Time, y = 0, yend = alpha, colour = Sample)) + geom_segment() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(bold(alpha))[i])) Podívejme se nyní na hodnoty \\(\\boldsymbol \\lambda_1\\) a \\(\\boldsymbol \\lambda_2\\) pro dvě pozorování. Code # spocitame matici K K &lt;- Kernel.RKHS(data.gr$t, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors Code # d d.RKHS &lt;- rankMM(K) # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = 2, nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace Lambda.RKHS[, 1] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha[, 1]) * eig.vals[1:d.RKHS] Lambda.RKHS[, 2] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha[, 2]) * eig.vals[1:d.RKHS] Code data.frame(lambda = c(Lambda.RKHS[, 1], Lambda.RKHS[, 2]), Time = c(1:d.RKHS, 1:d.RKHS), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = d.RKHS)) |&gt; ggplot(aes(xend = Time, x = Time, y = 0, yend = lambda, colour = Sample)) + geom_segment() + theme_bw() + labs(x = &#39;Index&#39;, y = expression(widehat(bold(lambda))[i])) + facet_wrap(~Sample) Nakonec si vykresleme rozdíly. Code data.frame(lambda = c(Lambda.RKHS[, 1] - Lambda.RKHS[, 2]), Time = c(1:d.RKHS)) |&gt; ggplot(aes(xend = Time, x = Time, y = 0, yend = lambda)) + geom_segment() + theme_bw() + labs(x = &#39;Index&#39;, y = expression(widehat(bold(lambda))[1] - widehat(bold(lambda))[2])) + ylim(c(-5, 3)) Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["podpůrné-materiály-pro-diplomovou-práci.html", "Kapitola 14 Podpůrné materiály pro diplomovou práci 14.1 Materiály pro Kapitolu 1 14.2 Materiály pro Kapitolu 2 14.3 Materiály pro Kapitolu 3 14.4 Materiály pro Kapitolu 4 14.5 Materiály pro Kapitolu 5 14.6 Materiály pro Kapitolu 6 14.7 Materiály pro Kapitolu 7", " Kapitola 14 Podpůrné materiály pro diplomovou práci V této poslední kapitole jsou uvedeny zdrojové kódy pro vygenerování grafů a dalších případných materiálů, které jsou použity v diplomové práci. Jedná se především o ilustrativní grafy určitých vlastností a fenoménů spojených s funkcionálními daty. Kapitola je členěna do sekcí, které odpovídají jednotlivým kapitolám v diplomové práci. Všechny grafy jsou vytvořeny pomocí balíčku ggplot2, který poskytuje celou řadu grafických funkcionalit, pomocí kterých jsme (alespoň subjektivně) schopni dosáhnout podstatně lépe a profesionálněni vypadajících grafických výstupů v porovnání s klasickou grafikou v R. Všechny grafy jsou uloženy pomocí funkce ggsave() ve formátu pdf nebo tikz, který umožňuje lepší kombinaci grafiky a symbolů v \\(\\LaTeX\\)u. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(tidyverse) library(patchwork) library(tikzDevice) set.seed(42) options(tz = &quot;UTC&quot;) # nacteni dat data &lt;- read.delim2(&#39;phoneme.txt&#39;, header = T, sep = &#39;,&#39;) # zmenime dve promenne na typ factor data &lt;- data |&gt; mutate(g = factor(g), speaker = factor(speaker)) # numericke promenne prevedeme opravdu na numericke data[, 2:257] &lt;- as.numeric(data[, 2:257] |&gt; as.matrix()) tr_vs_test &lt;- str_split(data$speaker, &#39;\\\\.&#39;) |&gt; unlist() tr_vs_test &lt;- tr_vs_test[seq(1, length(tr_vs_test), by = 4)] data$train &lt;- ifelse(tr_vs_test == &#39;train&#39;, TRUE, FALSE) # vybrane fonemy ke klasifikaci phoneme_subset &lt;- c(&#39;aa&#39;, &#39;ao&#39;) # testovaci a trenovaci data data_train &lt;- data |&gt; filter(train) |&gt; filter(g %in% phoneme_subset) data_test &lt;- data |&gt; filter(!train) |&gt; filter(g %in% phoneme_subset) # odstranime sloupce, ktere nenesou informaci o frekvenci a # transponujeme tak, aby ve sloupcich byly jednotlive zaznamy X_train &lt;- data_train[, -c(1, 258, 259, 260)] |&gt; t() X_test &lt;- data_test[, -c(1, 258, 259, 260)] |&gt; t() # prejmenujeme radky a sloupce rownames(X_train) &lt;- 1:256 colnames(X_train) &lt;- paste0(&#39;train&#39;, data_train$row.names) rownames(X_test) &lt;- 1:256 colnames(X_test) &lt;- paste0(&#39;test&#39;, data_test$row.names) # definujeme vektor fonemu y_train &lt;- data_train[, 258] |&gt; factor(levels = phoneme_subset) y_test &lt;- data_test[, 258] |&gt; factor(levels = phoneme_subset) y &lt;- c(y_train, y_test) 14.1 Materiály pro Kapitolu 1 V této sekci uvedeme podpůrné grafy pro první kapitolu diplomové práce. 14.1.1 Funkcionální průměr Nyní pro data phoneme spočítáme průměr. Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() XXaa &lt;- XX[, y == phoneme_subset[1]] lambda.vect &lt;- 10^seq(from = 1, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) ## pouze pro aa lambda.vect &lt;- 10^seq(from = 1, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmoothaa &lt;- smooth.basis(t, XXaa, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmoothaa$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmoothaa &lt;- smooth.basis(t, XXaa, curv.fdPar) XXfdaa &lt;- BSmoothaa$fd fdobjSmoothevalaa &lt;- eval.fd(fdobj = XXfdaa, evalarg = t) # prumer meanfd &lt;- mean.fd(XXfdaa) fdmean &lt;- eval.fd(fdobj = meanfd, evalarg = t) Code n &lt;- dim(XX)[2] DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Phoneme = rep(y, each = length(t))) |&gt; filter(Phoneme == &#39;aa&#39;) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(fdmean, fdmean), Phoneme = factor(rep(phoneme_subset, each = length(t)), levels = levels(y)) ) |&gt; filter(Phoneme == &#39;aa&#39;) # tikz(file = &quot;figures/DP_kap1_mean.tex&quot;, width = 6, height = 5) DFsmooth |&gt; filter(time %in% as.character(1:100)) |&gt; ggplot(aes(x = t, y = Smooth)) + geom_line(aes(group = time), linewidth = 0.2, colour = &#39;deepskyblue2&#39;, alpha = 0.6) + theme_bw() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;log-periodogram&#39;, colour = &#39;Phoneme&#39;) + scale_colour_discrete(labels = phoneme_subset) + geom_line(data = DFmean, aes(x = t, y = Mean, group = Phoneme), linewidth = 1, linetype = &#39;solid&#39;, colour = &#39;grey2&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.1: Vykreslení prvních 100 vyhlazených pozorovaných křivek. Černou čarou je zakreslen průměr. Code # dev.off() # ggsave(&quot;figures/DP_kap1_mean.pdf&quot;, width = 6, height = 5) 14.1.2 Variance Code varfd &lt;- var.fd(XXfdaa) fdvar &lt;- eval.bifd(t, t, varfd) Code dfs &lt;- data.frame( time = t, value = c(fdobjSmoothevalaa)) df &lt;- data.frame(dfs, fdmean = fdmean, fdvar = diag(fdvar)) # tikz(file = &quot;figures/DP_kap1_variance.tex&quot;, width = 6, height = 5) # df &lt;- df[seq(1, length(df$time), length = 1001), ] ggplot(data = df, aes(x = time, y = fdvar)) + geom_line(color = &#39;deepskyblue2&#39;, linewidth = 0.8) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Variance&#39;, colour = &#39;Phoneme&#39;) + theme_bw() Code # dev.off() # ggsave(&quot;figures/DP_kap1_variance.tex&quot;, width = 6, height = 5, device = tikz) 14.1.3 Kovariance a Korelace Code fdcor &lt;- cor.fd(t, XXfdaa) df &lt;- merge(t, t) df &lt;- data.frame(df, fdcov = c(fdvar), fdcor = c(fdcor)) df &lt;- df[seq(1, length(df$x), length = 68001), ] # tikz(file = &quot;figures/DP_kap1_cov.tex&quot;, width = 6, height = 6) p1 &lt;- ggplot(data = df, aes (x, y, z = fdcov)) + geom_raster(aes(fill = fdcov)) + geom_contour(colour = &quot;white&quot;) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Frekvence [Hz]&#39;, fill = &#39;Kovariance&#39;) + coord_fixed(ratio = 1) + theme_bw() + theme(legend.position = &#39;bottom&#39;) + scale_y_continuous(expand = c(0,0) + 0.01) + scale_x_continuous(expand = c(0,0) + 0.01) p1 Code # dev.off() # ggsave(&quot;figures/DP_kap1_cov.tex&quot;, width = 6, height = 6, device = tikz) # tikz(file = &quot;figures/DP_kap1_cor.tex&quot;, width = 6, height = 6) p2 &lt;- ggplot(data = df, aes (x, y, z = fdcor)) + geom_raster(aes(fill = fdcor)) + geom_contour(colour = &quot;white&quot;) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Frekvence [Hz]&#39;, fill = &#39;Korelace&#39;) + coord_fixed(ratio = 1) + theme_bw() + theme(legend.position = &#39;bottom&#39;) + scale_y_continuous(expand = c(0,0) + 0.01) + scale_x_continuous(expand = c(0,0) + 0.01) p2 Code # dev.off() # ggsave(&quot;figures/DP_kap1_cor.tex&quot;, width = 6, height = 6, device = tikz) 14.1.4 B-splinová báze Podívejme se na princip, jak se pomocí splinové báze dostaneme od diskrétních naměřených hodnot k funkcionálním datům. Uvažujme pro přehlednost opět data phoneme a pouze malý počet bázových funkcí. Uvedeme tři obrázky, jeden se znárorněnými bázovými funkcemi, druhý s bázovými funkcemi přenásobenými vypočtenou hodnotou parametru a třetí výslednou křivku poskládanou sečtením jednotlivých přeškálovaných bázových funkcí. Code # definice barev cols7 &lt;- c(&quot;#12DEE8&quot;, &quot;#4ECBF3&quot;, &quot;#127DE8&quot;, &quot;#4C3CD3&quot;, &quot;#4E65F3&quot;, &quot;#4E9EF3&quot;, &quot;#081D58&quot;) cols5 &lt;- c(&quot;#12DEE8&quot;, &quot;#1D89BC&quot;, &quot;#4E9EF3&quot;, &quot;#4C3CD3&quot;, &quot;#081D58&quot;) 14.1.4.1 pro norder = 2 Code df &lt;- data.frame(x = 1:256, y = data[5, 2:257] |&gt; c() |&gt; unlist()) breaks &lt;- quantile(df$x, probs = seq(0.1, 0.9, by = 0.2)) norder &lt;- 2 rangeval &lt;- range(df$x) bbasis &lt;- create.bspline.basis(rangeval, norder = norder, breaks = breaks) BSmooth &lt;- smooth.basis(df$x, df$y, bbasis) Code fdBSmootheval &lt;- eval.fd(fdobj = BSmooth$fd, evalarg = df$x) fdB &lt;- eval.basis(basisobj = bbasis, evalarg = df$x) basisdf1 &lt;- data.frame(bs = c(fdB), x = df$x, basis = rep(colnames(fdB), each = length(df$x))) ebchan &lt;- fdB * matrix(rep(BSmooth$fd$coefs, each = length(df$x)), nrow = length(df$x)) basisdf2 &lt;- data.frame(bs = c(ebchan), x = df$x, basis = rep(colnames(fdB), each = length(df$x))) library(RColorBrewer) # tikz(file = &quot;figures/DP_kap1_Bbasis_norder2.tex&quot;, width = 12, height = 4) # samotna baze p1 &lt;- ggplot(data = basisdf1, aes(x = x, y = bs * 10, colour = basis)) + geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey2&#39;) + geom_line() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;B-splajnová báze&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + ylim(c(0, 22)) + # scale_color_brewer(palette = &#39;Blues&#39;) + # scale_color_manual(values = colorRampPalette(brewer.pal(9, &quot;YlGnBu&quot;))(12)[c(6,8,9,10,12)]) scale_color_manual(values = cols5) # prenasobena koeficienty p2 &lt;- ggplot(data = basisdf2, aes(x = x, y = bs, colour = basis)) + geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey2&#39;) + geom_line() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;B-splajnová báze (škálovaná)&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + ylim(c(0, 22)) + # scale_color_brewer() # scale_color_manual(values = colorRampPalette(brewer.pal(9, &quot;YlGnBu&quot;))(12)[c(6,8,9,10,12)]) scale_color_manual(values = cols5) # vyhlazena data p3 &lt;- ggplot(data = df, aes(x, y)) + geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey2&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval)) + theme_classic() + #guides (colour = FALSE) + ylim(c(0, 22)) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) (p1 | p2 | p3) Obrázek 1.3: B-spliny. Code # dev.off() # ggsave(&#39;figures/DP_kap1_Bbasis_norder2.tex&#39;, device = tikz) 14.1.4.2 pro norder = 4 Code df &lt;- data.frame(x = 1:256, y = data[5, 2:257] |&gt; c() |&gt; unlist()) breaks &lt;- quantile(df$x, probs = seq(0.1, 0.9, by = 0.2)) norder &lt;- 4 rangeval &lt;- range(df$x) bbasis &lt;- create.bspline.basis (rangeval, norder = norder, breaks = breaks) BSmooth &lt;- smooth.basis(df$x, df$y, bbasis) Code fdBSmootheval &lt;- eval.fd(fdobj = BSmooth$fd, evalarg = df$x) fdB &lt;- eval.basis(basisobj = bbasis, evalarg = df$x) basisdf1 &lt;- data.frame(bs = c(fdB), x = df$x, basis = rep(colnames(fdB), each = length(df$x))) ebchan &lt;- fdB * matrix(rep(BSmooth$fd$coefs, each = length(df$x)), nrow = length(df$x)) basisdf2 &lt;- data.frame(bs = c(ebchan), x = df$x, basis = rep(colnames(fdB), each = length(df$x))) # tikz(file = &quot;figures/DP_kap1_Bbasis_norder4.tex&quot;, width = 12, height = 4) # samotna baze p1 &lt;- ggplot(data = basisdf1, aes(x = x, y = bs * 10, colour = basis)) + geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey2&#39;) + geom_line() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;B-splajnová báze&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + ylim(c(0, 22)) + # scale_color_brewer(palette = &#39;Blues&#39;) + # scale_color_manual(values = colorRampPalette(brewer.pal(9, &quot;YlGnBu&quot;))(12)[c(6,8,9,10,12)]) scale_color_manual(values = cols7) # prenasobena koeficienty p2 &lt;- ggplot(data = basisdf2, aes(x = x, y = bs, colour = basis)) + geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey2&#39;) + geom_line() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;B-splajnová báze (škálovaná)&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + ylim(c(0, 22)) + # scale_color_brewer() # scale_color_manual(values = colorRampPalette(brewer.pal(9, &quot;YlGnBu&quot;))(12)[c(6,8,9,10,12)]) scale_color_manual(values = cols7) # vyhlazena data p3 &lt;- ggplot(data = df, aes(x, y)) + geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey2&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval)) + theme_classic() + #guides (colour = FALSE) + ylim(c(0, 22)) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) (p1 | p2 | p3) Obrázek 1.5: B-spliny. Code # dev.off() #ggsave(&#39;figures/DP_kap1_Bbasis_norder4.pdf&#39;) 14.1.5 Fourierova báze Podívejme se na princip, jak se pomocí Fourierovské báze dostaneme od diskrétních naměřených hodnot k funkcionálním datům. Uvažujme pro přehlednost opět data phoneme a pouze malý počet bázových funkcí. Uvedeme tři obrázky, jeden se znárorněnými bázovými funkcemi, druhý s bázovými funkcemi přenásobenými vypočtenou hodnotou parametru a třetí výslednou křivku poskládanou sečtením jednotlivých přeškálovaných bázových funkcí. 14.1.5.1 pro nbasis = 5 Code df &lt;- data.frame(x = 1:256, y = data[5, 2:257] |&gt; c() |&gt; unlist()) nbasis &lt;- 5 rangeval &lt;- range(df$x) fbasis &lt;- create.fourier.basis(rangeval, nbasis = nbasis, period = 256) FSmooth &lt;- smooth.basis(df$x, df$y, fbasis) Code fdBSmootheval &lt;- eval.fd(fdobj = FSmooth$fd, evalarg = df$x) fdF &lt;- eval.basis(basisobj = fbasis, evalarg = df$x) basisdf1 &lt;- data.frame(bs = c(fdF), x = df$x, basis = rep(colnames(fdF), each = length(df$x))) ebchan &lt;- fdF * matrix(rep(FSmooth$fd$coefs, each = length(df$x)), nrow = length(df$x)) basisdf2 &lt;- data.frame(bs = c(ebchan), x = df$x, basis = rep(colnames(fdF), each = length(df$x))) # tikz(file = &quot;figures/DP_kap1_Fbasis_nbasis5.tex&quot;, width = 12, height = 4) # samotna baze p1 &lt;- ggplot(data = basisdf1, aes(x = x, y = bs, colour = basis)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_line() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Fourierova báze&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + #ylim(c(0, 22)) + # scale_color_brewer(palette = &#39;Blues&#39;) scale_color_manual(values = cols5) # prenasobena koeficienty p2 &lt;- ggplot(data = basisdf2, aes(x = x, y = bs, colour = basis)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_line() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Fourierova báze (škálovaná)&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + #ylim(c(0, 22)) + # scale_color_brewer() scale_color_manual(values = cols5) # vyhlazena data p3 &lt;- ggplot(data = df, aes(x, y)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval)) + theme_classic() + #guides (colour = FALSE) + ylim(c(0, 22)) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) (p1 | p2 | p3) Obrázek 2.2: Fourierova baze. Code # dev.off() #ggsave(&#39;figures/DP_kap1_Fbasis_nbasis5.pdf&#39;) 14.1.5.2 pro nbasis = 7 Code df &lt;- data.frame(x = 1:256, y = data[5, 2:257] |&gt; c() |&gt; unlist()) nbasis &lt;- 7 rangeval &lt;- range(df$x) fbasis &lt;- create.fourier.basis(rangeval, nbasis = nbasis, period = 256) FSmooth &lt;- smooth.basis(df$x, df$y, fbasis) Code fdBSmootheval &lt;- eval.fd(fdobj = FSmooth$fd, evalarg = df$x) fdF &lt;- eval.basis(basisobj = fbasis, evalarg = df$x) basisdf1 &lt;- data.frame(bs = c(fdF), x = df$x, basis = rep(colnames(fdF), each = length(df$x))) ebchan &lt;- fdF * matrix(rep(FSmooth$fd$coefs, each = length(df$x)), nrow = length(df$x)) basisdf2 &lt;- data.frame(bs = c(ebchan), x = df$x, basis = rep(colnames(fdF), each = length(df$x))) # tikz(file = &quot;figures/DP_kap1_Fbasis_nbasis7.tex&quot;, width = 12, height = 4) # samotna baze p1 &lt;- ggplot(data = basisdf1, aes(x = x, y = bs, colour = basis)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_line() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Fourierova báze&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + #ylim(c(0, 22)) + # scale_color_brewer(palette = &#39;Blues&#39;) scale_color_manual(values = cols7) # prenasobena koeficienty p2 &lt;- ggplot(data = basisdf2, aes(x = x, y = bs, colour = basis)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_line() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Fourierova báze (škálovaná)&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + #ylim(c(0, 22)) + # scale_color_brewer() scale_color_manual(values = cols7) # vyhlazena data p3 &lt;- ggplot(data = df, aes(x, y)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval)) + theme_classic() + #guides (colour = FALSE) + ylim(c(0, 22)) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) (p1 | p2 | p3) Obrázek 14.1: Fourierova baze. Code # dev.off() #ggsave(&#39;figures/DP_kap1_Fbasis_nbasis7.pdf&#39;) 14.2 Materiály pro Kapitolu 2 Ve druhé sekci se podíváme na materiály pro Kapitolu 2 diplomové práce. Bude nás zajímat vliv vyhlazovacího parametru \\(\\lambda\\) na výslednou odhadnutou křivku z diskrétních dat. Dále se podíváme na funkcionální analýzu hlavních komponent. Nejprve se ale podívejme na vliv počtu bázových funkcí na výsledný odhad funkce. 14.2.1 Počet bázových funkcí a výsledný odhad Code df &lt;- data.frame(x = 1:256, y = data[5, 2:257] |&gt; c() |&gt; unlist()) norder &lt;- 4 rangeval &lt;- range(df$x) bbasis1 &lt;- create.bspline.basis (rangeval, norder = norder, nbasis = 5) BSmooth1 &lt;- smooth.basis(df$x, df$y, bbasis1) fdBSmootheval1 &lt;- eval.fd(fdobj = BSmooth1$fd, evalarg = df$x) bbasis2 &lt;- create.bspline.basis (rangeval, norder = norder, nbasis = 15) BSmooth2 &lt;- smooth.basis(df$x, df$y, bbasis2) fdBSmootheval2 &lt;- eval.fd(fdobj = BSmooth2$fd, evalarg = df$x) bbasis3 &lt;- create.bspline.basis (rangeval, norder = norder, nbasis = 25) BSmooth3 &lt;- smooth.basis(df$x, df$y, bbasis3) fdBSmootheval3 &lt;- eval.fd(fdobj = BSmooth3$fd, evalarg = df$x) # 10 bazovych funkci p1 &lt;- ggplot(data = df, aes(x, y)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval1), linewidth = 0.7) + theme_classic() + ylim(c(0, 22)) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) # 10 bazovych funkci p2 &lt;- ggplot(data = df, aes(x, y)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval2), linewidth = 0.7) + theme_classic() + ylim(c(0, 22)) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) # 10 bazovych funkci p3 &lt;- ggplot(data = df, aes(x, y)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval3), linewidth = 0.7) + theme_classic() + ylim(c(0, 22)) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) # tikz(file = &quot;figures/DP_kap2_differentnbasis.tex&quot;, width = 12, height = 4) (p1 | p2 | p3) Obrázek 5.1: B-spliny. Code # dev.off() # ggsave(&#39;figures/DP_kap2_differentnbasis.tex&#39;, width = 12, height = 4, device = tikz) 14.2.2 Volba \\(\\lambda\\) Začněme volbou vyhlazovacího parametru \\(\\lambda &gt; 0\\). S rostoucí hodnotou \\(\\lambda\\) dáváme v penalizované sumě čtverců \\[ SS_{pen} = (\\boldsymbol y - \\boldsymbol B \\boldsymbol c)^\\top (\\boldsymbol y - \\boldsymbol B \\boldsymbol c) + \\lambda \\boldsymbol c^\\top \\boldsymbol R \\boldsymbol c \\] větší váhu penalizačnímu členu, tedy dostaneme více penalizované, více hladké křivky blížící se lineární funkci. Vykreslíme si obrázky, ve kterých bude zřejmé, jak se s měnící se hodnotou \\(\\lambda\\) mění výsledná vyhlazená křivka. Ke znázornění tohoto chování použijeme data phoneme z jedné z předchozích kapitol. Vybereme jedno zajímavé pozorování a ukážeme na něm toto chování. Za uzly bereme celý vektor frekvencí (1 až 256 Hz), standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() Zvolme nyní nějakých 6 hodnot pro vyhlazovací parametr \\(\\lambda\\) a spočítejme vyhlazené křivky pro jeden vybraný záznam. Code lambdas &lt;- c(0.01, 0.1, 50, 500, 10000, 1000000) # vektor lambd tt &lt;- seq(min(t), max(t), length = 1001) # objekt, do ktereho ulozime hodnoty res_plot &lt;- matrix(NA, ncol = length(lambdas), nrow = length(tt)) for(i in 1:length(lambdas)) { curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambdas[i]) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = tt)[, 1] res_plot[, i] &lt;- fdobjSmootheval } Code options(scipen = 999) library(scales) lam_labs &lt;- paste0(&#39;$\\\\lambda = &#39;, lambdas, &quot;$&quot;) names(lam_labs) &lt;- lambdas # tikz(file = &quot;figures/DP_kap2_lambdas.tex&quot;, width = 9, height = 6) data.frame(time = rep(tt, length(lambdas)), value = c(res_plot), lambda = rep(lambdas, each = length(tt))) |&gt; # mutate(lambda = factor(lambda)) |&gt; ggplot(aes(x = time, y = value)) + geom_point(data = data.frame(time = rep(t, length(lambdas)), value = rep(c(data[5, 2:257]) |&gt; unlist(), length(lambdas)), lambda = rep(lambdas, each = length(t))) , alpha = 0.5, size = 0.75, colour = &quot;deepskyblue2&quot;) + geom_line(linewidth = 0.7, colour = &quot;grey2&quot;) + facet_wrap(~lambda, ncol = 3, nrow = 2, labeller = labeller(lambda = lam_labs)) + theme_bw() + theme(legend.position = &#39;none&#39;) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) Obrázek 5.3: Log-periodogram vybraného fonému pro zvolené hodnoty vyhlazovacího parametru. Code # scale_color_brewer() # dev.off() # ggsave(&#39;figures/DP_kap2_lambdas.pdf&#39;) 14.2.3 Vyhlazení s optimální \\(\\lambda\\) V Kapitole Aplikace na reálných datech 2 jsme zjistili optimální hodnotu vyhlazovacího parametru. Tu nyní použijeme. Code lambdas &lt;- c(175.75) tt &lt;- seq(min(t), max(t), length = 1001) # objekt, do ktereho ulozime hodnoty res_plot &lt;- matrix(NA, ncol = length(lambdas), nrow = length(tt)) for(i in 1:length(lambdas)) { curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambdas[i]) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = tt)[, 1] res_plot[, i] &lt;- fdobjSmootheval } Code options(scipen = 999) library(scales) lam_labs &lt;- paste0(&#39;$\\\\lambda = &#39;, lambdas, &quot;$&quot;) names(lam_labs) &lt;- lambdas # tikz(file = &quot;figures/DP_kap2_optimal_lambda.tex&quot;, width = 6, height = 4) data.frame(time = rep(tt, length(lambdas)), value = c(res_plot), lambda = rep(lambdas, each = length(tt))) |&gt; # mutate(lambda = factor(lambda)) |&gt; ggplot(aes(x = time, y = value)) + geom_point(data = data.frame(time = rep(t, length(lambdas)), value = rep(c(data[5, 2:257]) |&gt; unlist(), length(lambdas)), lambda = rep(lambdas, each = length(t))) , alpha = 0.5, size = 0.75, colour = &quot;deepskyblue2&quot;) + geom_line(linewidth = 0.7, colour = &quot;grey2&quot;) + # facet_wrap(~lambda, ncol = 3, nrow = 2, labeller = labeller(lambda = lam_labs)) + theme_bw() + theme(legend.position = &#39;none&#39;) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) Obrázek 2.4: Log-periodogram vybraného fonému pro zvolené hodnoty vyhlazovacího parametru. Code # scale_color_brewer() # dev.off() # ggsave(&#39;figures/DP_kap2_lambdas.pdf&#39;) 14.2.4 Funkcionální PCA Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = 1, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(XXfd, nharm = 20) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p # data.PCA &lt;- pca.fd(XXfd, nharm = 20) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(y) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 9, které dohromady vysvětlují 90.47 % variability v datech. První hlavní komponenta potom vysvětluje 44.79 % a druhá 13.37 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code p1 &lt;- data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.2, alpha = 0.75) + labs(x = paste(&#39;1. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[1], 2), &#39;\\\\%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;\\\\%)&#39;), colour = &#39;Foném&#39;) + # scale_colour_discrete(labels = phoneme_subset) + theme_bw() + theme(legend.position = &#39;none&#39;) + lims(x = c(-70, 62), y = c(-70, 62)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) p2 &lt;- data.PCA.train |&gt; ggplot(aes(x = V1, y = V3, colour = Y)) + geom_point(size = 1.2, alpha = 0.75) + labs(x = paste(&#39;1. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[1], 2), &#39;\\\\%)&#39;), y = paste(&#39;3. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[3], 2), &#39;\\\\%)&#39;), colour = &#39;Foném&#39;) + # scale_colour_discrete(labels = phoneme_subset) + theme_bw() + theme(legend.position = &#39;none&#39;) + lims(x = c(-70, 62), y = c(-70, 62)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) p3 &lt;- data.PCA.train |&gt; ggplot(aes(x = V2, y = V3, colour = Y)) + geom_point(size = 1.2, alpha = 0.75) + labs(x = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;\\\\%)&#39;), y = paste(&#39;3. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[3], 2), &#39;\\\\%)&#39;), colour = &#39;Foném&#39;) + # scale_colour_discrete(labels = phoneme_subset) + theme_bw() + lims(x = c(-70, 62), y = c(-70, 62)) + theme(legend.position = c(0.84, 0.84)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) (p1|p2|p3) Obrázek 5.4: Hodnoty skórů prvních tří hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Code # tikz(file = &quot;figures/kap2_PCA_scores1.tex&quot;, width = 3, height = 3) # p1 # dev.off() # tikz(file = &quot;figures/kap2_PCA_scores2.tex&quot;, width = 3, height = 3) # p2 # dev.off() # tikz(file = &quot;figures/kap2_PCA_scores3.tex&quot;, width = 3, height = 3) # p3 # dev.off() # ggsave(&quot;figures/kap2_PCA_scores.tex&quot;, device = tikz, width = 12, height = 4) Code # 3D plot library(plotly) plot_ly(data = data.PCA.train, x = ~V1, y = ~V2, z = ~V3, color = ~Y, colors = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), type=&quot;scatter3d&quot;, mode=&quot;markers&quot;, marker = list(size = 2.5)) %&gt;% layout(scene = list(xaxis = list(title = &#39;1. hlavní komponenta&#39;), yaxis = list(title = &#39;2. hlavní komponenta&#39;), zaxis = list(title = &#39;3. hlavní komponenta&#39;))) Code data.frame(x = 1:15, y = pca.fd(XXfd, nharm = 15)$varprop |&gt; cumsum() * 100) |&gt; ggplot(aes(x, y)) + geom_point(col = &#39;deepskyblue2&#39;) + geom_line(col = &#39;deepskyblue2&#39;) + theme_bw() + labs(x = &#39;Počet hlavních komponent&#39;, y = &quot;Kumulativní vysvětlená variabilita [v \\\\%]&quot;) + geom_hline(aes(yintercept = 90), linetype = &#39;dashed&#39;, col = &#39;grey2&#39;) + scale_x_continuous(breaks = 1:15) + theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank()) Obrázek 2.5: Kumulativní vysvětlená variabilita [v %] proti po4tu hlavn9ch komponent. Code # ggsave(&quot;figures/kap2_PCA_nharm.tex&quot;, device = tikz, width = 5, height = 3) Code ## Looking at the principal components: fdobjPCAeval &lt;- eval.fd(fdobj = data.PCA$harmonics[1:3], evalarg = t) df.comp &lt;- data.frame( time = t, harmonics = c(fdobjPCAeval), component = factor(rep(1:3, each = 256)) ) df.comp |&gt; ggplot(aes(x = time, y = harmonics, color = component)) + geom_line(linewidth = 0.7) + labs(x = &quot;Frekvence [v Hz]&quot;, y = &quot;Hlavní komponenty&quot;, colour = &#39;&#39;) + theme_bw() + scale_color_manual(values=c(&quot;#127DE8&quot;, &quot;#4C3CD3&quot;, &quot;#12DEE8&quot;)) Obrázek 12.1: Hlavní komponenty. Code # ggsave(&quot;figures/kap2_PCA_components.tex&quot;, device = tikz, width = 5, height = 3) Code library(plyr) freq3 &lt;- seq(1,256) fdobjPCAeval3 &lt;- eval.fd(fdobj = data.PCA$harmonics[1:3], evalarg = freq3) df.comp3 &lt;- data.frame( freq = freq3, harmonics = c(fdobjPCAeval3), component = factor(rep(1:3, each = length(freq3))) ) fdm3 &lt;- c(eval.fd(fdobj = meanfd, evalarg = freq3)) df.3 &lt;- data.frame(df.comp3, m = fdm3) df.pv3 &lt;- ddply(df.3, .(component), mutate, m1 = m + 2*sqrt(data.PCA$values[component])*harmonics, m2 = m - 2*sqrt(data.PCA$values[component])*harmonics, pov = paste0(&quot;Komponenta &quot;, component,&quot;, Vysvětlená variabilita = &quot;, round(100*data.PCA$varprop[component], 1), &#39; \\\\%&#39;)) df.pv3 |&gt; ggplot(aes (x = freq, y = m)) + geom_line() + geom_line(aes(y = m1), linetype = &#39;solid&#39;, color = &#39;deepskyblue2&#39;) + geom_line(aes(y = m2), linetype = &#39;dashed&#39;, color = &#39;deepskyblue2&#39;) + labs(x = &quot;Frekvence [v Hz]&quot;, y = &quot;Log-periodogram&quot;, colour = &#39;Komponenta&#39;) + theme_bw() + theme(legend.position = &#39;none&#39;) + facet_wrap(~ pov, nrow = 1) Obrázek 5.5: Vliv komponent. Code # ggsave(&quot;figures/kap2_PCA_impactofcomponents.tex&quot;, device = tikz, width = 9, height = 3) Code meanfd &lt;- mean.fd(XXfd) fdm &lt;- eval.fd(fdobj = meanfd, evalarg = t) colnames(fdm) &lt;- NULL scores &lt;- data.PCA$scores PCs &lt;- eval.fd(fdobj = data.PCA$harmonics, evalarg = t) # vyhodnoceni df &lt;- data.frame(dfs[1:256, ], reconstruction = fdm, estimate = &quot;mean&quot;) p0 &lt;- ggplot(data = df, aes (x = time, y = value)) + geom_line(color = &quot;grey2&quot;, linewidth = 0.5, alpha = 0.7) + geom_line(aes(y = reconstruction), colour = &quot;deepskyblue2&quot;, linewidth = 0.6) + labs(x = &quot;Frekvence [v Hz]&quot;, y = &quot;Log-periodogram&quot;) + theme_bw() print(p0) Obrázek 1.9: Původní křivka a průměr. Code for (k in 1:20){ df1 &lt;- data.frame(dfs[1:256, ], reconstruction = fdm + c(PCs[, 1:k] %*% t(scores[, 1:k]))[1:256], estimate = paste0(&quot;comp&quot;, k)) df &lt;- rbind(df, df1) p1 &lt;- ggplot(data = df1, aes (x = time, y = value)) + geom_line(color = &quot;grey2&quot;, linewidth = 0.5, alpha = 0.7) + geom_line(aes(y = reconstruction), colour = &quot;deepskyblue2&quot;, linewidth = 0.6) + labs(x = &quot;Frekvence [v Hz]&quot;, y = &quot;Log-periodogram&quot;) + theme_bw() # print(p1) } df |&gt; mutate(estimate = factor(estimate)) |&gt; filter(estimate %in% c(&#39;mean&#39;, &#39;comp1&#39;, &#39;comp2&#39;, &#39;comp3&#39;, &#39;comp9&#39;, &#39;comp20&#39;)) |&gt; mutate(estimate = factor(estimate, levels = c(&#39;mean&#39;, &#39;comp1&#39;, &#39;comp2&#39;, &#39;comp3&#39;, &#39;comp9&#39;, &#39;comp20&#39;))) |&gt; ggplot(aes (x = time, y = value)) + geom_line(color = &quot;grey2&quot;, linewidth = 0.5, alpha = 0.5) + geom_line(aes(y = reconstruction), colour = &quot;deepskyblue2&quot;, linewidth = 0.7) + labs(x = &quot;Frekvence [v Hz]&quot;, y = &quot;Log-periodogram&quot;) + theme_bw() + facet_wrap(~estimate, ncol = 3, nrow = 2) Obrázek 11.1: Původní křivka a její rekonstrukce. Code # ggsave(&quot;figures/kap2_PCA_reconstruction.tex&quot;, device = tikz, width = 9, height = 6) 14.3 Materiály pro Kapitolu 3 Tyto materiály jsou převzaty z Kapitoly 11. 14.4 Materiály pro Kapitolu 4 V této sekci uvedeme podpůrné grafy pro čtvrtou kapitolu diplomové práce. 14.4.1 Maximal margin classifier Nejprve simulujeme data ze dvou klasifikačních tříd, které budou lineárně separabilní. Code library(MASS) library(dplyr) library(ggplot2) set.seed(21) # simulace dat n_0 &lt;- 40 n_1 &lt;- 40 mu_0 &lt;- c(0, 0) mu_1 &lt;- c(3, 4.5) Sigma_0 &lt;- matrix(c(1.3, -0.7, -0.7, 1.3), ncol = 2) Sigma_1 &lt;- matrix(c(1.5, -0.25, -0.25, 1.5), ncol = 2) df_MMC &lt;- rbind( mvrnorm(n = n_0, mu = mu_0, Sigma = Sigma_0), mvrnorm(n = n_1, mu = mu_1, Sigma = Sigma_1)) |&gt; as.data.frame() |&gt; mutate(Y = rep(c(&#39;-1&#39;, &#39;1&#39;), c(n_0, n_1))) colnames(df_MMC) &lt;- c(&#39;x1&#39;, &#39;x2&#39;, &#39;Y&#39;) Nyní vykreslíme data. Code p1 &lt;- ggplot(data = df_MMC, aes(x = x1, y = x2, colour = Y)) + geom_point() + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.title.align = 0.5) + scale_x_continuous(breaks = seq(-2, 6, by = 2), limits = c(-3.5, 6.5)) + scale_y_continuous(breaks = seq(-4, 8, by = 2), limits = c(-2.5, 7)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) + labs(x = &#39;$X_1$&#39;, y = &#39;$X_2$&#39;, colour = &#39;Klasifikační\\n třída&#39;) p1 Natrénujeme klasifikátor a vykreslíme dělicí nadrovinu společně s podpůrnými vektory. Code library(e1071) clf &lt;- svm(Y ~ ., data = df_MMC, type = &#39;C-classification&#39;, scale = FALSE, kernel = &#39;linear&#39;) Code df_SV &lt;- df_MMC[clf$index, ] p2 &lt;- p1 + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.7, size = 2) p2 Dokreslíme dělicí nadrovinu. Code # vektor koeficientů w &lt;- t(clf$coefs) %*% clf$SV slope &lt;- - w[1] / w[2] intercept &lt;- clf$rho / w[2] p3 &lt;- p2 + geom_abline(slope = slope, intercept = intercept, col = &#39;grey2&#39;, linewidth = 0.7, alpha = 0.8) + geom_abline(slope = slope, intercept = intercept - 1 / w[2], col = &#39;grey2&#39;, linewidth = 0.5, alpha = 0.8, linetype = &#39;dashed&#39;) + geom_abline(slope = slope, intercept = intercept + 1 / w[2], col = &#39;grey2&#39;, linewidth = 0.5, alpha = 0.8, linetype = &#39;dashed&#39;) + geom_point() + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.4, size = 2) p3 Code # ggsave(&quot;figures/kap4_MMC.tex&quot;, device = tikz, width = 6, height = 4) 14.4.2 Support vector classifier Nejprve simulujeme data ze dvou klasifikačních tříd, které budou lineárně neseparabilní. Code set.seed(42) # simulace dat n_0 &lt;- 50 n_1 &lt;- 50 mu_0 &lt;- c(0, 0) mu_1 &lt;- c(3, 4.5) Sigma_0 &lt;- matrix(c(2, -0.55, -0.55, 2), ncol = 2) Sigma_1 &lt;- matrix(c(2.75, -0.3, -0.3, 2.75), ncol = 2) df_MMC &lt;- rbind( mvrnorm(n = n_0, mu = mu_0, Sigma = Sigma_0), mvrnorm(n = n_1, mu = mu_1, Sigma = Sigma_1)) |&gt; as.data.frame() |&gt; mutate(Y = rep(c(&#39;-1&#39;, &#39;1&#39;), c(n_0, n_1))) colnames(df_MMC) &lt;- c(&#39;x1&#39;, &#39;x2&#39;, &#39;Y&#39;) Nyní vykreslíme data. Code p1 &lt;- ggplot(data = df_MMC, aes(x = x1, y = x2, colour = Y)) + geom_point() + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.title.align = 0.5) + scale_x_continuous(breaks = seq(-2, 6, by = 2), limits = c(-2.75, 6)) + scale_y_continuous(breaks = seq(-4, 8, by = 2), limits = c(-4.5, 8)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) + labs(x = &#39;$X_1$&#39;, y = &#39;$X_2$&#39;, colour = &#39;Klasifikační\\n třída&#39;) p1 Natrénujeme klasifikátor a vykreslíme dělicí nadrovinu společně s podpůrnými vektory. Code clf &lt;- svm(Y ~ ., data = df_MMC, type = &#39;C-classification&#39;, scale = FALSE, kernel = &#39;linear&#39;) Code df_SV &lt;- df_MMC[clf$index, ] p2 &lt;- p1 + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.7, size = 2) p2 Dokreslíme dělicí nadrovinu. Code # vektor koeficientů w &lt;- t(clf$coefs) %*% clf$SV slope &lt;- - w[1] / w[2] intercept &lt;- clf$rho / w[2] p3 &lt;- p2 + geom_abline(slope = slope, intercept = intercept, col = &#39;grey2&#39;, linewidth = 0.7, alpha = 0.8) + geom_abline(slope = slope, intercept = intercept - 1 / w[2], col = &#39;grey2&#39;, linewidth = 0.5, alpha = 0.8, linetype = &#39;dashed&#39;) + geom_abline(slope = slope, intercept = intercept + 1 / w[2], col = &#39;grey2&#39;, linewidth = 0.5, alpha = 0.8, linetype = &#39;dashed&#39;) + geom_point() + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.4, size = 2) p3 Code # ggsave(&quot;figures/kap4_SVC.tex&quot;, device = tikz, width = 6, height = 4) Nakonec přidáme popisky k podpůrným vektorům. Code df_SVlab &lt;- cbind(df_SV, data.frame(label = 1:dim(df_SV)[1])) p4 &lt;- p3 + geom_text(data = df_SVlab, aes(label = label), colour = &#39;grey2&#39;, check_overlap = T, size = 3, vjust = -0.55, hjust = 1) p4 Code # ggsave(&quot;figures/kap4_SVC.tex&quot;, device = tikz, width = 6, height = 4) 14.4.2.1 Změna šířky tolerančního pásma při změně hyperparametru \\(C\\) Podívejme se ještě na změnu tolerančního pásma v závislosti na hyperparametru \\(C\\). Code C &lt;- c(0.005, 0.1, 100) clf1 &lt;- svm(Y ~ ., data = df_MMC, type = &#39;C-classification&#39;, scale = FALSE, kernel = &#39;linear&#39;, cost = C[1]) clf2 &lt;- svm(Y ~ ., data = df_MMC, type = &#39;C-classification&#39;, scale = FALSE, kernel = &#39;linear&#39;, cost = C[2]) clf3 &lt;- svm(Y ~ ., data = df_MMC, type = &#39;C-classification&#39;, scale = FALSE, kernel = &#39;linear&#39;, cost = C[3]) df_SV &lt;- rbind(df_MMC[clf1$index, ] |&gt; mutate(cost = C[1]), df_MMC[clf2$index, ] |&gt; mutate(cost = C[2]), df_MMC[clf3$index, ] |&gt; mutate(cost = C[3])) p2 &lt;- p1 + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.7, size = 2) + facet_wrap(~cost) # vektor koeficientů w &lt;- t(clf1$coefs) %*% clf1$SV slope &lt;- - w[1] / w[2] intercept &lt;- clf1$rho / w[2] df_lines &lt;- data.frame(slope = slope, intercept = intercept, lb = intercept - 1 / w[2], rb = intercept + 1 / w[2], cost = C[1]) # pro clf2 w &lt;- t(clf2$coefs) %*% clf2$SV slope &lt;- - w[1] / w[2] intercept &lt;- clf2$rho / w[2] df_lines &lt;- rbind(df_lines, data.frame(slope = slope, intercept = intercept, lb = intercept - 1 / w[2], rb = intercept + 1 / w[2], cost = C[2]) ) # pro clf3 w &lt;- t(clf3$coefs) %*% clf3$SV slope &lt;- - w[1] / w[2] intercept &lt;- clf3$rho / w[2] df_lines &lt;- rbind(df_lines, data.frame(slope = slope, intercept = intercept, lb = intercept - 1 / w[2], rb = intercept + 1 / w[2], cost = C[3]) ) p3 &lt;- p2 + geom_abline(data = df_lines, aes(slope = slope, intercept = intercept), col = &#39;grey2&#39;, linewidth = 0.7, alpha = 0.8) + geom_abline(data = df_lines, aes(slope = slope, intercept = lb), col = &#39;grey2&#39;, linewidth = 0.5, alpha = 0.8, linetype = &#39;dashed&#39;) + geom_abline(data = df_lines, aes(slope = slope, intercept = rb), col = &#39;grey2&#39;, linewidth = 0.5, alpha = 0.8, linetype = &#39;dashed&#39;) + geom_point() + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.4, size = 2) + theme(legend.position = &#39;none&#39;) p3 Code # ggsave(&quot;figures/kap4_SVC_C_dependency.tex&quot;, device = tikz, width = 8, height = 3) 14.4.3 Support vector machines Pro ilustraci této metody využijeme data tecator, kterým se podrobně věnujeme v Kapitole 11. Code # nacteni dat library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) data &lt;- ddalpha::dataf.tecator() data.gr &lt;- data$dataf[[1]]$vals for(i in 2:length(data$labels)) { data.gr &lt;- rbind(data.gr, data$dataf[[i]]$vals) } data.gr &lt;- cbind(data.frame(wave = data$dataf[[1]]$args), t(data.gr)) # vektor trid labels &lt;- data$labels |&gt; unlist() # prejmenovani podle tridy colnames(data.gr) &lt;- c(&#39;wavelength&#39;, paste0(labels, 1:length(data$labels))) Code t &lt;- data.gr$wavelength rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # penalizujeme 4. derivaci # spojeni pozorovani do jedne matice XX &lt;- data.gr[, -1] |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast set.seed(42) library(caTools) split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 # Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) # X.train &lt;- XXfd # Y.train &lt;- Y table(Y.train) ## Y.train ## 0 1 ## 91 59 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.6066667 0.3933333 Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) k_cv &lt;- 10 # rozdelime trenovaci data na k casti library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;lattice&#39; ## The following object is masked from &#39;package:fda&#39;: ## ## melanoma ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift Code folds &lt;- createMultiFolds(1:length(Y.train), k = k_cv, time = 1) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- c(2, 3, 4, 5) coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1) - 5, to = max(data.PCA.train$V1) + 5, length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2) - 5, to = max(data.PCA.train$V2) + 5, length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;Lineární&#39;, &#39;Polynomiální&#39;, &#39;Radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) df_SV &lt;- rbind(data.PCA.train[clf.SVM.l.PCA$index, ] |&gt; mutate(kernel = &#39;Lineární&#39;), data.PCA.train[clf.SVM.p.PCA$index, ] |&gt; mutate(kernel = &#39;Polynomiální&#39;), data.PCA.train[clf.SVM.r.PCA$index, ] |&gt; mutate(kernel = &#39;Radiální&#39;)) pSVM &lt;- ggplot(data = data.PCA.train, aes(x = V1, y = V2, colour = Y)) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;grey2&#39;, size = 0.25) + labs(x = &#39;$X_1$&#39;, y = &#39;$X_2$&#39;, colour = &#39;Klasifikační\\n třída&#39;, fill = &#39;none&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.title.align = 0.5, legend.position = &#39;none&#39;) + scale_y_continuous(expand = c(-0.02, -0.02), limits = c(-3.5, 2.5)) + scale_x_continuous(expand = c(-0.02, -0.02), limits = c(-15, 26)) + facet_wrap(~kernel) + geom_contour_filled(data = nd, aes(x = V1, y = V2, z = prd, colour = prd), breaks = c(1, 2, 3), alpha = 0.1, show.legend = F) + scale_fill_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.7, size = 1.5) + geom_point(size = 1.2) + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.4, size = 1.5) pSVM Obrázek 10.4: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code # ggsave(&quot;figures/kap4_SVM.tex&quot;, device = tikz, width = 8, height = 3) 14.5 Materiály pro Kapitolu 5 V této sekci uvedeme podpůrné grafy pro pátou kapitolu diplomové práce. 14.5.1 Diskretizace intervalu Chtěli bychom se podívat na hodnoty skalárních součinů funkcí, které jsou blízko u sebe a naopak které se tvarem velmi liší. 14.5.1.1 tecator data Podívejme se také na data tecator. Code data &lt;- ddalpha::dataf.tecator() data.gr &lt;- data$dataf[[1]]$vals for(i in 2:length(data$labels)) { data.gr &lt;- rbind(data.gr, data$dataf[[i]]$vals) } data.gr &lt;- cbind(data.frame(wave = data$dataf[[1]]$args), t(data.gr)) # vektor trid labels &lt;- data$labels |&gt; unlist() # prejmenovani podle tridy colnames(data.gr) &lt;- c(&#39;wavelength&#39;, paste0(labels, 1:length(data$labels))) Code library(fda.usc) t &lt;- data.gr$wavelength rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # penalizujeme 4. derivaci # spojeni pozorovani do jedne matice XX &lt;- data.gr[, -1] |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd # * as.numeric(1 / norm.fd(BSmooth$fd[1])) # set norm equal to one norms &lt;- c() for (i in 1:215) {norms &lt;- c(norms, as.numeric(1 / norm.fd(BSmooth$fd[i])))} XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = 215, nrow = 104, byrow = T) fdobjSmootheval &lt;- eval.fd(fdobj = XXfd_norm, evalarg = t) # rozdeleni na testovaci a trenovaci cast set.seed(42) library(caTools) split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 # Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) # X.train &lt;- XXfd # Y.train &lt;- Y Spočítáme skalární součiny prvního s ostatními. Code nn &lt;- 3 Inprod_vect &lt;- inprod(XXfd_norm[nn], XXfd_norm[1:dim(XXfd$coefs)[2]]) labels[order(Inprod_vect)[1:10]] ## [1] &quot;large&quot; &quot;large&quot; &quot;large&quot; &quot;small&quot; &quot;small&quot; &quot;small&quot; &quot;small&quot; &quot;small&quot; &quot;small&quot; ## [10] &quot;small&quot; Code labels[rev(order(Inprod_vect))[1:10]] ## [1] &quot;small&quot; &quot;small&quot; &quot;small&quot; &quot;small&quot; &quot;small&quot; &quot;small&quot; &quot;small&quot; &quot;small&quot; &quot;small&quot; ## [10] &quot;small&quot; Code n &lt;- dim(XX)[2] abs.labs &lt;- c(&quot;$&lt; 20 \\\\%$&quot;, &quot;$&gt; 20 \\\\%$&quot;) # abs.labs &lt;- c(&quot;$Y = {-1}$&quot;, &quot;$Y = 1$&quot;) names(abs.labs) &lt;- c(&#39;small&#39;, &#39;large&#39;) DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Fat = factor(rep(labels, each = length(t)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , labels == &#39;small&#39;], 1, mean), apply(fdobjSmootheval[ , labels == &#39;large&#39;], 1, mean)), Fat = factor(rep(c(&#39;small&#39;, &#39;large&#39;), each = length(t)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFsmooth |&gt; filter(time %in% as.character(c(nn))) |&gt; ggplot(aes(x = t, y = Smooth, color = Fat)) + geom_line(linewidth = 1.1, aes(group = time, linetype = &#39;apozor x1&#39;)) + geom_line(data = DFsmooth |&gt; filter(time %in% as.character(order(Inprod_vect)[1:4])), aes(group = time, linetype = &#39;nejmensi&#39;), linewidth = 0.6) + geom_line(data = DFsmooth |&gt; filter(time %in% as.character(rev(order(Inprod_vect))[1:5])), aes(group = time, linetype = &#39;nejvetsi&#39;), linewidth = 0.6) + geom_line(linewidth = 1.1, aes(group = time, linetype = &#39;apozor x1&#39;)) + theme_bw() + # facet_wrap(~Fat, # labeller = labeller(Fat = abs.labs)) + labs(x = &quot;Vlnová délka [v nm]&quot;, y = &quot;Absorbance&quot;, colour = &#39;Obsah tuku&#39;,#&quot;Klasifikační\\n třída&quot;, linetype = &#39;Styl čáry&#39;) + # scale_color_discrete(guide=&quot;none&quot;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), labels = abs.labs) + # guides(color = guide_legend(position = &#39;none&#39;)) + # scale_color_discrete(labels = abs.labs) + scale_linetype_manual(values=c(&#39;solid&#39;, &quot;dotted&quot;, &quot;longdash&quot;)) + theme(legend.position = c(0.15, 0.75), #legend.box=&quot;vertical&quot;, panel.grid = element_blank()) Obrázek 3.8: Vykreslení prvních 100 vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čarou je zakreslen průměr pro každou třídu. Code # ggsave(&quot;figures/kap5_discretization_tecator.tex&quot;, device = tikz, width = 4.5, height = 4.5) 14.5.1.2 phoneme data Použijeme data phoneme. Code library(fda.usc) # nacteni dat data &lt;- read.delim2(&#39;phoneme.txt&#39;, header = T, sep = &#39;,&#39;) # zmenime dve promenne na typ factor data &lt;- data |&gt; mutate(g = factor(g), speaker = factor(speaker)) # numericke promenne prevedeme opravdu na numericke data[, 2:257] &lt;- as.numeric(data[, 2:257] |&gt; as.matrix()) tr_vs_test &lt;- str_split(data$speaker, &#39;\\\\.&#39;) |&gt; unlist() tr_vs_test &lt;- tr_vs_test[seq(1, length(tr_vs_test), by = 4)] data$train &lt;- ifelse(tr_vs_test == &#39;train&#39;, TRUE, FALSE) # vybrane fonemy ke klasifikaci phoneme_subset &lt;- c(&#39;aa&#39;, &#39;ao&#39;) # testovaci a trenovaci data data_train &lt;- data |&gt; filter(train) |&gt; filter(g %in% phoneme_subset) data_test &lt;- data |&gt; filter(!train) |&gt; filter(g %in% phoneme_subset) # odstranime sloupce, ktere nenesou informaci o frekvenci a # transponujeme tak, aby ve sloupcich byly jednotlive zaznamy X_train &lt;- data_train[, -c(1, 258, 259, 260)] |&gt; t() X_test &lt;- data_test[, -c(1, 258, 259, 260)] |&gt; t() # prejmenujeme radky a sloupce rownames(X_train) &lt;- 1:256 colnames(X_train) &lt;- paste0(&#39;train&#39;, data_train$row.names) rownames(X_test) &lt;- 1:256 colnames(X_test) &lt;- paste0(&#39;test&#39;, data_test$row.names) # definujeme vektor fonemu y_train &lt;- data_train[, 258] |&gt; factor(levels = phoneme_subset) y_test &lt;- data_test[, 258] |&gt; factor(levels = phoneme_subset) y &lt;- c(y_train, y_test) Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = 1, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(BSmooth$fd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) fdobjSmootheval &lt;- eval.fd(fdobj = XXfd_norm, evalarg = t) Spočítáme skalární součiny prvního log-periodogramu s ostatními. Code nn &lt;- 1 Inprod_vect &lt;- inprod(XXfd_norm[nn], XXfd_norm[1:dim(XXfd$coefs)[2]]) y[(order(Inprod_vect))[1:10]] ## [1] ao ao ao ao ao ao ao ao ao ao ## Levels: aa ao Code y[rev(order(Inprod_vect))[1:10]] ## [1] ao aa aa aa aa ao aa aa aa aa ## Levels: aa ao Code n &lt;- dim(XX)[2] y &lt;- c(y_train, y_test) DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Phoneme = rep(y, each = length(t))) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , y == phoneme_subset[1]], 1, mean), apply(fdobjSmootheval[ , y == phoneme_subset[2]], 1, mean)), Phoneme = factor(rep(phoneme_subset, each = length(t)), levels = levels(y)) ) DFsmooth |&gt; filter(time %in% as.character(nn)) |&gt; ggplot(aes(x = t, y = Smooth, color = Phoneme)) + geom_line(linewidth = 1.1, aes(group = time, linetype = &#39;apozor x1&#39;)) + geom_line(data = DFsmooth |&gt; filter(time %in% as.character(order(Inprod_vect)[1:4])), aes(group = time, linetype = &#39;nejmensi&#39;), linewidth = 0.6) + geom_line(data = DFsmooth |&gt; filter(time %in% as.character(rev(order(Inprod_vect))[1:5])), aes(group = time, linetype = &#39;nejvetsi&#39;), linewidth = 0.6) + geom_line(linewidth = 1.1, aes(group = time, linetype = &#39;apozor x1&#39;)) + theme_bw() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;, linetype = &#39;Styl čáry&#39;) + # scale_colour_discrete(labels = phoneme_subset) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), labels = phoneme_subset) + scale_linetype_manual(values=c(&#39;solid&#39;, &quot;dotted&quot;, &quot;longdash&quot;)) + theme(legend.position = c(0.8, 0.75), panel.grid = element_blank()) Obrázek 6.1: Vykreslení prvních 100 vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čarou je zakreslen průměr pro každou třídu. Code # ggsave(&quot;figures/kap5_discretization_phoneme.tex&quot;, device = tikz, width = 4.5, height = 4.5) 14.5.2 Support vector regression (SVR) Ukázka metody SVR na obou datových souborech. 14.5.2.1 tecator data Code t &lt;- data.gr$wavelength rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # penalizujeme 4. derivaci # spojeni pozorovani do jedne matice XX &lt;- data.gr[, -1] |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Code library(e1071) library(caret) df_plot &lt;- data.frame() # model for(i in 1:5) { df.svm &lt;- data.frame(x = t, y = fdobjSmootheval[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.03, gamma = 0.5, cost = 1, tolerance = 0.001, shrinking = TRUE, scale = TRUE) svm.RKHS &lt;- train(y ~ x, data = df.svm, method = &#39;svmRadial&#39;, metric = &quot;RMSE&quot;, preProcess = c(&#39;center&#39;, &#39;scale&#39;), trControl = trainControl( method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE ) # trControl = trainControl(method = &quot;none&quot;), # Telling caret not to re-tune # tuneGrid = data.frame(sigma = 1000, C = 1000) # Specifying the parameters ) df_plot &lt;- rbind( df_plot, data.frame( x = t, y = svm.RKHS$finalModel@fitted * svm.RKHS$finalModel@scaling$y.scale$`scaled:scale` + svm.RKHS$finalModel@scaling$y.scale$`scaled:center`, line = &#39;estimate&#39;, curve = as.character(i)) |&gt; rbind(data.frame( x = t, y = fdobjSmootheval[, i], line = &#39;sample&#39;, curve = as.character(i) ))) } Vykresleme si pro lepší představu odhad křivky (červeně) společně s pozorovanou křivkou (modře). Code df_plot |&gt; filter(curve %in% c(&#39;1&#39;, &#39;2&#39;, &#39;3&#39;)) |&gt; ggplot(aes(x, y, col = line, linetype = curve)) + geom_line(linewidth = 0.8) + theme_bw() + labs(x = &quot;Vlnová délka [v nm]&quot;, y = &quot;Absorbance&quot;, colour = &#39;Křivka&#39;) + # scale_colour_discrete(labels = phoneme_subset) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), labels = c(&quot;odhadnutá&quot;, &quot;pozorovaná&quot;)) + scale_linetype_manual(values = c(&#39;solid&#39;, &quot;dotted&quot;, &quot;dashed&quot;)) + theme(legend.position = c(0.17, 0.85), panel.grid = element_blank()) + guides(linetype = &#39;none&#39;) Obrázek 1.20: Porovnání pozorované a odhadnuté křivky. Code # ggsave(&quot;figures/kap5_SVR_tecator.tex&quot;, device = tikz, width = 4.5, height = 4.5) 14.5.2.2 phoneme data Code data &lt;- read.delim2(&#39;phoneme.txt&#39;, header = T, sep = &#39;,&#39;) # zmenime dve promenne na typ factor data &lt;- data |&gt; mutate(g = factor(g), speaker = factor(speaker)) # numericke promenne prevedeme opravdu na numericke data[, 2:257] &lt;- as.numeric(data[, 2:257] |&gt; as.matrix()) tr_vs_test &lt;- str_split(data$speaker, &#39;\\\\.&#39;) |&gt; unlist() tr_vs_test &lt;- tr_vs_test[seq(1, length(tr_vs_test), by = 4)] data$train &lt;- ifelse(tr_vs_test == &#39;train&#39;, TRUE, FALSE) # vybrane fonemy ke klasifikaci phoneme_subset &lt;- c(&#39;aa&#39;, &#39;ao&#39;) # testovaci a trenovaci data data_train &lt;- data |&gt; filter(train) |&gt; filter(g %in% phoneme_subset) data_test &lt;- data |&gt; filter(!train) |&gt; filter(g %in% phoneme_subset) # odstranime sloupce, ktere nenesou informaci o frekvenci a # transponujeme tak, aby ve sloupcich byly jednotlive zaznamy X_train &lt;- data_train[, -c(1, 258, 259, 260)] |&gt; t() X_test &lt;- data_test[, -c(1, 258, 259, 260)] |&gt; t() # prejmenujeme radky a sloupce rownames(X_train) &lt;- 1:256 colnames(X_train) &lt;- paste0(&#39;train&#39;, data_train$row.names) rownames(X_test) &lt;- 1:256 colnames(X_test) &lt;- paste0(&#39;test&#39;, data_test$row.names) # definujeme vektor fonemu y_train &lt;- data_train[, 258] |&gt; factor(levels = phoneme_subset) y_test &lt;- data_test[, 258] |&gt; factor(levels = phoneme_subset) y &lt;- c(y_train, y_test) Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = 1, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Code df_plot &lt;- data.frame() # model for(i in 1:5) { df.svm &lt;- data.frame(x = t, y = fdobjSmootheval[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.03, gamma = 0.5, cost = 1, tolerance = 0.001, shrinking = TRUE, scale = TRUE) svm.RKHS &lt;- train(y ~ x, data = df.svm, method = &#39;svmRadial&#39;, metric = &quot;RMSE&quot;, preProcess = c(&#39;center&#39;, &#39;scale&#39;), trControl = trainControl( method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE ) # trControl = trainControl(method = &quot;none&quot;), # Telling caret not to re-tune # tuneGrid = data.frame(sigma = 1000, C = 1000) # Specifying the parameters ) df_plot &lt;- rbind( df_plot, data.frame( x = t, y = svm.RKHS$finalModel@fitted * svm.RKHS$finalModel@scaling$y.scale$`scaled:scale` + svm.RKHS$finalModel@scaling$y.scale$`scaled:center`, line = &#39;estimate&#39;, curve = as.character(i)) |&gt; rbind(data.frame( x = t, y = fdobjSmootheval[, i], line = &#39;sample&#39;, curve = as.character(i) ))) } Vykresleme si pro lepší představu odhad křivky (červeně) společně s pozorovanou křivkou (modře). Code df_plot |&gt; filter(curve %in% c(&#39;1&#39;, &#39;5&#39;, &#39;3&#39;)) |&gt; ggplot(aes(x, y, col = line, linetype = curve)) + geom_line(linewidth = 0.8) + theme_bw() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Křivka&#39;) + # scale_colour_discrete(labels = phoneme_subset) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), labels = c(&quot;odhadnutá&quot;, &quot;pozorovaná&quot;)) + scale_linetype_manual(values = c(&#39;solid&#39;, &quot;dotted&quot;, &quot;dashed&quot;)) + theme(legend.position = c(0.8, 0.85), panel.grid = element_blank()) + guides(linetype = &#39;none&#39;) Obrázek 14.2: Porovnání pozorované a odhadnuté křivky. Code # ggsave(&quot;figures/kap5_SVR_phoneme.tex&quot;, device = tikz, width = 4.5, height = 4.5) 14.6 Materiály pro Kapitolu 6 Veškeré grafické podklady i číselné výstupy prezentované v Diplomové práci v Kapitole 6 jsou k dispozici v Kapitole 5 (případně také v Kapitolách 6, 7 a 8) a v Kapitole 9. Krabicové diagramy testovacích chybovostí jsme vizuálně upravili pro potřeby diplomové práce (barevnost, změna měřítka, popisky), kód použitý k jejich vygenerování je k vidění v příslušných buňkách (zapoznámkovaný). 14.7 Materiály pro Kapitolu 7 Veškeré grafické podklady i číselné výstupy prezentované v Diplomové práci v Kapitole 7 jsou k dispozici v Kapitole 11, která je věnována datovému souboru phoneme, a Kapitole 12 věnované datovému souboru tecator. Krabicové diagramy testovacích chybovostí jsme vizuálně upravili pro potřeby diplomové práce (barevnost, změna měřítka, popisky), kód použitý k jejich vygenerování je k vidění v příslušných buňkách (zapoznámkovaný). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
