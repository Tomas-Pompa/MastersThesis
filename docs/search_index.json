[["index.html", "Diplomová práce SVM pro funkcionální data", " Diplomová práce Tomáš Pompa 25. 04. 2024 SVM pro funkcionální data Cílem bude aplikovat poznatky o metodě podpůrných vektorů (SVM) pro mnohorozměrná data na data funkcionálního typu, tedy nekonečně-rozměrné objekty. K tomu využijeme jednak převod (redukci) objektů z nekonečné dimenze na objekty konečné dimenze a následným využitím známých postupů a také modifikaci SVM přímo pro funkcionální data, k čemuž využijeme poznatky o Hilbertových prostorech a skalárním součinu. Dalším cílem bude porovnání jednotlivých metod pro klasifikaci funkcionálních dat na reálných a simulovaných datech. Bylo by dobré vymyslet nějakou zajímavou simulační studii, která bude demonstrovat různá chování uvažovaných metod. Mezi uvažované klasifikační metody patří: \\(K\\) nejbližších sousedů (KNN), logistická regrese (jak obyčejná (LR) tak její funkcionální modifikace (LR_fda)), lineární (LDA) a kvadratická (QDA) diskriminační analýza, rozhodovací stromy (DT), náhodné lesy (RF) a Support Vector Machines. Postupně jednotlivé metody projdeme, nejprve na simulovaných datech, a následně budeme konstruovat metodu podpůrných vektorů pro funkcionální data (SVM_fda). Základním balíčkem v R pro práci s funkcionálními objekty je fda. Dalšími užitečnými balíčky budou MASS, e1071, fda.usc, refund a další. "],["simulace1.html", "Kapitola 1 Simulace 1 1.1 Simulace funkcionálních dat 1.2 Vyhlazení pozorovaných křivek 1.3 Klasifikace křivek 1.4 Tabulka výsledků 1.5 Simulační studie", " Kapitola 1 Simulace 1 1.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro simulaci nejprve: zvolíme vhodné funkce, generujeme body ze zvoleného intervalu, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 1]\\). Pro třídu \\(Y = 0\\) a \\(Y = 1\\) uvažujme funkce \\[ f_0(x) = 2 \\cdot \\sin\\left(\\frac{\\pi x}{2}\\right) + x^2 + \\frac{1}{2}, \\] \\[ f_1(x) = 3 \\cdot \\sin\\left(\\frac{\\pi x}{2}\\right) + \\frac{1}{2} x. \\] Jejich grafy jsou na obrázcích níže. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- function(x) {return(2 * sin(x*pi/2) + x^2 + 0.5)} # pridat nahodny posun v zacatku nebo periode funkce_1 &lt;- function(x) {return(3 * sin(x*pi/2) + 0.5 * x)} Code x &lt;- seq(0, 1, length = 501) y0 &lt;- funkce_0(x) y1 &lt;- funkce_1(x) df &lt;- data.frame(x = rep(x, 2), y = c(y0, y1), Y = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), each = length(x))) df |&gt; ggplot(aes(x = x, y = y, colour = Y)) + geom_line(linewidth = 1) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 1.1: Znázornění dvou funkcí na intervalu \\([0, 1]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet vygenerovaných funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce. Tento posun budeme generovat s exponenciálního rozdělění s parametrem \\(\\lambda_{\\text{exp}}\\). Code generate_values &lt;- function(t, fun, n, sigma, lambda_exp = Inf) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # lambda_exp ... parameter of exponential distribution # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rexp(n, rate = lambda_exp), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 1] t &lt;- seq(0, 1, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 0.6) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 0.6) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 1.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 1.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nejedná o periodické křivky na intervalu \\(I = [0, 1]\\), využijeme k vyhlazení vhodnější B-sline bázi. Za uzly bereme celý vektor t, standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross-validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;dashed&#39;, linewidth = 0.8) + geom_point(size = 2.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 1.3: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda =\\) 0.1 nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 1.4: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;black&#39;, linewidth = 1, linetype = &#39;twodash&#39;) Obrázek 1.5: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. 1.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 66 74 Code table(Y.test) ## Y.test ## 0 1 ## 34 26 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.4714286 0.5285714 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.5666667 0.4333333 1.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # summary(neighb.model) # shrnuti modelu # plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K # neighb.model$max.prob # maximalni presnost (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 9 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) # CV.results |&gt; t() Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 3 s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.2665. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 1.6: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # summary(neighb.model) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost # 1 - presnost Vidíme tedy, že chybovost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 3, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.2786 a na testovacích datech 0.2833. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 1.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Zároveň kvůli grafickému zakreslení přidáme podmínku, že za počet komponent \\(p\\) volíme alespoň 2. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 99.76 % variability v datech. První hlavní komponenta potom vysvětluje 98.94 % a druhá 0.82 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 1.7: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (21.43 %), tak i na testovacích datech (21.67 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.8: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak chybovost klasifikátoru na trénovacích (22.14 %), tak i na testovacích datech (21.67 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.9: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 1.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. Ten bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřednostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 1.10: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 6, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chybovost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 7 s validační chybovostí 0.2075. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) Obrázek 1.11: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 7 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 18.57 %) i testovací chybovost (rovna 20 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Klasifikační\\n třída&#39;) Obrázek 1.12: Závislost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 1, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Obrázek 1.13: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 1]\\). Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro časy \\(t\\) z prostředka intervalu \\([0, 1]\\), zatímco pro krajní časy jsou hodnoty vyšší. To ukazuje na rozdílnost funkcí z klasifikačních tříd na začátku a konci intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy chybovost klasifikátoru na trénovacích (20.71 %) i na testovacích datech (23.33 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.14: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 1.3.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [0, 1]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 1, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost klasifikátoru na testovacích datech je tedy 33.33 % a na trénovacích datech 27.14 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.15: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.16: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 2 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na testovacích datech je tedy 25 % a na trénovacích datech 21.43 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.17: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.18: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na trénovacích datech je tedy 27.86 % a na testovacích datech 26.67 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.19: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.20: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 1.3.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 1]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 2.14 % a na testovacích datech 26.67 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních p = 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost lesu na trénovacích datech je tedy 7.86 % a na testovacích datech 26.67 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost tohoto klasifikátoru na trénovacích datech je 2.14 % a na testovacích datech 26.67 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního a radiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 1.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 1.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 1.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 1.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 1.3.7.5. 1.3.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 1]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(BSmooth$fd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 30.71 % pro lineární jádro, 27.86 % pro polynomiální jádro a 32.86 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 38.33 % pro lineární jádro, 40 % pro polynomiální jádro a 40 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 22.86 % pro lineární jádro, 21.43 % pro polynomiální jádro a 22.14 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 23.33 % pro lineární jádro, 26.67 % pro polynomiální jádro a 25 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 1.21: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 19.29 % pro lineární jádro, 19.29 % pro polynomiální jádro a 25 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 15 % pro lineární jádro, 25 % pro polynomiální jádro a 25 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 7 0.1853480 ## poly 7 0.2058242 ## radial 7 0.2773993 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 7 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1853, 7 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.2058 a 7 pro radiální jádro s hodnotou chybovosti 0.2774. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &quot;Jádro&quot;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 1.22: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 17.86 % pro lineární jádro, 12.14 % pro polynomiální jádro a 18.57 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 25 % pro lineární jádro, 25 % pro polynomiální jádro a 40 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 1.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše uvedená v Lemmatu 1.1 se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 1.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 1.3.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 1.3.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku1 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 1.3.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). 1.3.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;)^2)) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.2071 0.5333 SVM poly - RKHS 0.1571 0.5000 SVM rbf - RKHS 0.2000 0.4167 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 1, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 1.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 13 0.5179 0.3066 linear poly 19 0.1179 0.2888 polynomial radial 10 0.0439 0.2874 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 13 a \\(\\gamma={}\\) 0.5179 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.3066, \\(d={}\\) 19 a \\(\\gamma={}\\) 0.1179 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.2888 a \\(d={}\\) 10 a \\(\\gamma={}\\) 0.0439 pro radiální jádro s hodnotou chybovosti 0.2874. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) ## Warning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Spectral is 11 ## Returning the palette you asked for with that many colors Obrázek 1.23: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.2714 0.4000 SVM poly - RKHS - radial 0.1786 0.4167 SVM rbf - RKHS - radial 0.2643 0.4000 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 27.14 % pro lineární jádro, 17.86 % pro polynomiální jádro a 26.43 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 40 % pro lineární jádro, 41.67 % pro polynomiální jádro a 40 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) d p CV Kernel linear 5 5 0.3133 linear poly 8 4 0.3200 polynomial radial 7 5 0.3224 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 5 a \\(p={}\\) 5 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.3133, \\(d={}\\) 8 a \\(p={}\\) 4 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.32 a \\(d={}\\) 7 a \\(p={}\\) 5 pro radiální jádro s hodnotou chybovosti 0.3224. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.4: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.3214 0.3667 SVM poly - RKHS - poly 0.2286 0.4833 SVM rbf - RKHS - poly 0.2857 0.4333 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 32.14 % pro lineární jádro, 22.86 % pro polynomiální jádro a 28.57 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 36.67 % pro lineární jádro, 48.33 % pro polynomiální jádro a 43.33 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 1.5: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 22 0.3048 linear poly 5 0.3000 polynomial radial 8 0.3071 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 22 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.3048, \\(d={}\\) 5 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.3 a \\(d={}\\) 8 pro radiální jádro s hodnotou chybovosti 0.3071. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.6: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.2786 0.3667 SVM poly - RKHS - linear 0.2571 0.3833 SVM rbf - RKHS - linear 0.2929 0.3333 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 27.86 % pro lineární jádro, 25.71 % pro polynomiální jádro a 29.29 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 36.67 % pro lineární jádro, 38.33 % pro polynomiální jádro a 33.33 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 1.4 Tabulka výsledků Tabulka 1.7: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.2786 0.2833 LDA 0.2143 0.2167 QDA 0.2214 0.2167 LR functional 0.1857 0.2000 LR score 0.2071 0.2333 Tree - diskr. 0.2714 0.3333 Tree - score 0.2143 0.2500 Tree - Bbasis 0.2786 0.2667 RForest - diskr 0.0214 0.2667 RForest - score 0.0786 0.2667 RForest - Bbasis 0.0214 0.2667 SVM linear - diskr 0.3071 0.3833 SVM poly - diskr 0.2786 0.4000 SVM rbf - diskr 0.3286 0.4000 SVM linear - PCA 0.2286 0.2333 SVM poly - PCA 0.2143 0.2667 SVM rbf - PCA 0.2214 0.2500 SVM linear - Bbasis 0.1929 0.1500 SVM poly - Bbasis 0.1929 0.2500 SVM rbf - Bbasis 0.2500 0.2500 SVM linear - projection 0.1786 0.2500 SVM poly - projection 0.1214 0.2500 SVM rbf - projection 0.1857 0.4000 SVM linear - RKHS - radial 0.2714 0.4000 SVM poly - RKHS - radial 0.1786 0.4167 SVM rbf - RKHS - radial 0.2643 0.4000 SVM linear - RKHS - poly 0.3214 0.3667 SVM poly - RKHS - poly 0.2286 0.4833 SVM rbf - RKHS - poly 0.2857 0.4333 SVM linear - RKHS - linear 0.2786 0.3667 SVM poly - RKHS - linear 0.2571 0.3833 SVM rbf - RKHS - linear 0.2929 0.3333 1.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). Code # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 1] t &lt;- seq(0, 1, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 0.6) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 0.6) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 3, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 1, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # normovani dat norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(BSmooth$fd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by =2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 1, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_01.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/simulace_01_res.RData&#39;) 1.5.1 Výsledky Tabulka 1.8: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.3217 0.3640 0.0512 0.0625 LDA 0.4623 0.5293 0.0304 0.0638 QDA 0.4659 0.5213 0.0250 0.0607 LR_functional 0.1974 0.2622 0.0450 0.0650 LR_score 0.4641 0.5218 0.0325 0.0667 Tree_discr 0.2868 0.4157 0.0645 0.0759 Tree_score 0.4083 0.5245 0.0641 0.0576 Tree_Bbasis 0.2900 0.4178 0.0674 0.0796 RF_discr 0.0326 0.3362 0.0137 0.0607 RF_score 0.1366 0.5112 0.0202 0.0643 RF_Bbasis 0.0330 0.3352 0.0139 0.0592 SVM linear - diskr 0.3089 0.3532 0.0410 0.0767 SVM poly - diskr 0.2916 0.3565 0.0426 0.0802 SVM rbf - diskr 0.2965 0.3567 0.0408 0.0773 SVM linear - PCA 0.4691 0.5405 0.0225 0.0475 SVM poly - PCA 0.4431 0.5227 0.0328 0.0629 SVM rbf - PCA 0.4326 0.5138 0.0329 0.0725 SVM linear - Bbasis 0.2216 0.2425 0.0392 0.0547 SVM poly - Bbasis 0.2286 0.2815 0.0370 0.0591 SVM rbf - Bbasis 0.2741 0.3483 0.0436 0.0767 SVM linear - projection 0.1869 0.2695 0.0427 0.0638 SVM poly - projection 0.1459 0.2952 0.0490 0.0727 SVM rbf - projection 0.1836 0.3317 0.0317 0.0870 SVM linear - RKHS - radial 0.2649 0.3647 0.0472 0.0687 SVM poly - RKHS - radial 0.2342 0.3638 0.0535 0.0740 SVM rbf - RKHS - radial 0.2499 0.3578 0.0465 0.0690 SVM linear - RKHS - poly 0.2779 0.3777 0.0466 0.0788 SVM poly - RKHS - poly 0.2360 0.3803 0.0713 0.0632 SVM rbf - RKHS - poly 0.2516 0.3753 0.0463 0.0713 SVM linear - RKHS - linear 0.3044 0.3643 0.0524 0.0760 SVM poly - RKHS - linear 0.2679 0.3737 0.0605 0.0770 SVM rbf - RKHS - linear 0.2936 0.3622 0.0479 0.0683 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 1.24: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[test])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 1.25: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 1.9: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 6.0 nharm 1.0 LR_func_n_basis 9.0 SVM_d_Linear 9.5 SVM_d_Poly 6.0 SVM_d_Radial 6.0 SVM_RKHS_radial_gamma1 0.2 SVM_RKHS_radial_gamma2 0.2 SVM_RKHS_radial_gamma3 0.1 SVM_RKHS_radial_d1 19.0 SVM_RKHS_radial_d2 13.0 SVM_RKHS_radial_d3 17.0 SVM_RKHS_poly_p1 3.0 SVM_RKHS_poly_p2 3.0 SVM_RKHS_poly_p3 4.0 SVM_RKHS_poly_d1 11.0 SVM_RKHS_poly_d2 7.0 SVM_RKHS_poly_d3 9.0 SVM_RKHS_linear_d1 9.0 SVM_RKHS_linear_d2 5.0 SVM_RKHS_linear_d3 8.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.26: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.27: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.28: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.29: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.30: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["simulace2.html", "Kapitola 2 Simulace 2 2.1 Simulace funkcionálních dat 2.2 Vyhlazení pozorovaných křivek 2.3 Klasifikace křivek 2.4 Tabulka výsledků 2.5 Simulační studie", " Kapitola 2 Simulace 2 2.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro tuto simulaci zvolíme následující postup: zvolíme vhodné body (pro každou klasifikační třídu jiné), které proložíme interpolačním splajnem, takto získané funkce využijeme ke generování náhodných křivek pro obě třídy, generujeme body ze zvoleného intervalu pomocí vyhlazených funkcí interpolačním splajnem, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(stats) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 12]\\). V tomto případě si definujeme v prvním kroku body, kterými má náš interpolační splajn procházet. Následně proložíme těmito body interpolační splajn, k čemuž využijeme funkci spline() z knihovny stats. Pro lepší interpretovatelnost simulovaných dat jsme definovali interval \\(I = [0, 12]\\) a funkce pro obě klasifikační třídy zvolíme tak, aby při troše fantazie mohli představovat například vývoj určité veličiny (teplota, tlak, srážky, nezaměstnanost, prodej nějaké komodity atd.) v průběhu roku. Budeme předpokládat, že se vývoj této veličiny periodicky opakuje v čase (s roční periodou), proto ve funkci spline() zvolíme parametr method = 'periodic'. Code # definujici body pro tridu 0 x.0 &lt;- c(0, 2, 2.96, 3.64, 4.2, 5.3, 6.22, 7.32, 8.3, 9.3, 10.6, 11.54, 12) y.0 &lt;- c(3, 4.66, 4.6, 3.48, 2.08, 1.5, 1.32, 2.78, 3.82, 4.48, 3.88, 3.16, 3) # definujici body pro tridu 1 x.1 &lt;- c(0, 1.3, 2, 2.7, 3.33, 3.68, 4.06, 4.72, 5.9, 6.9, 8, 9.02, 10.1, 11.25, 12) y.1 &lt;- c(2.8, 3.95, 4.82, 5.1, 4.8, 4, 2.73, 1.67, 1.53, 2.2, 3, 3.73, 3.69, 3.6, 2.8) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Jejich grafy jsou na obrázcích níže. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- function(n) { sp &lt;- spline(x.0, y.0, method = &#39;periodic&#39;, n = n) return(list(x = sp$x, y = sp$y)) } # pridat nahodny posun v zacatku nebo periode funkce_1 &lt;- function(n) { sp &lt;- spline(x.1, y.1, method = &#39;periodic&#39;, n = n) return(list(x = sp$x, y = sp$y)) } Code n_x &lt;- 501 x &lt;- funkce_0(n_x)$x y0 &lt;- funkce_0(n_x)$y y1 &lt;- funkce_1(n_x)$y df &lt;- data.frame(x = rep(x, 2), y = c(y0, y1), Y = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), each = length(x))) df |&gt; ggplot(aes(x = x, y = y, colour = Y)) + geom_line(linewidth = 1) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 1.2: Znázornění dvou funkcí na intervalu \\([0, 12]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce. Tento posun budeme generovat s normálního rozdělění s parametrem \\(\\sigma^2 = 3.75^2\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(fun(length(t))$y, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(X + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 12] t &lt;- seq(0, 12, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 3.75) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 3.75) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 1.3: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 2.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nyní jedná o periodické křivky na intervalu \\(I = [0, 12]\\), využijeme k vyhlazení fourierovu bázi. Za uzly bereme celý vektor t, budeme uvažovat tzv. harmonic acceleration penalties. Harmonic acceleration pro funkci \\(x(t)\\) je \\[ Lx = \\omega^2 Dx + D^3x, \\] kde \\(D^mx\\) značí \\(m\\)-tou derivaci funkce \\(x(t)\\) podle \\(t\\). Platí přitom \\(L\\sin(\\omega x) = 0 = L\\cos(\\omega x)\\). Potom jako penalizaci bereme hodnotu funkcionálu \\[ J_L(x) = \\int \\left[Lx(t)\\right]^2\\text dt. \\] Code rangeval &lt;- range(t) fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = length(t)) omega &lt;- 2 * pi / diff(rangeval) acvec &lt;- c(0, omega^2, 0) harmLfd &lt;- vec2Lfd(bwtvec = acvec, rangeval = rangeval) Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(fbasis, harmLfd, lambda.vect[index]) FSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(FSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;dashed&#39;, linewidth = 0.8) + geom_point(size = 2.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 2.1: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(fbasis, harmLfd, lambda.opt) FSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- FSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.2: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;black&#39;, linewidth = 1, linetype = &#39;twodash&#39;) Obrázek 2.3: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. 2.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 2.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # summary(neighb.model) # shrnuti modelu # plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K # neighb.model$max.prob # maximalni presnost (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 1 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) # CV.results Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 3 s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.2579. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 2.4: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # summary(neighb.model) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost # 1 - presnost Vidíme tedy, že chybovost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 3, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.3071 a na testovacích datech 0.3333. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 2.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 99.52 % variability v datech. První hlavní komponenta potom vysvětluje 99.16 % a druhá 0.36 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 1.8: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (11.43 %), tak i na testovacích datech (15 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 2.5: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak chybovost klasifikátoru na trénovacích (12.86 %), tak i na testovacích datech (15 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 2.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. V tomto případě však funkce fregre.glm() hlásí chybu, že systém je výpočetně singulární, proto nyní uvažujme pouze druhou z možných implementací logistické regrese pro funkcionální data. 2.3.4.1 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy chybovost klasifikátoru na trénovacích (10.71 %) i na testovacích datech (16.67 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.12: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 2.3.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [0, 12]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 12, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost klasifikátoru na testovacích datech je tedy 53.33 % a na trénovacích datech 32.86 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 2.7: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 2.8: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 2 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na testovacích datech je tedy 16.67 % a na trénovacích datech 10 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.14: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 2.9: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí ve Fourierově bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na trénovacích datech je tedy 12.86 % a na testovacích datech 15 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.16: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 2.10: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Fbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 2.3.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 12]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 1.43 % a na testovacích datech 28.33 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na trénovacích datech je tedy 3.57 % a na testovacích datech 16.67 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí Fourierovské báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost tohoto klasifikátoru na trénovacích datech je 0 % a na testovacích datech 15 %. Code Res &lt;- data.frame(model = &#39;RForest - Fbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního a radiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 2.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 2.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí Fourierovy báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 2.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými nebo Fourierovými, funkcemi (sekce 2.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 2.3.7.5. 2.3.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 12]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 10.71 % pro lineární jádro, 12.86 % pro polynomiální jádro a 19.29 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 10 % pro lineární jádro, 18.33 % pro polynomiální jádro a 30 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 2.3.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 10 % pro lineární jádro, 12.14 % pro polynomiální jádro a 12.14 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 18.33 % pro lineární jádro, 18.33 % pro polynomiální jádro a 18.33 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 2.11: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 2.3.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí Fourierovy báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 13.33 % pro lineární jádro, 21.67 % pro polynomiální jádro a 26.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 2.3.7.4 Projekce na Fourierovu bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na Fourierovu bázi, protože využití B-splinové báze není pro naše periodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Code k_cv &lt;- 10 # k-fold CV # hodnoty pro Fourierovu bazi rangeval &lt;- range(t) n_basis_min &lt;- 3 n_basis_max &lt;- 40 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na Fourierovu bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = fbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 34 0.09031136 ## poly 40 0.11414835 ## radial 18 0.12593407 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 34 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0903, 40 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1141 a 18 pro radiální jádro s hodnotou chybovosti 0.1259. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 2.12: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na Fourierovu bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0 % pro lineární jádro, 0 % pro polynomiální jádro a 2.86 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 18.33 % pro lineární jádro, 15 % pro polynomiální jádro a 15 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 2.3.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 1.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše uvedená v Lemmatu 1.1 se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 2.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 2.3.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 2.3.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku2 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 2.3.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). 2.3.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 2.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0571 0.25 SVM poly - RKHS 0.0357 0.30 SVM rbf - RKHS 0.0857 0.30 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 1, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 2.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 29 0.5179 0.1219 linear poly 40 0.0439 0.1423 polynomial radial 8 0.8483 0.1276 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 29 a \\(\\gamma={}\\) 0.5179 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1219, \\(d={}\\) 40 a \\(\\gamma={}\\) 0.0439 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1423 a \\(d={}\\) 8 a \\(\\gamma={}\\) 0.8483 pro radiální jádro s hodnotou chybovosti 0.1276. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 2.13: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 2.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0571 0.1667 SVM poly - RKHS - radial 0.0357 0.2500 SVM rbf - RKHS - radial 0.1000 0.1167 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 5.71 % pro lineární jádro, 3.57 % pro polynomiální jádro a 10 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 16.67 % pro lineární jádro, 25 % pro polynomiální jádro a 11.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 2.3.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 2.4: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 10 4 0.1179 linear poly 11 4 0.1230 polynomial radial 18 4 0.1245 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 10 a \\(p={}\\) 4 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1179, \\(d={}\\) 11 a \\(p={}\\) 4 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.123 a \\(d={}\\) 18 a \\(p={}\\) 4 pro radiální jádro s hodnotou chybovosti 0.1245. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, coef0 = 1, epsilon = 0.1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.1071 0.2167 SVM poly - RKHS - poly 0.0500 0.2833 SVM rbf - RKHS - poly 0.0857 0.2167 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 10.71 % pro lineární jádro, 5 % pro polynomiální jádro a 8.57 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 21.67 % pro lineární jádro, 28.33 % pro polynomiální jádro a 21.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 2.3.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, coef0 = 1, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 2.5: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 9 0.1505 linear poly 9 0.1863 polynomial radial 31 0.1643 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 9 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1505, \\(d={}\\) 9 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1863 a \\(d={}\\) 31 pro radiální jádro s hodnotou chybovosti 0.1643. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.4: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.1286 0.2333 SVM poly - RKHS - linear 0.0857 0.2500 SVM rbf - RKHS - linear 0.1071 0.2333 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 12.86 % pro lineární jádro, 8.57 % pro polynomiální jádro a 10.71 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 23.33 % pro lineární jádro, 25 % pro polynomiální jádro a 23.33 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 2.4 Tabulka výsledků Tabulka 2.6: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3071 0.3333 LDA 0.1143 0.1500 QDA 0.1286 0.1500 LR score 0.1071 0.1667 Tree - diskr. 0.3286 0.5333 Tree - score 0.1000 0.1667 Tree - Fbasis 0.1286 0.1500 RForest - diskr 0.0143 0.2833 RForest - score 0.0357 0.1667 RForest - Fbasis 0.0000 0.1500 SVM linear - diskr 0.1071 0.1000 SVM poly - diskr 0.1286 0.1833 SVM rbf - diskr 0.1929 0.3000 SVM linear - PCA 0.1000 0.1833 SVM poly - PCA 0.1214 0.1833 SVM rbf - PCA 0.1214 0.1833 SVM linear - Fbasis 0.0000 0.1333 SVM poly - Fbasis 0.0000 0.2167 SVM rbf - Fbasis 0.0000 0.2667 SVM linear - projection 0.0000 0.1833 SVM poly - projection 0.0000 0.1500 SVM rbf - projection 0.0286 0.1500 SVM linear - RKHS - radial 0.0571 0.1667 SVM poly - RKHS - radial 0.0357 0.2500 SVM rbf - RKHS - radial 0.1000 0.1167 SVM linear - RKHS - poly 0.1071 0.2167 SVM poly - RKHS - poly 0.0500 0.2833 SVM rbf - RKHS - poly 0.0857 0.2167 SVM linear - RKHS - linear 0.1286 0.2333 SVM poly - RKHS - linear 0.0857 0.2500 SVM rbf - RKHS - linear 0.1071 0.2333 2.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Fbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Fbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část n.sim-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na Fourierovu bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 12] t &lt;- seq(0, 12, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 3.75) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 3.75) rangeval &lt;- range(t) fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = length(t)) omega &lt;- 2 * pi / diff(rangeval) acvec &lt;- c(0, omega^2, 0) harmLfd &lt;- vec2Lfd(bwtvec = acvec, rangeval = rangeval) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(fbasis, harmLfd, lambda.vect[index]) FSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(FSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(fbasis, harmLfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 12, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Fbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Fbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # normovani dat norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na Fourierovu bázi # hodnoty pro Fourierovu bazi rangeval &lt;- range(t) n_basis_min &lt;- 3 n_basis_max &lt;- 40 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = fbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = fbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by =2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, coef0 = 1, epsilon = 0.1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_02.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/simulace_02_res.RData&#39;) 2.5.1 Výsledky Tabulka 2.7: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.2639 0.3015 0.0478 0.0642 LDA 0.1161 0.1222 0.0288 0.0443 QDA 0.1164 0.1218 0.0290 0.0454 LR_score 0.1154 0.1252 0.0279 0.0475 Tree_discr 0.2890 0.4555 0.0757 0.0703 Tree_score 0.1004 0.1315 0.0246 0.0442 Tree_Fbasis 0.1129 0.1557 0.0282 0.0492 RF_discr 0.0166 0.2907 0.0103 0.0692 RF_score 0.0412 0.1347 0.0153 0.0418 RF_Fbasis 0.0000 0.1145 0.0000 0.0427 SVM linear - diskr 0.0892 0.1190 0.0299 0.0427 SVM poly - diskr 0.1089 0.1570 0.0343 0.0610 SVM rbf - diskr 0.1944 0.2570 0.0542 0.0916 SVM linear - PCA 0.1157 0.1257 0.0283 0.0477 SVM poly - PCA 0.1093 0.1297 0.0277 0.0486 SVM rbf - PCA 0.1095 0.1307 0.0285 0.0462 SVM linear - Fbasis 0.0017 0.1750 0.0053 0.0592 SVM poly - Fbasis 0.0000 0.1550 0.0000 0.0503 SVM rbf - Fbasis 0.0030 0.1465 0.0048 0.0487 SVM linear - projection 0.0581 0.1177 0.0314 0.0513 SVM poly - projection 0.0143 0.1475 0.0307 0.0495 SVM rbf - projection 0.0291 0.1237 0.0240 0.0487 SVM linear - RKHS - radial 0.0526 0.1217 0.0277 0.0458 SVM poly - RKHS - radial 0.0231 0.1483 0.0270 0.0550 SVM rbf - RKHS - radial 0.0435 0.1342 0.0217 0.0451 SVM linear - RKHS - poly 0.0834 0.1467 0.0314 0.0461 SVM poly - RKHS - poly 0.0819 0.1532 0.0368 0.0503 SVM rbf - RKHS - poly 0.0908 0.1467 0.0299 0.0518 SVM linear - RKHS - linear 0.1146 0.1985 0.0362 0.0593 SVM poly - RKHS - linear 0.0721 0.1975 0.0306 0.0565 SVM rbf - RKHS - linear 0.1068 0.1892 0.0290 0.0550 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 2.14: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[test])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 2.15: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 2.8: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 3.0 nharm 1.0 LR_func_n_basis NA SVM_d_Linear 12.0 SVM_d_Poly 14.0 SVM_d_Radial 13.0 SVM_RKHS_radial_gamma1 5.2 SVM_RKHS_radial_gamma2 8.5 SVM_RKHS_radial_gamma3 8.5 SVM_RKHS_radial_d1 17.0 SVM_RKHS_radial_d2 17.0 SVM_RKHS_radial_d3 17.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 4.0 SVM_RKHS_poly_p3 4.0 SVM_RKHS_poly_d1 13.0 SVM_RKHS_poly_d2 7.0 SVM_RKHS_poly_d3 9.0 SVM_RKHS_linear_d1 17.0 SVM_RKHS_linear_d2 23.0 SVM_RKHS_linear_d3 24.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 2.16: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 2.17: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 2.18: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 2.19: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.26: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["simulace2sigma.html", "Kapitola 3 Závislost na parametru \\(\\sigma^2\\) 3.1 Simulace funkcionálních dat 3.2 Vyhlazení pozorovaných křivek 3.3 Klasifikace křivek 3.4 Tabulka výsledků 3.5 Simulační studie", " Kapitola 3 Závislost na parametru \\(\\sigma^2\\) V této části se budeme zabývat závislostí výsledků z předchozí sekce 2 na hodnotě \\(\\sigma^2\\), která definuje rozptyl normálního rozdělení, ze kterého generujeme náhodné chyby kolem generujících křivek (můžeme říci, že \\(\\sigma^2\\) nese informaci například o chybovosti měření určitého přístroje). Očekáváme, že s rostoucí hodnotou \\(\\sigma^2\\) se budou výsledky jednotlivých metod zhoršovat a tudíž klasifikace nebude tak úspěšná. V následující sekci 4 se pak podíváme na závislost výsledků na hodnotě \\(\\sigma^2_{shift}\\), tedy na rozptylu normálního rozdělení, ze kterého generujeme posun pro generované křivky. 3.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro tuto simulaci zvolíme následující postup: zvolíme vhodné body (pro každou klasifikační třídu jiné), které proložíme interpolačním splajnem, takto získané funkce využijeme ke generování náhodných křivek pro obě třídy, generujeme body ze zvoleného intervalu pomocí vyhlazených funkcí interpolačním splajnem, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(stats) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 12]\\). V tomto případě si definujeme v prvním kroku body, kterými má náš interpolační splajn procházet. Následně proložíme těmito body interpolační splajn, k čemuž využijeme funkci spline() z knihovny stats. Pro lepší interpretovatelnost simulovaných dat jsme definovali interval \\(I = [0, 12]\\) a funkce pro obě klasifikační třídy zvolíme tak, aby při troše fantazie mohli představovat například vývoj určité veličiny (teplota, tlak, srážky, nezaměstnanost, prodej nějaké komodity atd.) v průběhu roku. Budeme předpokládat, že se vývoj této veličiny periodicky opakuje v čase (s roční periodou), proto ve funkci spline() zvolíme parametr method = 'periodic'. Code # definujici body pro tridu 0 x.0 &lt;- c(0, 2, 2.96, 3.64, 4.2, 5.3, 6.22, 7.32, 8.3, 9.3, 10.6, 11.54, 12) y.0 &lt;- c(3, 4.66, 4.6, 3.48, 2.08, 1.5, 1.32, 2.78, 3.82, 4.48, 3.88, 3.16, 3) # definujici body pro tridu 1 x.1 &lt;- c(0, 1.3, 2, 2.7, 3.33, 3.68, 4.06, 4.72, 5.9, 6.9, 8, 9.02, 10.1, 11.25, 12) y.1 &lt;- c(2.8, 3.95, 4.82, 5.1, 4.8, 4, 2.73, 1.67, 1.53, 2.2, 3, 3.73, 3.69, 3.6, 2.8) # # definujici body pro tridu 0 # x.0 &lt;- c(0, 2, 2.96, 3.64, 4.2, 5.3, 6.22, 7.32, 8.3, 9.3, 10.6, 11.54, 12) # y.0 &lt;- c(3, 4.66, 4.6, 3.48, 2.08, 1.5, 1.32, 2.78, 3.82, 4.48, 3.88, 3.16, 3) # # # definujici body pro tridu 1 # x.1 &lt;- c(0, 1.3, 2, 2.7, 3.33, 3.68, 4.06, 4.72, 5.9, 6.9, 8, 9.02, 10.1, 11.25, 12) # y.1 &lt;- c(2.9, 3.95, 4.62, 4.6, 4.1, 3.6, 2.73, 1.67, 1.53, 2.2, 3, 3.73, 3.69, 3.6, 2.9) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Jejich grafy jsou na obrázcích níže. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- function(n) { sp &lt;- spline(x.0, y.0, method = &#39;periodic&#39;, n = n) return(list(x = sp$x, y = sp$y)) } # pridat nahodny posun v zacatku nebo periode funkce_1 &lt;- function(n) { sp &lt;- spline(x.1, y.1, method = &#39;periodic&#39;, n = n) return(list(x = sp$x, y = sp$y)) } Code n_x &lt;- 501 x &lt;- funkce_0(n_x)$x y0 &lt;- funkce_0(n_x)$y y1 &lt;- funkce_1(n_x)$y df &lt;- data.frame(x = rep(x, 2), y = c(y0, y1), Y = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), each = length(x))) df |&gt; ggplot(aes(x = x, y = y, colour = Y)) + geom_line(linewidth = 1) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 3.1: Znázornění dvou funkcí na intervalu \\([0, 1]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce. Tento posun budeme generovat s normálního rozdělění s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(fun(length(t))$y, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(X + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 12] t &lt;- seq(0, 12, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 3.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 3.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nyní jedná o periodické křivky na intervalu \\(I = [0, 12]\\), využijeme k vyhlazení fourierovu bázi. Za uzly bereme celý vektor t, budeme uvažovat tzv. harmonic acceleration penalties. Harmonic acceleration pro funkci \\(x(t)\\) je \\[ Lx = \\omega^2 Dx + D^3x, \\] kde \\(D^mx\\) značí \\(m\\)-tou derivaci funkce \\(x(t)\\) podle \\(t\\). Platí přitom \\(L\\sin(\\omega x) = 0 = L\\cos(\\omega x)\\). Potom jako penalizaci bereme hodnotu funkcionálu \\[ J_L(x) = \\int \\left[Lx(t)\\right]^2\\text dt. \\] Code rangeval &lt;- range(t) fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = length(t)) omega &lt;- 2 * pi / diff(rangeval) acvec &lt;- c(0, omega^2, 0) harmLfd &lt;- vec2Lfd(bwtvec = acvec, rangeval = rangeval) Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(fbasis, harmLfd, lambda.vect[index]) FSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(FSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;dashed&#39;, linewidth = 0.8) + geom_point(size = 2.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 1.5: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(fbasis, harmLfd, lambda.opt) FSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- FSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = &#39;Time&#39;, y = &#39;Function&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.1: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25) + theme_bw() + labs(x = &#39;Time&#39;, y = &#39;Function&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;black&#39;, linewidth = 1, linetype = &#39;twodash&#39;) Obrázek 2.2: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. 3.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 3.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # summary(neighb.model) # shrnuti modelu # plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K # neighb.model$max.prob # maximalni presnost (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 5 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) # CV.results Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 5 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.756. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 3.3: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # summary(neighb.model) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost # 1 - presnost Vidíme tedy, že chybovost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 5, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.25 a na testovacích datech 0.2167. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 3.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 98.3 % variability v datech. První hlavní komponenta potom vysvětluje 97.06 % a druhá 1.24 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 3.4: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (11.43 %), tak i na testovacích datech (15 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.5: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 3.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak chybovost klasifikátoru na trénovacích (12.86 %), tak i na testovacích datech (15 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 3.3.4 Support Vector Machines Definujeme pro další metody data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 12, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 2.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 2.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí Fourierovy báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 2.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými nebo Fourierovými, funkcemi (sekce 2.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 2.3.7.5. 3.3.4.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 12]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 11.43 % pro lineární jádro, 7.86 % pro polynomiální jádro a 11.43 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 10 % pro lineární jádro, 13.33 % pro polynomiální jádro a 15 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 3.3.4.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 11.43 % pro lineární jádro, 12.14 % pro polynomiální jádro a 12.14 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 18.33 % pro lineární jádro, 18.33 % pro polynomiální jádro a 18.33 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 3.7: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 3.3.4.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí Fourierovy báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 13.33 % pro lineární jádro, 21.67 % pro polynomiální jádro a 26.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 3.3.4.4 Projekce na Fourierovu bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na Fourierovu bázi, protože využití B-splinové báze není pro naše periodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Code k_cv &lt;- 10 # k-fold CV # hodnoty pro Fourierovu bazi rangeval &lt;- range(t) n_basis_min &lt;- 3 n_basis_max &lt;- 40 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na Fourierovu bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = fbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 34 0.09638278 ## poly 4 0.12443223 ## radial 4 0.12418040 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 34 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0964, 4 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1244 a 4 pro radiální jádro s hodnotou chybovosti 0.1242. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 1.14: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na Fourierovu bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = fbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0 % pro lineární jádro, 5.71 % pro polynomiální jádro a 7.86 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 21.67 % pro lineární jádro, 18.33 % pro polynomiální jádro a 15 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 3.4 Tabulka výsledků Tabulka 3.1: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.2500 0.2167 LDA 0.1143 0.1500 QDA 0.1286 0.1500 SVM linear - diskr 0.1143 0.1000 SVM poly - diskr 0.0786 0.1333 SVM rbf - diskr 0.1143 0.1500 SVM linear - PCA 0.1143 0.1833 SVM poly - PCA 0.1214 0.1833 SVM rbf - PCA 0.1214 0.1833 SVM linear - Fbasis 0.0000 0.1333 SVM poly - Fbasis 0.0000 0.2167 SVM rbf - Fbasis 0.0000 0.2667 SVM linear - projection 0.0000 0.2167 SVM poly - projection 0.0571 0.1833 SVM rbf - projection 0.0786 0.1500 3.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 10\\). Nyní zopakujeme celou předchozí část n.sim-krát a hodnoty chybovostí si budeme ukládat to objektu SIMUL_params. Přitom budeme měnit hodnotu parametru \\(\\sigma\\) a podíváme se, jak se mění výsledky jednotlivých vybraných klasifikačních metod v závislosti na této hodnotě. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci pro kazdou hodnotu simulacniho parametru n.sim &lt;- 10 methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;) # vektor smerodatnych odchylek definujicich rozptyl kolem generujicich krivek sigma_vector &lt;- seq(0.1, 5, length = 20) # vysledny objekt, do nehoz ukladame vysledky simulaci SIMUL_params &lt;- array(data = NA, dim = c(length(methods), 4, length(sigma_vector)), dimnames = list( method = methods, metric = c(&#39;ERRtrain&#39;, &#39;Errtest&#39;, &#39;SDtrain&#39;, &#39;SDtest&#39;), sigma = paste0(sigma_vector))) for (n_sigma in 1:length(sigma_vector)) { ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 12] t &lt;- seq(0, 12, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, sigma_vector[n_sigma], 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, sigma_vector[n_sigma], 2) rangeval &lt;- range(t) fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = length(t)) omega &lt;- 2 * pi / diff(rangeval) acvec &lt;- c(0, omega^2, 0) harmLfd &lt;- vec2Lfd(bwtvec = acvec, rangeval = rangeval) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -5, to = 3, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(fbasis, harmLfd, lambda.vect[index]) FSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(FSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(fbasis, harmLfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 12, length = 101) # normovani dat norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na Fourierovu bázi # hodnoty pro Fourierovu bazi rangeval &lt;- range(t) n_basis_min &lt;- 3 n_basis_max &lt;- 40 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = fbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = fbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, paste0(n_sigma, &#39;: &#39;, sim)) } # Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) SIMUL_params[, , n_sigma] &lt;- as.matrix(SIMULACE.df) } # ulozime vysledne hodnoty save(SIMUL_params, file = &#39;RData/simulace_parametry_sigma_02a.RData&#39;) 3.5.1 Grafický výstup Podívejme se na závislost simulovaných výsledků na hodnotě parametru směrodatné odchylky vertikálního posunu. Code # pro testovaci data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(sigma_vector), names_to = &#39;sigma&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), sigma = as.numeric(sigma)) |&gt; filter(method %in% c(&#39;KNN&#39;, &#39;LDA&#39;,&#39;SVM linear - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Fbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM rbf - projection&#39;)) |&gt; ggplot(aes(x = sigma, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[x]), y = expression(widehat(Err)[test]), colour = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;bottom&#39;) Obrázek 3.8: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{x}\\), který definuje směrodatnou odchylku pro generování náhodných odchylek kolem generujících křivek. "],["simulace2shift.html", "Kapitola 4 Závislost na parametru \\(\\sigma_{shift}\\) 4.1 Simulace funkcionálních dat 4.2 Vyhlazení pozorovaných křivek 4.3 Klasifikace křivek 4.4 Tabulka výsledků 4.5 Simulační studie", " Kapitola 4 Závislost na parametru \\(\\sigma_{shift}\\) V této části se budeme zabývat závislostí výsledků ze sekce 2 na hodnotě \\(\\sigma^2_{shift}\\), která definuje rozptyl normálního rozdělení, ze kterého generujeme posun pro generované křivky. Očekáváme, že s rostoucí hodnotou \\(\\sigma^2_{shift}\\) se budou výsledky jednotlivých metod zhoršovat a tudíž klasifikace nebude tak úspěšná. Přitom předpokládáme, že metody, které využívají funkcionální podstatu dat, budou více úspěšné v porovnání s klasickými metodami při zvětšující se hodnotě \\(\\sigma^2_{shift}\\). V předchozí sekci 3 jsme se podívali na závislost výsledků na hodnotě \\(\\sigma^2\\), tedy na rozptylu normálního rozdělení, ze kterého generujeme náhodné chyby kolem generujících křivek. 4.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro tuto simulaci zvolíme následující postup: zvolíme vhodné body (pro každou klasifikační třídu jiné), které proložíme interpolačním splajnem, takto získané funkce využijeme ke generování náhodných křivek pro obě třídy, generujeme body ze zvoleného intervalu pomocí vyhlazených funkcí interpolačním splajnem, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(stats) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 12]\\). V tomto případě si definujeme v prvním kroku body, kterými má náš interpolační splajn procházet. Následně proložíme těmito body interpolační splajn, k čemuž využijeme funkci spline() z knihovny stats. Pro lepší interpretovatelnost simulovaných dat jsme definovali interval \\(I = [0, 12]\\) a funkce pro obě klasifikační třídy zvolíme tak, aby při troše fantazie mohli představovat například vývoj určité veličiny (teplota, tlak, srážky, nezaměstnanost, prodej nějaké komodity atd.) v průběhu roku. Budeme předpokládat, že se vývoj této veličiny periodicky opakuje v čase (s roční periodou), proto ve funkci spline() zvolíme parametr method = 'periodic'. Code # definujici body pro tridu 0 x.0 &lt;- c(0, 2, 2.96, 3.64, 4.2, 5.3, 6.22, 7.32, 8.3, 9.3, 10.6, 11.54, 12) y.0 &lt;- c(3, 4.66, 4.6, 3.48, 2.08, 1.5, 1.32, 2.78, 3.82, 4.48, 3.88, 3.16, 3) # definujici body pro tridu 1 x.1 &lt;- c(0, 1.3, 2, 2.7, 3.33, 3.68, 4.06, 4.72, 5.9, 6.9, 8, 9.02, 10.1, 11.25, 12) y.1 &lt;- c(2.8, 3.95, 4.82, 5.1, 4.8, 4, 2.73, 1.67, 1.53, 2.2, 3, 3.73, 3.69, 3.6, 2.8) # # definujici body pro tridu 0 # x.0 &lt;- c(0, 2, 2.96, 3.64, 4.2, 5.3, 6.22, 7.32, 8.3, 9.3, 10.6, 11.54, 12) # y.0 &lt;- c(3, 4.66, 4.6, 3.48, 2.08, 1.5, 1.32, 2.78, 3.82, 4.48, 3.88, 3.16, 3) # # # definujici body pro tridu 1 # x.1 &lt;- c(0, 1.3, 2, 2.7, 3.33, 3.68, 4.06, 4.72, 5.9, 6.9, 8, 9.02, 10.1, 11.25, 12) # y.1 &lt;- c(2.9, 3.95, 4.62, 4.6, 4.1, 3.6, 2.73, 1.67, 1.53, 2.2, 3, 3.73, 3.69, 3.6, 2.9) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Jejich grafy jsou na obrázcích níže. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- function(n) { sp &lt;- spline(x.0, y.0, method = &#39;periodic&#39;, n = n) return(list(x = sp$x, y = sp$y)) } # pridat nahodny posun v zacatku nebo periode funkce_1 &lt;- function(n) { sp &lt;- spline(x.1, y.1, method = &#39;periodic&#39;, n = n) return(list(x = sp$x, y = sp$y)) } Code n_x &lt;- 501 x &lt;- funkce_0(n_x)$x y0 &lt;- funkce_0(n_x)$y y1 &lt;- funkce_1(n_x)$y df &lt;- data.frame(x = rep(x, 2), y = c(y0, y1), Y = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), each = length(x))) df |&gt; ggplot(aes(x = x, y = y, colour = Y)) + geom_line(linewidth = 1) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 3.1: Znázornění dvou funkcí na intervalu \\([0, 1]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce. Tento posun budeme generovat s normálního rozdělění s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(fun(length(t))$y, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(X + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 12] t &lt;- seq(0, 12, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = &#39;x&#39;, y = &#39;y&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 3.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 4.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nyní jedná o periodické křivky na intervalu \\(I = [0, 12]\\), využijeme k vyhlazení fourierovu bázi. Za uzly bereme celý vektor t, budeme uvažovat tzv. harmonic acceleration penalties. Harmonic acceleration pro funkci \\(x(t)\\) je \\[ Lx = \\omega^2 Dx + D^3x, \\] kde \\(D^mx\\) značí \\(m\\)-tou derivaci funkce \\(x(t)\\) podle \\(t\\). Platí přitom \\(L\\sin(\\omega x) = 0 = L\\cos(\\omega x)\\). Potom jako penalizaci bereme hodnotu funkcionálu \\[ J_L(x) = \\int \\left[Lx(t)\\right]^2\\text dt. \\] Code rangeval &lt;- range(t) fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = length(t)) omega &lt;- 2 * pi / diff(rangeval) acvec &lt;- c(0, omega^2, 0) harmLfd &lt;- vec2Lfd(bwtvec = acvec, rangeval = rangeval) Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(fbasis, harmLfd, lambda.vect[index]) FSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(FSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;dashed&#39;, linewidth = 0.8) + geom_point(size = 2.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 1.5: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(fbasis, harmLfd, lambda.opt) FSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- FSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = &#39;Time&#39;, y = &#39;Function&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.1: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25) + theme_bw() + labs(x = &#39;Time&#39;, y = &#39;Function&#39;, colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;black&#39;, linewidth = 1, linetype = &#39;twodash&#39;) Obrázek 2.2: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. 4.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 4.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # summary(neighb.model) # shrnuti modelu # plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K # neighb.model$max.prob # maximalni presnost (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 5 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) # CV.results Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 5 s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.244. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 3.3: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # summary(neighb.model) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost # 1 - presnost Vidíme tedy, že chybovost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 5, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.25 a na testovacích datech 0.2167. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 4.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 98.3 % variability v datech. První hlavní komponenta potom vysvětluje 97.06 % a druhá 1.24 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 3.4: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (11.43 %), tak i na testovacích datech (15 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.5: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak chybovost klasifikátoru na trénovacích (12.86 %), tak i na testovacích datech (15 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.3.4 Support Vector Machines Definujeme pro další metody data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 12, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 2.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 2.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí Fourierovy báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 2.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými nebo Fourierovými, funkcemi (sekce 2.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 2.3.7.5. 4.3.4.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 12]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 11.43 % pro lineární jádro, 7.86 % pro polynomiální jádro a 11.43 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 10 % pro lineární jádro, 13.33 % pro polynomiální jádro a 15 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 4.3.4.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 11.43 % pro lineární jádro, 12.14 % pro polynomiální jádro a 12.14 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 18.33 % pro lineární jádro, 18.33 % pro polynomiální jádro a 18.33 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 3.7: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 4.3.4.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí Fourierovy báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 13.33 % pro lineární jádro, 21.67 % pro polynomiální jádro a 26.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 4.3.4.4 Projekce na Fourierovu bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na Fourierovu bázi, protože využití B-splinové báze není pro naše periodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Code k_cv &lt;- 10 # k-fold CV # hodnoty pro Fourierovu bazi rangeval &lt;- range(t) n_basis_min &lt;- 3 n_basis_max &lt;- 40 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na Fourierovu bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = fbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 34 0.09638278 ## poly 4 0.12443223 ## radial 4 0.12418040 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 34 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0964, 4 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1244 a 4 pro radiální jádro s hodnotou chybovosti 0.1242. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 1.14: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na Fourierovu bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = fbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0 % pro lineární jádro, 5.71 % pro polynomiální jádro a 7.86 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 21.67 % pro lineární jádro, 18.33 % pro polynomiální jádro a 15 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 4.4 Tabulka výsledků Tabulka 3.1: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.2500 0.2167 LDA 0.1143 0.1500 QDA 0.1286 0.1500 SVM linear - diskr 0.1143 0.1000 SVM poly - diskr 0.0786 0.1333 SVM rbf - diskr 0.1143 0.1500 SVM linear - PCA 0.1143 0.1833 SVM poly - PCA 0.1214 0.1833 SVM rbf - PCA 0.1214 0.1833 SVM linear - Fbasis 0.0000 0.1333 SVM poly - Fbasis 0.0000 0.2167 SVM rbf - Fbasis 0.0000 0.2667 SVM linear - projection 0.0000 0.2167 SVM poly - projection 0.0571 0.1833 SVM rbf - projection 0.0786 0.1500 4.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 10\\). Nyní zopakujeme celou předchozí část n.sim-krát a hodnoty chybovostí si budeme ukládat to objektu SIMUL_params. Přitom budeme měnit hodnotu parametru \\(\\sigma_{shift}\\) a podíváme se, jak se mění výsledky jednotlivých vybraných klasifikačních metod v závislosti na této hodnotě. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci pro kazdou hodnotu simulacniho parametru n.sim &lt;- 10 methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;) # vektor smerodatnych odchylek definujicich posunuti generovanych krivek shift_vector &lt;- seq(0, 10, length = 21) # vysledny objekt, do nehoz ukladame vysledky simulaci SIMUL_params &lt;- array(data = NA, dim = c(length(methods), 4, length(shift_vector)), dimnames = list( method = methods, metric = c(&#39;ERRtrain&#39;, &#39;Errtest&#39;, &#39;SDtrain&#39;, &#39;SDtest&#39;), sigma = paste0(shift_vector))) for (n_shift in 1:length(shift_vector)) { ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 12] t &lt;- seq(0, 12, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, shift_vector[n_shift]) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, shift_vector[n_shift]) rangeval &lt;- range(t) fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = length(t)) omega &lt;- 2 * pi / diff(rangeval) acvec &lt;- c(0, omega^2, 0) harmLfd &lt;- vec2Lfd(bwtvec = acvec, rangeval = rangeval) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -5, to = 3, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(fbasis, harmLfd, lambda.vect[index]) FSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(FSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(fbasis, harmLfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 12, length = 101) # normovani dat norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Fbasis&#39;, &#39;SVM poly - Fbasis&#39;, &#39;SVM rbf - Fbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na Fourierovu bázi # hodnoty pro Fourierovu bazi rangeval &lt;- range(t) n_basis_min &lt;- 3 n_basis_max &lt;- 40 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = fbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] fbasis &lt;- create.fourier.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = fbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, paste0(n_shift, &#39;: &#39;, sim)) } # Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) SIMUL_params[, , n_shift] &lt;- as.matrix(SIMULACE.df) } # ulozime vysledne hodnoty save(SIMUL_params, file = &#39;RData/simulace_parametry_shift_02b.RData&#39;) 4.5.1 Grafický výstup Podívejme se na závislost simulovaných výsledků na hodnotě parametru směrodatné odchylky vertikálního posunu. Code # pro testovaci data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(shift_vector), names_to = &#39;shift&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), shift = as.numeric(shift)) |&gt; filter(method %in% c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Fbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM rbf - projection&#39;)) |&gt; ggplot(aes(x = shift, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[shift]), y = expression(widehat(Err)[test]), colour = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;bottom&#39;) Obrázek 3.8: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{shift}\\), který definuje směrodatnou odchylku pro generování vertikálního posunutí simulovaných křivek. "],["simulace3.html", "Kapitola 5 Simulace 3 5.1 Simulace funkcionálních dat 5.2 Vyhlazení pozorovaných křivek 5.3 Klasifikace křivek 5.4 Tabulka výsledků 5.5 Simulační studie", " Kapitola 5 Simulace 3 5.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro simulaci nejprve: zvolíme vhodné funkce, generujeme body ze zvoleného intervalu, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 6]\\). Nyní vytvoříme funkce pomocí interpolačních polynomů. Nejprve si definujeme body, kterými má procházet naše křivka, a následně jimi proložíme interpolační polynom, který použijeme pro generování křivek pro klasifikaci. Code # definujici body pro tridu 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # definujici body pro tridu 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) Obrázek 5.1: Body definující interpolační polynomy pro obě klasifikační třídy. Pro výpočet interpolačních polynomů využijeme funkci poly.calc() z knihovny polynom. Dále definujeme funkce poly.0() a poly.1(), které budou počítat hodnoty polynomů v daném bodě intervalu. K jejich vytvoření použijeme funkci predict(), na jejíž vstup zadáme příslušný polynom a bod, ve kterám chceme polynom vyhodnotit. Code # vypocet polynomu polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # vykresleni polynomu xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) Obrázek 3.2: Znázornění dvou funkcí na intervalu \\(I = [0, 6]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce (parametr sigma_shift). Tento posun budeme generovat s normálního rozdělení s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 5.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Opět využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor t, standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -4, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) Obrázek 5.2: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 5.3: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code abs.labs &lt;- paste(&quot;Klasifikační třída:&quot;, c(&quot;$Y = 0$&quot;, &quot;$Y = 1$&quot;)) names(abs.labs) &lt;- c(&#39;0&#39;, &#39;1&#39;) fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(eval.fd(fdobj = mean.fd(XXfd[1:n]), evalarg = t), eval.fd(fdobj = mean.fd(XXfd[(n + 1):(2 * n)]), evalarg = t)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, #group = interaction(time, group), colour = group)) + geom_line(aes(group = time), linewidth = 0.05, alpha = 0.5) + theme_bw() + labs(x = &quot;$t$&quot;, y = &quot;$x_i(t)$&quot;, colour = &#39;Klasifikační\\n třída&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;1&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;tomato&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;0&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;deepskyblue2&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + geom_line(data = DFmean |&gt; mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))), aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 0.8, linetype = &#39;dashed&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + facet_wrap(~group, labeller = labeller(group = abs.labs)) + scale_y_continuous(expand = c(0.02, 0.02)) + theme(legend.position = &#39;none&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Obrázek 1.6: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čarou je zakreslen průměr pro každou třídu, přerušovaně pak pro druhou třídu. Code # ggsave(&quot;figures/kap6_sim_03_curves.tex&quot;, device = tikz, width = 8, height = 4) Podívejme se ještě na srovnání průměrných průběhů z detailního pohledu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Obrázek 5.4: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 5.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěšnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 5.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # neighb.model$max.prob # maximalni presnost # (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) # CV.results Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 5 s hodnotou chybovosti spočtené pomocí 10-násobné CV 0.3429. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 5.5: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() Vidíme tedy, že chybovost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 5, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.3571 a na testovacích datech 0.3833. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 5.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 98.72 % variability v datech. První hlavní komponenta potom vysvětluje 98.2 % a druhá 0.52 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 5.6: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (41.43 %), tak i na testovacích datech (40 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak chybovost klasifikátoru na trénovacích (35.71 %), tak i na testovacích datech (40 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 5.7: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 5.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 3.7: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 29, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 35, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 35 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 11 s validační chybovostí 0.072. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) Obrázek 5.8: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 11 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 3.57 %) i testovací chybovost (rovna 8.33 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Klasifikační\\n třída&#39;) Obrázek 1.14: Závislost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line() + theme_bw() + labs(x = expression(x[1]), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Obrázek 2.9: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 6]\\). Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro časy \\(t\\) z prostředka a začátku intervalu \\([0, 6]\\), zatímco pro pozdějsí časy jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd na začátku a konci intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy chybovost klasifikátoru na trénovacích (40.71 %) i na testovacích datech (40 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.17: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 5.3.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [0, 6]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost klasifikátoru na testovacích datech je tedy 46.67 % a na trénovacích datech 33.57 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 5.9: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.19: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 2 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na testovacích datech je tedy 40 % a na trénovacích datech 37.86 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 5.10: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 5.11: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na trénovacích datech je tedy 33.57 % a na testovacích datech 45 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 2.11: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 5.12: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 5.3.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 6]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 0.71 % a na testovacích datech 40 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních p = 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost na trénovacích datech je tedy 4.29 % a na testovacích datech 40 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost tohoto klasifikátoru na trénovacích datech je 0.71 % a na testovacích datech 36.67 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.3.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního a radiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na takových hodnotách, které jsme odhadli pomocí CV na jednom vygenerovaném datovém souboru, volíme přitom \\(\\alpha_0 = 1\\). V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 5.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 5.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 5.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 5.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 5.3.7.5. 5.3.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 6]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 5.71 % pro lineární jádro, 2.14 % pro polynomiální jádro a 3.57 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 16.67 % pro lineární jádro, 16.67 % pro polynomiální jádro a 13.33 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.3.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.6, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.01, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 44.29 % pro lineární jádro, 37.86 % pro polynomiální jádro a 37.14 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 48.33 % pro lineární jádro, 43.33 % pro polynomiální jádro a 40 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) # + Obrázek 5.13: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code # geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), # colour = &#39;black&#39;) + # geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), # colour = &#39;black&#39;) Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.3.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 500, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 500, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.005, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 4.29 % pro lineární jádro, 4.29 % pro polynomiální jádro a 7.14 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 10 % pro lineární jádro, 11.67 % pro polynomiální jádro a 15 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.3.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 10 0.06457875 ## poly 10 0.07783883 ## radial 10 0.07838828 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 10 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0646, 10 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0778 a 10 pro radiální jádro s hodnotou chybovosti 0.0784. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &#39;Jádro&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 5.14: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 3.57 % pro lineární jádro, 3.57 % pro polynomiální jádro a 5 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 8.33 % pro lineární jádro, 8.33 % pro polynomiální jádro a 10 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 5.3.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 5.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 5.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 5.3.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 5.3.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku3 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 5.3.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). 5.3.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 5.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.1000 0.3833 SVM poly - RKHS 0.0286 0.3000 SVM rbf - RKHS 0.0786 0.2667 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 5.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 11 8.4834 0.0727 linear poly 12 37.2759 0.1008 polynomial radial 40 0.4394 0.0993 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 11 a \\(\\gamma={}\\) 8.4834 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0727, \\(d={}\\) 12 a \\(\\gamma={}\\) 37.2759 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1008 a \\(d={}\\) 40 a \\(\\gamma={}\\) 0.4394 pro radiální jádro s hodnotou chybovosti 0.0993. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 5.15: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.4: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0571 0.1333 SVM poly - RKHS - radial 0.0643 0.1167 SVM rbf - RKHS - radial 0.0143 0.1500 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 5.71 % pro lineární jádro, 6.43 % pro polynomiální jádro a 1.43 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 13.33 % pro lineární jádro, 11.67 % pro polynomiální jádro a 15 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 5.3.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 1.5: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 18 4 0.1432 linear poly 15 4 0.1644 polynomial radial 30 4 0.1472 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 18 a \\(p={}\\) 4 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1432, \\(d={}\\) 15 a \\(p={}\\) 4 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1644 a \\(d={}\\) 30 a \\(p={}\\) 4 pro radiální jádro s hodnotou chybovosti 0.1472. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.6: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.1071 0.1500 SVM poly - RKHS - poly 0.0571 0.1833 SVM rbf - RKHS - poly 0.1000 0.2167 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 10.71 % pro lineární jádro, 5.71 % pro polynomiální jádro a 10 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 15 % pro lineární jádro, 18.33 % pro polynomiální jádro a 21.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 5.3.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 5.3: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 29 0.3799 linear poly 39 0.3469 polynomial radial 37 0.3674 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 29 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.3799, \\(d={}\\) 39 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.3469 a \\(d={}\\) 37 pro radiální jádro s hodnotou chybovosti 0.3674. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 5.4: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.2643 0.3000 SVM poly - RKHS - linear 0.1786 0.3000 SVM rbf - RKHS - linear 0.2714 0.3667 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 26.43 % pro lineární jádro, 17.86 % pro polynomiální jádro a 27.14 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 30 % pro lineární jádro, 30 % pro polynomiální jádro a 36.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 5.4 Tabulka výsledků Tabulka 5.5: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3571 0.3833 LDA 0.4143 0.4000 QDA 0.3571 0.4000 LR functional 0.0357 0.0833 LR score 0.4071 0.4000 Tree - diskr. 0.3357 0.4667 Tree - score 0.3786 0.4000 Tree - Bbasis 0.3357 0.4500 RForest - diskr 0.0071 0.4000 RForest - score 0.0429 0.4000 RForest - Bbasis 0.0071 0.3667 SVM linear - diskr 0.0571 0.1667 SVM poly - diskr 0.0214 0.1667 SVM rbf - diskr 0.0357 0.1333 SVM linear - PCA 0.4429 0.4833 SVM poly - PCA 0.3786 0.4333 SVM rbf - PCA 0.3714 0.4000 SVM linear - Bbasis 0.0429 0.1000 SVM poly - Bbasis 0.0429 0.1167 SVM rbf - Bbasis 0.0714 0.1500 SVM linear - projection 0.0357 0.0833 SVM poly - projection 0.0357 0.0833 SVM rbf - projection 0.0500 0.1000 SVM linear - RKHS - radial 0.0571 0.1333 SVM poly - RKHS - radial 0.0643 0.1167 SVM rbf - RKHS - radial 0.0143 0.1500 SVM linear - RKHS - poly 0.1071 0.1500 SVM poly - RKHS - poly 0.0571 0.1833 SVM rbf - RKHS - poly 0.1000 0.2167 SVM linear - RKHS - linear 0.2643 0.3000 SVM poly - RKHS - linear 0.1786 0.3000 SVM rbf - RKHS - linear 0.2714 0.3667 5.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -5, to = 3, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # normovani dat norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.6, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, gamma = 0.01, cost = 1000, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 500, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 500, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.005, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by =2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_03_cv.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/simulace_03_res_cv.RData&#39;) 5.5.1 Výsledky Tabulka 5.6: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.3487 0.3998 0.0403 0.0741 LDA 0.3654 0.4083 0.0672 0.0963 QDA 0.3607 0.4167 0.0660 0.0981 LR_functional 0.0377 0.1207 0.0294 0.0464 LR_score 0.3680 0.4027 0.0724 0.0894 Tree_discr 0.2857 0.4315 0.0760 0.0749 Tree_score 0.3334 0.4277 0.0780 0.1041 Tree_Bbasis 0.2884 0.4378 0.0702 0.0755 RF_discr 0.0114 0.3485 0.0093 0.0738 RF_score 0.0525 0.4452 0.0172 0.0955 RF_Bbasis 0.0105 0.3433 0.0081 0.0743 SVM linear - diskr 0.0549 0.1172 0.0223 0.0423 SVM poly - diskr 0.0385 0.1978 0.0194 0.0571 SVM rbf - diskr 0.0517 0.1355 0.0237 0.0449 SVM linear - PCA 0.4074 0.4790 0.0773 0.1069 SVM poly - PCA 0.3486 0.4363 0.0621 0.1018 SVM rbf - PCA 0.3555 0.4295 0.0638 0.1036 SVM linear - Bbasis 0.0430 0.1008 0.0202 0.0334 SVM poly - Bbasis 0.0219 0.1690 0.0167 0.0536 SVM rbf - Bbasis 0.0611 0.1743 0.0254 0.0577 SVM linear - projection 0.0434 0.0955 0.0200 0.0412 SVM poly - projection 0.0290 0.1137 0.0165 0.0470 SVM rbf - projection 0.0506 0.1175 0.0181 0.0521 SVM linear - RKHS - radial 0.0545 0.1440 0.0252 0.0457 SVM poly - RKHS - radial 0.0162 0.1647 0.0186 0.0510 SVM rbf - RKHS - radial 0.0452 0.1535 0.0210 0.0504 SVM linear - RKHS - poly 0.0762 0.1842 0.0363 0.0572 SVM poly - RKHS - poly 0.0341 0.1913 0.0217 0.0643 SVM rbf - RKHS - poly 0.0757 0.1877 0.0255 0.0575 SVM linear - RKHS - linear 0.2557 0.3897 0.0588 0.0618 SVM poly - RKHS - linear 0.1883 0.3723 0.0490 0.0612 SVM rbf - RKHS - linear 0.2490 0.3728 0.0371 0.0693 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # nastavime jinak nazvy klasifikacnich metod methods_names &lt;- c( &#39;$K$ nejbližších sousedů&#39;, &#39;Lineární diskriminační analýza&#39;, &#39;Kvadratická diskriminační analýza&#39;, &#39;Funkcionální logistická regrese&#39;, &#39;Logistické regrese s fPCA&#39;, &#39;Rozhodovací strom -- diskretizace&#39;, &#39;Rozhodovací strom -- fPCA&#39;, &#39;Rozhodovací strom -- bázové koeficienty&#39;, &#39;Náhodný les -- diskretizace&#39;, &#39;Náhodný les -- fPCA&#39;, &#39;Náhodný les -- bázové koeficienty&#39;, &#39;SVM (linear) -- diskretizace&#39;, &#39;SVM (poly) -- diskretizace&#39;, &#39;SVM (radial) -- diskretizace&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (poly) -- fPCA&#39;, &#39;SVM (radial) -- fPCA&#39;, &#39;SVM (linear) -- bázové koeficienty&#39;, &#39;SVM (poly) -- bázové koeficienty&#39;, &#39;SVM (radial) -- bázové koeficienty&#39;, &#39;SVM (linear) -- projekce&#39;, &#39;SVM (poly) -- projekce&#39;, &#39;SVM (radial) -- projekce&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39;, &#39;RKHS (radial SVR) $+$ SVM (poly)&#39;, &#39;RKHS (radial SVR) $+$ SVM (radial)&#39;, &#39;RKHS (poly SVR) $+$ SVM (linear)&#39;, &#39;RKHS (poly SVR) $+$ SVM (poly)&#39;, &#39;RKHS (poly SVR) $+$ SVM (radial)&#39;, &#39;RKHS (linear SVR) $+$ SVM (linear)&#39;, &#39;RKHS (linear SVR) $+$ SVM (poly)&#39;, &#39;RKHS (linear SVR) $+$ SVM (radial)&#39; ) # barvy pro boxploty box_col &lt;- c(&#39;#4dd2ff&#39;, &#39;#0099cc&#39;, &#39;#00ace6&#39;, &#39;#00bfff&#39;, &#39;#1ac5ff&#39;, rep(&#39;#33ccff&#39;, 3), rep(&#39;#0086b3&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff6347&#39;, 3), rep(&#39;#ff7961&#39;, 3), rep(&#39;#ff4d2e&#39;, 3), rep(&#39;#fa2600&#39;, 9)) # box_col &lt;- c(&#39;#CA0A0A&#39;, &#39;#fa2600&#39;, &#39;#fa2600&#39;, &#39;#D15804&#39;, # &#39;#D15804&#39;, rep(&#39;#D3006D&#39;, 3), rep(&#39;#BE090F&#39;, 3), c(&quot;#12DEE8&quot;, &quot;#4ECBF3&quot;, &quot;#127DE8&quot;, &quot;#4C3CD3&quot;, &quot;#4E65F3&quot;, &quot;#4E9EF3&quot;, &quot;#081D58&quot;) |&gt; rep(each = 3)) # alpha pro boxploty box_alpha &lt;- c(0.9, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, seq(0.9, 0.6, length = 9)) #- 0.3 Code # pro trenovaci data # tikz(file = &quot;figures/DP_sim_03_boxplot_train.tex&quot;, width = 10, height = 6) SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods2, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 5.16: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # dev.off() Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, # y = &quot;$\\\\widehat{\\\\textnormal{Err}}_{test}$&quot; y = expression(widehat(Err)[test]) ) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 50, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 3, color = &quot;black&quot;, alpha = 0.9) + # scale_x_discrete(labels = methods_names) + # theme(plot.margin = unit(c(0.5, 0.5, 2, 2), &quot;cm&quot;)) + # scale_fill_manual(values = box_col) + # scale_alpha_manual(values = box_alpha) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;gray20&#39;, alpha = 0.8) Obrázek 5.17: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # ggsave(&quot;figures/kap6_sim_03_boxplot_test.tex&quot;, device = tikz, width = 9, height = 7) Chtěli bychom nyní formálně otestovat, zda jsou některé klasifikační metody na základě předchozí simulace na těchto datech lepší než jiné, případně ukázat, že je můžeme považovat za stejně úspěšné. Vzhledem k nesplnění předpokladu normality nemůžeme použít klasický párový t-test. Využijeme jeho neparametrickou alternativu - párový Wilcoxonův test. Musíme si však v tomto případě dávat pozor na interpretaci. Code # overeni normality library(nortest) lillie.test(SIMULACE$test[, &#39;LR_functional&#39;])$p.value ## [1] 6.007473e-06 Code lillie.test(SIMULACE$test[, &#39;SVM linear - projection&#39;])$p.value ## [1] 0.01873872 Code wilcox.test(SIMULACE$test[, &#39;SVM linear - Bbasis&#39;], SIMULACE$test[, &#39;SVM linear - projection&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.1607668 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.5918735 Podívejme se na srovnání funkcionální logistické regrese a projekce na B-splinovou bázi. Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - projection&#39;], alternative = &#39;greater&#39;, paired = T)$p.value ## [1] 1.587437e-06 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM poly - projection&#39;], alternative = &#39;greater&#39;, paired = T)$p.value ## [1] 0.08107918 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM rbf - projection&#39;], alternative = &#39;greater&#39;, paired = T)$p.value ## [1] 0.2597939 Vidíme, že lineární jádro má statisticky významně menší chybovost v porovnání s funkcionální logistickou regresí (\\(p\\)-hodnota 1.5874375^{-6}). Naopak u zbylých jader metody SVM se nepodařilo ukázat, že medián jejich testovacích chybovostí je menší než medián fLR. Ještě by nás mohly zajímat další porovnání. Code # wilcox.test(SIMULACE$test[, &#39;SVM rbf - RKHS - radial&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;less&#39;, paired = T)$p.value # wilcox.test(SIMULACE$test[, &#39;SVM linear - RKHS - radial&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;less&#39;, paired = T)$p.value wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - RKHS - radial&#39;], alternative = &#39;less&#39;, paired = T)$p.value ## [1] 2.343969e-06 Code # wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM rbf - RKHS - poly&#39;], alternative = &#39;less&#39;, paired = T)$p.value wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM rbf - RKHS - radial&#39;], alternative = &#39;less&#39;, paired = T)$p.value ## [1] 6.179561e-08 Code # wilcox.test(SIMULACE$test[, &#39;SVM poly - diskr&#39;], SIMULACE$test[, &#39;SVM poly - Bbasis&#39;], alternative = &#39;greater&#39;, paired = T)$p.value Ještě se podívejme na srovnání jader u projekce. Code wilcox.test(SIMULACE$test[, &#39;SVM linear - projection&#39;], SIMULACE$test[, &#39;SVM poly - projection&#39;], alternative = &#39;less&#39;, paired = T)$p.value ## [1] 4.017156e-05 Code wilcox.test(SIMULACE$test[, &#39;SVM rbf - projection&#39;], SIMULACE$test[, &#39;SVM poly - projection&#39;], alternative = &#39;greater&#39;, paired = T)$p.value ## [1] 0.1987694 Code wilcox.test(SIMULACE$test[, &#39;SVM linear - projection&#39;], SIMULACE$test[, &#39;SVM rbf - projection&#39;], alternative = &#39;less&#39;, paired = T)$p.value ## [1] 1.374897e-05 Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 5.7: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 3.0 nharm 1.0 LR_func_n_basis 12.0 SVM_d_Linear 13.0 SVM_d_Poly 12.0 SVM_d_Radial 11.0 SVM_RKHS_radial_gamma1 6.8 SVM_RKHS_radial_gamma2 3.2 SVM_RKHS_radial_gamma3 1.2 SVM_RKHS_radial_d1 17.0 SVM_RKHS_radial_d2 19.0 SVM_RKHS_radial_d3 17.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 4.0 SVM_RKHS_poly_p3 4.0 SVM_RKHS_poly_d1 25.0 SVM_RKHS_poly_d2 30.0 SVM_RKHS_poly_d3 31.0 SVM_RKHS_linear_d1 21.0 SVM_RKHS_linear_d2 22.0 SVM_RKHS_linear_d3 23.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.18: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.19: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.20: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.21: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.22: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["simulace3sigma.html", "Kapitola 6 Závislost na parametru \\(\\sigma^2\\) 6.1 Simulace funkcionálních dat 6.2 Vyhlazení pozorovaných křivek 6.3 Klasifikace křivek 6.4 Tabulka výsledků 6.5 Simulační studie", " Kapitola 6 Závislost na parametru \\(\\sigma^2\\) V této části se budeme zabývat závislostí výsledků z předchozí Kapitoly 5 na hodnotě \\(\\sigma^2\\), která definuje rozptyl normálního rozdělení, ze kterého generujeme náhodné chyby kolem generujících křivek (můžeme říci, že \\(\\sigma^2\\) nese informaci například o chybovosti měření určitého přístroje). Očekáváme, že s rostoucí hodnotou \\(\\sigma^2\\) se budou výsledky jednotlivých metod zhoršovat a tudíž klasifikace nebude tak úspěšná. V následující sekci 7 se pak podíváme na závislost výsledků na hodnotě \\(\\sigma^2_{shift}\\), tedy na rozptylu normálního rozdělení, ze kterého generujeme posun pro generované křivky. 6.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro simulaci nejprve: zvolíme vhodné funkce, generujeme body ze zvoleného intervalu, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 6]\\). Nyní vytvoříme funkce pomocí interpolačních polynomů. Nejprve si definujeme body, kterými má procházet naše křivka, a následně jimi proložíme interpolační polynom, který použijeme pro generování křivek pro klasifikaci. Code # definujici body pro tridu 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # definujici body pro tridu 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 1.1: Body definující interpolační polynomy. Pro výpočet interpolačních polynomů využijeme funkci poly.calc() z knihovny polynom. Dále definujeme funkce poly.0() a poly.1(), které budou počítat hodnoty polynomů v daném bodě intervalu. K jejich vytvoření použijeme funkci predict(), na jejíž vstup zadáme příslušný polynom a bod, ve kterám chceme polynom vyhodnotit. Code # vypocet polynomu polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # vykresleni polynomu xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 1.2: Znázornění dvou funkcí na intervalu \\(I = [0, 6]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce (parametr sigma_shift). Tento posun budeme generovat s normálního rozdělení s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 1.5: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 6.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Opět využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor t, standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) Obrázek 2.3: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 6.1: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Obrázek 5.2: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Tlustou čarou je zakreslen průměr pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Obrázek 5.3: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Tlustou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 6.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 6.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # summary(neighb.model) # shrnuti modelu # plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K # neighb.model$max.prob # maximalni presnost (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 5 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) # CV.results Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 5 s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.3429. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 3.4: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # summary(neighb.model) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost # 1 - presnost Vidíme tedy, že chybovost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 5, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.3571 a na testovacích datech 0.3833. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 6.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 98.72 % variability v datech. První hlavní komponenta potom vysvětluje 98.2 % a druhá 0.52 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 2.5: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (41.43 %), tak i na testovacích datech (40 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.9: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak chybovost klasifikátoru na trénovacích (35.71 %), tak i na testovacích datech (40 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 6.2: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 6.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 1.13: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 29, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) # CV.results Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 11 s validační chybovostí 0.072. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) Obrázek 2.7: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 11 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 3.57 %) i testovací chybovost (rovna 8.33 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Třída&#39;) Obrázek 5.8: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Obrázek 6.3: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 6]\\). Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.5 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 6.3.5.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 6]\\). Code grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 0 % a na testovacích datech 38.33 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.5.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních p = 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost na trénovacích datech je tedy 3.57 % a na testovacích datech 41.67 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.5.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost tohoto klasifikátoru na trénovacích datech je 0 % a na testovacích datech 41.67 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.6 Support Vector Machines Definujeme pro další metody data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. 6.3.6.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 6]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Parametry pro jednotlivá jádra jsme odhadli pomocí CV na jednom vygenerovaném datovém souboru. Tyto hodnoty \\(C\\), \\(d\\) a \\(\\gamma\\) použijeme pro všechny datové soubory v této simulaci. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 5.71 % pro lineární jádro, 2.14 % pro polynomiální jádro a 3.57 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 16.67 % pro lineární jádro, 16.67 % pro polynomiální jádro a 13.33 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.3.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních p = 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.6, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.01, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 44.29 % pro lineární jádro, 37.86 % pro polynomiální jádro a 37.14 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 48.33 % pro lineární jádro, 43.33 % pro polynomiální jádro a 40 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 5.11: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.3.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 500, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 500, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.005, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 4.29 % pro lineární jádro, 4.29 % pro polynomiální jádro a 7.14 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 10 % pro lineární jádro, 11.67 % pro polynomiální jádro a 15 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.3.6.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 11 0.04306319 ## poly 11 0.07699176 ## radial 10 0.06449176 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 11 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0431, 11 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.077 a 10 pro radiální jádro s hodnotou chybovosti 0.0645. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 5.12: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 4.29 % pro lineární jádro, 3.57 % pro polynomiální jádro a 5 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 10 % pro lineární jádro, 8.33 % pro polynomiální jádro a 10 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 6.3.6.5 RKHS + SVM 6.3.6.5.0.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 6.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.1000 0.3833 SVM poly - RKHS 0.0286 0.3000 SVM rbf - RKHS 0.0786 0.2667 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 6.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 10 13.8950 0.0630 linear poly 19 22.7585 0.0912 polynomial radial 40 0.4394 0.1016 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 10 a \\(\\gamma={}\\) 13.895 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.063, \\(d={}\\) 19 a \\(\\gamma={}\\) 22.7585 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0912 a \\(d={}\\) 40 a \\(\\gamma={}\\) 0.4394 pro radiální jádro s hodnotou chybovosti 0.1016. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 1.22: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 2.2: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0571 0.1333 SVM poly - RKHS - radial 0.0571 0.1667 SVM rbf - RKHS - radial 0.0143 0.1500 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 5.71 % pro lineární jádro, 5.71 % pro polynomiální jádro a 1.43 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 13.33 % pro lineární jádro, 16.67 % pro polynomiální jádro a 15 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 6.4 Tabulka výsledků Tabulka 6.3: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3571 0.3833 LDA 0.4143 0.4000 QDA 0.3571 0.4000 LR functional 0.0357 0.0833 RForest - diskr 0.0000 0.3833 RForest - score 0.0357 0.4167 RForest - Bbasis 0.0000 0.4167 SVM linear - diskr 0.0571 0.1667 SVM poly - diskr 0.0214 0.1667 SVM rbf - diskr 0.0357 0.1333 SVM linear - PCA 0.4429 0.4833 SVM poly - PCA 0.3786 0.4333 SVM rbf - PCA 0.3714 0.4000 SVM linear - Bbasis 0.0429 0.1000 SVM poly - Bbasis 0.0429 0.1167 SVM rbf - Bbasis 0.0714 0.1500 SVM linear - projection 0.0429 0.1000 SVM poly - projection 0.0357 0.0833 SVM rbf - projection 0.0500 0.1000 SVM linear - RKHS - radial 0.0571 0.1333 SVM poly - RKHS - radial 0.0571 0.1667 SVM rbf - RKHS - radial 0.0143 0.1500 6.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 25\\). Nyní zopakujeme celou předchozí část n.sim-krát a hodnoty chybovostí si budeme ukládat to objektu SIMUL_params. Přitom budeme měnit hodnotu parametru \\(\\sigma\\) a podíváme se, jak se mění výsledky jednotlivých vybraných klasifikačních metod v závislosti na této hodnotě. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci pro kazdou hodnotu simulacniho parametru n.sim &lt;- 25 methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39; ) # vektor smerodatnych odchylek definujicich rozptyl kolem generujicich krivek sigma_vector &lt;- seq(0.1, 5, length = 30) # vysledny objekt, do nehoz ukladame vysledky simulaci SIMUL_params &lt;- array(data = NA, dim = c(length(methods), 4, length(sigma_vector)), dimnames = list( method = methods, metric = c(&#39;ERRtrain&#39;, &#39;Errtest&#39;, &#39;SDtrain&#39;, &#39;SDtest&#39;), sigma = paste0(sigma_vector))) for (n_sigma in 1:length(sigma_vector)) { ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, sigma_vector[n_sigma], 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, sigma_vector[n_sigma], 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -5, to = 3, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 5 # k-fold CV neighbours &lt;- 1:15 #c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max # k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) # normovani dat norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.6, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.01, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 500, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 500, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.005, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 # length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # # jadro a jadrova matice ... Gaussovske s parametrem gamma # Gauss.kernel &lt;- function(x, y, gamma) { # return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) # } # # Kernel.RKHS &lt;- function(x, gamma) { # K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) # for(i in 1:nrow(K)) { # for(j in 1:ncol(K)) { # K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) # } # } # return(K) # } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(10, 40, by = 5) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 5) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) # CV_RESULTS$SVM_RKHS_radial_gamma[sim] &lt;- gamma.opt # CV_RESULTS$SVM_RKHS_radial_d[sim] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, paste0(n_sigma, &#39;: &#39;, sim)) } # Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) SIMUL_params[, , n_sigma] &lt;- as.matrix(SIMULACE.df) } # ulozime vysledne hodnoty save(SIMUL_params, file = &#39;RData/simulace_parametry_sigma_03_rf_cv.RData&#39;) 6.5.1 Grafický výstup Podívejme se na závislost simulovaných výsledků na hodnotě parametru směrodatné odchylky normálního rozdělení pro chyby měření. Nejprve si vykresleme data pro trénovací chybovosti. Code # pro testovaci data SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(sigma_vector), names_to = &#39;sigma&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), sigma = as.numeric(sigma)) |&gt; # filter(method %in% c(&#39;KNN&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;,# &#39;SVM linear - diskr&#39;, # &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, # &#39;SVM linear - projection&#39;, &#39;SVM rbf - projection&#39;, # &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, # &#39;SVM rbf - RKHS - radial&#39;)) |&gt; # filter(method %in% c(&#39;KNN&#39;, &#39;LR_functional&#39;, &#39;SVM linear - diskr&#39;, # &#39;RF_Bbasis&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, # &#39;SVM linear - projection&#39;, &#39;SVM linear - RKHS - radial&#39;)) |&gt; ggplot(aes(x = sigma, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[x]), y = expression(widehat(Err)[test]), colour = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;bottom&#39;) Obrázek 6.4: Graf závislosti trénovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{x}\\), který definuje směrodatnou odchylku pro generování náhodných odchylek kolem generujících křivek. Následně pro testovací chybovosti. Code # pro testovaci data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(sigma_vector), names_to = &#39;sigma&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), sigma = as.numeric(sigma)) |&gt; # filter(method %in% c(&#39;KNN&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;,# &#39;SVM linear - diskr&#39;, # &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, # &#39;SVM linear - projection&#39;, &#39;SVM rbf - projection&#39;, # &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, # &#39;SVM rbf - RKHS - radial&#39;)) |&gt; # filter(method %in% c(&#39;KNN&#39;, &#39;LR_functional&#39;, &#39;SVM linear - diskr&#39;, # &#39;RF_Bbasis&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, # &#39;SVM linear - projection&#39;, &#39;SVM linear - RKHS - radial&#39;)) |&gt; ggplot(aes(x = sigma, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[x]), y = expression(widehat(Err)[test]), colour = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;bottom&#39;) Obrázek 6.5: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{x}\\), který definuje směrodatnou odchylku pro generování náhodných odchylek kolem generujících křivek. Jelikož jsou výsledky zatíženy náhodnými výchylkami a zvýšení počtu opakování n.sim by bylo výpočetně velmi náročné, vyhlaďme nyní křivky závislosti průměrné testovací a trénovací chybovosti. Jelikož chybovost je nezáporná veličina, budeme křivky vyhlazovat s tímto vědomím. Navíc (aby vyhlazené křivky dobře kopírovaly pozorované diskrétní hodnoty) uvažujeme jinou váhu pro malé a velké hodnoty \\(\\sigma\\). Code methods_subset &lt;- c(&#39;KNN&#39;, &#39;LR_functional&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM linear - RKHS - radial&#39;) Dat &lt;- SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; t() breaks &lt;- sigma_vector rangeval &lt;- range(breaks) norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd # gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV # for(index in 1:length(lambda.vect)) { # curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) # BSmooth &lt;- smooth.basis(breaks, Dat, curv.Fdpar) # vyhlazeni # gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky # } # # GCV &lt;- data.frame( # lambda = round(log10(lambda.vect), 3), # GCV = gcv # ) # # # najdeme hodnotu minima # lambda.opt &lt;- lambda.vect[which.min(gcv)] # # curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) # BSmooth &lt;- smooth.basis(breaks, Dat, curv.fdPar) # smooth.pos # XXfd &lt;- BSmooth$fd # Wfdobj # # fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, # eval.posfd # evalarg = seq(min(sigma_vector), max(sigma_vector), # length = 101)) # df_plot_smooth &lt;- data.frame( # method = rep(colnames(fdobjSmootheval), each = dim(fdobjSmootheval)[1]), # value = c(fdobjSmootheval), # sigma = rep(seq(min(sigma_vector), max(sigma_vector), length = 101), length(methods)) # ) |&gt; # mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) ## for positive smoothing # najdeme hodnotu minima lambda.opt &lt;- 1e-4 curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.pos(breaks, Dat, curv.fdPar, dbglev = 0, wtvec = c(rep(1000, 5), rep(8, 5), rep(1, 20))) # smooth.pos Code XXfd &lt;- BSmooth$Wfdobj # Wfdobj fdobjSmootheval &lt;- eval.posfd(Wfdobj = XXfd, # eval.posfd evalarg = seq(min(sigma_vector), max(sigma_vector), length = 101)) df_plot_smooth &lt;- data.frame( method = rep(methods, each = dim(fdobjSmootheval)[1]), value = c(fdobjSmootheval), sigma = rep(seq(min(sigma_vector), max(sigma_vector), length = 101), length(methods)) ) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) Grafy ke zvýšení přehlednosti vykreslíme pouze pro podmnožinu metod. Code methods_names &lt;- c( &#39;$K$ nejbližších sousedů&#39;, &#39;Funkcionální logistická regrese&#39;, &#39;Náhodný les -- bázové koeficienty&#39;, &#39;SVM (linear) -- diskretizace&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (linear) -- bázové koeficienty&#39;, &#39;SVM (linear) -- projekce&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39; ) Code # pro testovaci data SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(sigma_vector), names_to = &#39;sigma&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), sigma = as.numeric(sigma)) |&gt; # filter(method %in% c(&#39;KNN&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, # &#39;RF_discr&#39;, #&#39;RF_score&#39;, &#39;RF_Bbasis&#39;, # &#39;SVM linear - diskr&#39;,# &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, # # &#39;SVM linear - PCA&#39;,# &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, # &#39;SVM linear - Bbasis&#39;,# &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, # &#39;SVM linear - projection&#39;,# &#39;SVM poly - projection&#39;, # # &#39;SVM rbf - projection&#39;, # &#39;SVM linear - RKHS - radial&#39;#, &#39;SVM poly - RKHS - radial&#39;, # # &#39;SVM rbf - RKHS - radial&#39; # )) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) |&gt; ggplot(aes(x = sigma, y = Err, colour = method, shape = method)) + geom_point(alpha = 0.7, size = 0.6) + theme_bw() + labs(x = &#39;sigma&#39;, y = &#39;Testovaci chybovost&#39;, colour = &#39;Klasifikační metoda&#39;, linetype = &#39;Klasifikační metoda&#39;, shape = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;right&#39;, plot.margin = unit(c(0.1, 0.7, 0.3, 0.3), &quot;cm&quot;), panel.grid.minor.x = element_blank()) + # guides(colour=guide_legend(direction = &#39;vertical&#39;), # linetype=guide_legend(direction = &#39;vertical&#39;), # shape=guide_legend(direction = &#39;vertical&#39;)) + # scale_y_log10() + geom_line(data = df_plot_smooth, aes(x = sigma, y = value, col = method, linetype = method), linewidth = 0.95) + scale_colour_manual(values = rep(c(&#39;deepskyblue2&#39;, &#39;tomato&#39;), c(4, 4)), labels = methods_names) + scale_linetype_manual(values = rep(c(&#39;dotdash&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dotted&#39;), 2), labels = methods_names) + scale_shape_manual(values = rep(c(16, 1, 17, 4, 16, 1, 17, 4)), labels = methods_names) + guides(colour = guide_legend(override.aes = list(size = 1.2, alpha = 0.7)), linetype = guide_legend(override.aes = list(linewidth = 0.8))) Obrázek 6.6: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{x}\\). Code # ggsave(&quot;figures/kap6_sim_03sigma_curvesErr.tex&quot;, device = tikz, width = 9, height = 4.5) Nyní provedeme totéž pro testovací chybovosti. Code methods_subset &lt;- c(&#39;KNN&#39;, &#39;LR_functional&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM linear - RKHS - radial&#39;) Dat &lt;- SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; t() breaks &lt;- sigma_vector rangeval &lt;- range(breaks) norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd # gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV # for(index in 1:length(lambda.vect)) { # curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) # BSmooth &lt;- smooth.basis(breaks, Dat, curv.Fdpar) # vyhlazeni # gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky # } # # GCV &lt;- data.frame( # lambda = round(log10(lambda.vect), 3), # GCV = gcv # ) # # # najdeme hodnotu minima # lambda.opt &lt;- lambda.vect[which.min(gcv)] # # curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) # BSmooth &lt;- smooth.basis(breaks, Dat, curv.fdPar) # smooth.pos # XXfd &lt;- BSmooth$fd # Wfdobj # # fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, # eval.posfd # evalarg = seq(min(sigma_vector), max(sigma_vector), # length = 101)) # df_plot_smooth &lt;- data.frame( # method = rep(colnames(fdobjSmootheval), each = dim(fdobjSmootheval)[1]), # value = c(fdobjSmootheval), # sigma = rep(seq(min(sigma_vector), max(sigma_vector), length = 101), length(methods)) # ) |&gt; # mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) ## for positive smoothing # najdeme hodnotu minima lambda.opt &lt;- 3e-4 curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.pos(breaks, Dat, curv.fdPar, dbglev = 0, # wtvec = c(rep(1000, 5), rep(100, 5), rep(1, 20)) wtvec = c(rep(1000, 5), seq(100, 10, length = 5), rep(1, 20)) ) # smooth.pos Code XXfd &lt;- BSmooth$Wfdobj # Wfdobj fdobjSmootheval &lt;- eval.posfd(Wfdobj = XXfd, # eval.posfd evalarg = seq(min(sigma_vector), max(sigma_vector), length = 101)) df_plot_smooth &lt;- data.frame( method = rep(methods, each = dim(fdobjSmootheval)[1]), value = c(fdobjSmootheval), sigma = rep(seq(min(sigma_vector), max(sigma_vector), length = 101), length(methods)) ) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) Code # pro testovaci data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(sigma_vector), names_to = &#39;sigma&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), sigma = as.numeric(sigma)) |&gt; # filter(method %in% c(&#39;KNN&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, # &#39;RF_discr&#39;, #&#39;RF_score&#39;, &#39;RF_Bbasis&#39;, # &#39;SVM linear - diskr&#39;,# &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, # # &#39;SVM linear - PCA&#39;,# &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, # &#39;SVM linear - Bbasis&#39;,# &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, # &#39;SVM linear - projection&#39;,# &#39;SVM poly - projection&#39;, # # &#39;SVM rbf - projection&#39;, # &#39;SVM linear - RKHS - radial&#39;#, &#39;SVM poly - RKHS - radial&#39;, # # &#39;SVM rbf - RKHS - radial&#39; # )) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) |&gt; ggplot(aes(x = sigma, y = Err, colour = method, shape = method)) + geom_point(alpha = 0.7, size = 0.6) + theme_bw() + labs(x = &#39;sigma&#39;, y = &#39;Testovaci chybovost&#39;, colour = &#39;Klasifikační metoda&#39;, linetype = &#39;Klasifikační metoda&#39;, shape = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;right&#39;, plot.margin = unit(c(0.1, 0.7, 0.3, 0.3), &quot;cm&quot;), panel.grid.minor.x = element_blank()) + # guides(colour=guide_legend(direction = &#39;vertical&#39;), # linetype=guide_legend(direction = &#39;vertical&#39;), # shape=guide_legend(direction = &#39;vertical&#39;)) + # scale_y_log10() + geom_line(data = df_plot_smooth, aes(x = sigma, y = value, col = method, linetype = method), linewidth = 1.05) + scale_colour_manual(values = rep(c(&#39;deepskyblue2&#39;, &#39;tomato&#39;), c(4, 4)), labels = methods_names) + scale_linetype_manual(values = rep(c(&#39;dotdash&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dotted&#39;), 2), labels = methods_names) + scale_shape_manual(values = rep(c(16, 1, 17, 4, 16, 1, 17, 4)), labels = methods_names) + guides(colour = guide_legend(override.aes = list(size = 1.4, alpha = 0.6)), linetype = guide_legend(override.aes = list(linewidth = 0.7))) Obrázek 6.7: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{x}\\). Code # ggsave(&quot;figures/kap6_sim_03sigma_curvesErr.tex&quot;, device = tikz, width = 8, height = 4) Komentáře ke grafům lze najít v Diplomové práci. My si jen všimněme, že s rostoucí hodnotou \\(\\sigma\\) roste u všech metod i testovací chybovost. Nejlépe si vede funkcionální logistická regrese a projekce na B-splinovou bázi v kombinaci s metodou SVM s lineárním jádrem. "],["simulace3shift.html", "Kapitola 7 Závislost na parametru \\(\\sigma_{shift}\\) 7.1 Simulace funkcionálních dat 7.2 Vyhlazení pozorovaných křivek 7.3 Klasifikace křivek 7.4 Tabulka výsledků 7.5 Simulační studie", " Kapitola 7 Závislost na parametru \\(\\sigma_{shift}\\) V této části se budeme zabývat závislostí výsledků ze sekce 5 na hodnotě \\(\\sigma^2_{shift}\\), která definuje rozptyl normálního rozdělení, ze kterého generujeme posun pro generované křivky. Očekáváme, že s rostoucí hodnotou \\(\\sigma^2_{shift}\\) se budou výsledky jednotlivých metod zhoršovat a tudíž klasifikace nebude tak úspěšná. Přitom předpokládáme, že metody, které využívají funkcionální podstatu dat, budou více úspěšné v porovnání s klasickými metodami při zvětšující se hodnotě \\(\\sigma^2_{shift}\\). V předchozí sekci 6 jsme se podívali na závislost výsledků na hodnotě \\(\\sigma^2\\), tedy na rozptylu normálního rozdělení, ze kterého generujeme náhodné chyby kolem generujících křivek. 7.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro simulaci nejprve: zvolíme vhodné funkce, generujeme body ze zvoleného intervalu, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 6]\\). Nyní vytvoříme funkce pomocí interpolačních polynomů. Nejprve si definujeme body, kterými má procházet naše křivka, a následně jimi proložíme interpolační polynom, který použijeme pro generování křivek pro klasifikaci. Code # definujici body pro tridu 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # definujici body pro tridu 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 1.1: Body, kterými definujeme interpolační polynomy. Pro výpočet interpolačních polynomů využijeme funkci poly.calc() z knihovny polynom. Dále definujeme funkce poly.0() a poly.1(), které budou počítat hodnoty polynomů v daném bodě intervalu. K jejich vytvoření použijeme funkci predict(), na jejíž vstup zadáme příslušný polynom a bod, ve kterám chceme polynom vyhodnotit. Code # vypocet polynomu polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # vykresleni polynomu xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 7.1: Znázornění dvou funkcí na intervalu \\(I = [0, 6]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce (parametr sigma_shift). Tento posun budeme generovat s normálního rozdělení s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.1: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 7.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Opět využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor t, standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) Obrázek 6.1: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 5.2: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Obrázek 5.3: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Tlustou čarou je zakreslen průměr pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Obrázek 1.6: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Tlustou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 7.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 7.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # summary(neighb.model) # shrnuti modelu # plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K # neighb.model$max.prob # maximalni presnost (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 5 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) # CV.results Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 5 s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.3429. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 1.8: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # summary(neighb.model) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost # 1 - presnost Vidíme tedy, že chybovost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 5, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.3571 a na testovacích datech 0.3833. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 7.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 98.72 % variability v datech. První hlavní komponenta potom vysvětluje 98.2 % a druhá 0.52 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 7.2: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (41.43 %), tak i na testovacích datech (40 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 7.3: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 7.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak chybovost klasifikátoru na trénovacích (35.71 %), tak i na testovacích datech (40 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.11: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 7.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 7.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 7.4: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 29, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) # CV.results Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 11 s validační chybovostí 0.072. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) Obrázek 2.8: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 11 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 3.57 %) i testovací chybovost (rovna 8.33 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Třída&#39;) Obrázek 6.3: Závislost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Obrázek 1.14: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 6]\\). Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 7.3.5 Support Vector Machines Definujeme pro další metody data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. 7.3.5.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 6]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 5.71 % pro lineární jádro, 2.14 % pro polynomiální jádro a 3.57 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 16.67 % pro lineární jádro, 16.67 % pro polynomiální jádro a 13.33 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 7.3.5.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních p = 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.6, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.01, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 44.29 % pro lineární jádro, 37.86 % pro polynomiální jádro a 37.14 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 48.33 % pro lineární jádro, 43.33 % pro polynomiální jádro a 40 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 7.5: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 7.3.5.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 500, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 500, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.005, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 4.29 % pro lineární jádro, 4.29 % pro polynomiální jádro a 7.14 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 10 % pro lineární jádro, 11.67 % pro polynomiální jádro a 15 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 7.3.5.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 11 0.05578755 ## poly 11 0.07673993 ## radial 10 0.06959707 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 11 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0558, 11 pro polynomiální jádro s hodnotou spočtenou pomocí 10-násobné CV 0.0767 a 10 pro radiální jádro s hodnotou 0.0696. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 7.6: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 4.29 % pro lineární jádro, 3.57 % pro polynomiální jádro a 5 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 10 % pro lineární jádro, 8.33 % pro polynomiální jádro a 10 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 7.3.5.5 RKHS + SVM 7.3.5.5.0.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 7.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.1000 0.3833 SVM poly - RKHS 0.0286 0.3000 SVM rbf - RKHS 0.0786 0.2667 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 7.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 31 8.4834 0.0655 linear poly 24 8.4834 0.1213 polynomial radial 26 0.7197 0.1218 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 31 a \\(\\gamma={}\\) 8.4834 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0655, \\(d={}\\) 24 a \\(\\gamma={}\\) 8.4834 pro polynomiální jádro s hodnotou spočtenou pomocí 10-násobné CV 0.1213 a \\(d={}\\) 26 a \\(\\gamma={}\\) 0.7197 pro radiální jádro s hodnotou chybovosti 0.1218. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 7.7: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 6.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0214 0.1833 SVM poly - RKHS - radial 0.0000 0.2000 SVM rbf - RKHS - radial 0.0214 0.1667 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 2.14 % pro lineární jádro, 0 % pro polynomiální jádro a 2.14 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 18.33 % pro lineární jádro, 20 % pro polynomiální jádro a 16.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 7.4 Tabulka výsledků Tabulka 7.3: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3571 0.3833 LDA 0.4143 0.4000 QDA 0.3571 0.4000 LR functional 0.0357 0.0833 SVM linear - diskr 0.0571 0.1667 SVM poly - diskr 0.0214 0.1667 SVM rbf - diskr 0.0357 0.1333 SVM linear - PCA 0.4429 0.4833 SVM poly - PCA 0.3786 0.4333 SVM rbf - PCA 0.3714 0.4000 SVM linear - Bbasis 0.0429 0.1000 SVM poly - Bbasis 0.0429 0.1167 SVM rbf - Bbasis 0.0714 0.1500 SVM linear - projection 0.0429 0.1000 SVM poly - projection 0.0357 0.0833 SVM rbf - projection 0.0500 0.1000 SVM linear - RKHS - radial 0.0214 0.1833 SVM poly - RKHS - radial 0.0000 0.2000 SVM rbf - RKHS - radial 0.0214 0.1667 7.5 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 25\\). Nyní zopakujeme celou předchozí část n.sim-krát a hodnoty chybovostí si budeme ukládat to objektu SIMUL_params. Přitom budeme měnit hodnotu parametru \\(\\sigma_{shift}\\) a podíváme se, jak se mění výsledky jednotlivých vybraných klasifikačních metod v závislosti na této hodnotě. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci pro kazdou hodnotu simulacniho parametru n.sim &lt;- 25 methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39; ) # vektor smerodatnych odchylek definujicich posunuti generovanych krivek shift_vector &lt;- seq(0.1, 5, length = 30) # vysledny objekt, do nehoz ukladame vysledky simulaci SIMUL_params &lt;- array(data = NA, dim = c(length(methods), 4, length(shift_vector)), dimnames = list( method = methods, metric = c(&#39;ERRtrain&#39;, &#39;Errtest&#39;, &#39;SDtrain&#39;, &#39;SDtest&#39;), shift = paste0(shift_vector))) for (n_shift in 1:length(shift_vector)) { ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, shift_vector[n_shift]) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, shift_vector[n_shift]) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -4, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 5 # k-fold CV neighbours &lt;- 1:15 #c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max # k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) # normovani dat norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.6, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.01, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 500, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 500, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.005, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 #length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma # Gauss.kernel &lt;- function(x, y, gamma) { # return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) # } # # Kernel.RKHS &lt;- function(x, gamma) { # K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) # for(i in 1:nrow(K)) { # for(j in 1:ncol(K)) { # K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) # } # } # return(K) # } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(10, 40, by = 5) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 5) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) # CV_RESULTS$SVM_RKHS_radial_gamma[sim] &lt;- gamma.opt # CV_RESULTS$SVM_RKHS_radial_d[sim] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, paste0(n_shift, &#39;: &#39;, sim)) } # Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) SIMUL_params[, , n_shift] &lt;- as.matrix(SIMULACE.df) } # ulozime vysledne hodnoty save(SIMUL_params, file = &#39;RData/simulace_parametry_shift_03_rf_cv.RData&#39;) 7.5.1 Grafický výstup Podívejme se na závislost simulovaných výsledků na hodnotě parametru směrodatné odchylky normálního rozdělení pro chyby měření. Nejprve si vykresleme data pro trénovací chybovosti. Code # pro testovaci data SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(shift_vector), names_to = &#39;shift&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), shift = as.numeric(shift)) |&gt; # filter(method %in% c(&#39;KNN&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;,# &#39;SVM linear - diskr&#39;, # &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, # &#39;SVM linear - projection&#39;, &#39;SVM rbf - projection&#39;, # &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, # &#39;SVM rbf - RKHS - radial&#39;)) |&gt; # filter(method %in% c(&#39;KNN&#39;, &#39;LR_functional&#39;, &#39;SVM linear - diskr&#39;, # &#39;RF_Bbasis&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, # &#39;SVM linear - projection&#39;, &#39;SVM linear - RKHS - radial&#39;)) |&gt; ggplot(aes(x = shift, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[shift]), y = expression(widehat(Err)[train]), colour = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;bottom&#39;) Obrázek 7.8: Graf závislosti trénovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{shift}\\), který definuje směrodatnou odchylku pro generování náhodného vertikálního posunu. Následně pro testovací chybovosti. Code # pro testovaci data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(shift_vector), names_to = &#39;shift&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), shift = as.numeric(shift)) |&gt; # filter(method %in% c(&#39;KNN&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;,# &#39;SVM linear - diskr&#39;, # &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, # &#39;SVM linear - projection&#39;, &#39;SVM rbf - projection&#39;, # &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, # &#39;SVM rbf - RKHS - radial&#39;)) |&gt; # filter(method %in% c(&#39;KNN&#39;, &#39;LR_functional&#39;, &#39;SVM linear - diskr&#39;, # &#39;RF_Bbasis&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, # &#39;SVM linear - projection&#39;, &#39;SVM linear - RKHS - radial&#39;)) |&gt; ggplot(aes(x = shift, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[shift]), y = expression(widehat(Err)[test]), colour = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;bottom&#39;) Obrázek 5.13: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{shift}\\), který definuje směrodatnou odchylku pro generování náhodných vertikálních posunutí. Jelikož jsou výsledky zatíženy náhodnými výchylkami a zvýšení počtu opakování n.sim by bylo výpočetně velmi náročné, vyhlaďme nyní křivky závislosti průměrné testovací a trénovací chybovosti. Jelikož chybovost je nezáporná veličina, budeme křivky vyhlazovat s tímto vědomím. Navíc (aby vyhlazené křivky dobře kopírovaly pozorované diskrétní hodnoty) uvažujeme jinou váhu pro malé a velké hodnoty \\(\\sigma\\). Code methods_subset &lt;- c(&#39;KNN&#39;, &#39;LR_functional&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM linear - RKHS - radial&#39;) Dat &lt;- SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; t() breaks &lt;- shift_vector rangeval &lt;- range(breaks) norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(breaks, Dat, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(breaks, Dat, curv.fdPar) # smooth.pos XXfd &lt;- BSmooth$fd # Wfdobj fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, # eval.posfd evalarg = seq(min(breaks), max(breaks), length = 101)) # df_plot_smooth &lt;- data.frame( # method = rep(colnames(fdobjSmootheval), each = dim(fdobjSmootheval)[1]), # value = c(fdobjSmootheval), # sigma = rep(seq(min(sigma_vector), max(sigma_vector), length = 101), length(methods)) # ) |&gt; # mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) ## for positive smoothing # najdeme hodnotu minima # lambda.opt &lt;- 1e-4 # # curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) # BSmooth &lt;- smooth.pos(breaks, Dat, curv.fdPar, dbglev = 0, # wtvec = c(rep(1000, 5), rep(8, 5), rep(1, 20))) # smooth.pos # XXfd &lt;- BSmooth$Wfdobj # Wfdobj # # fdobjSmootheval &lt;- eval.posfd(Wfdobj = XXfd, # eval.posfd # evalarg = seq(min(sigma_vector), max(sigma_vector), # length = 101)) df_plot_smooth &lt;- data.frame( method = rep(methods, each = dim(fdobjSmootheval)[1]), value = c(fdobjSmootheval), shift = rep(seq(min(breaks), max(breaks), length = 101), length(methods)) ) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) Grafy ke zvýšení přehlednosti vykreslíme pouze pro podmnožinu metod. Code methods_names &lt;- c( &#39;$K$ nejbližších sousedů&#39;, &#39;Funkcionální logistická regrese&#39;, &#39;Náhodný les -- bázové koeficienty&#39;, &#39;SVM (linear) -- diskretizace&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (linear) -- bázové koeficienty&#39;, &#39;SVM (linear) -- projekce&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39; ) Code # pro testovaci data SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(shift_vector), names_to = &#39;shift&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), shift = as.numeric(shift)) |&gt; # filter(method %in% c(&#39;KNN&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, # &#39;RF_discr&#39;, #&#39;RF_score&#39;, &#39;RF_Bbasis&#39;, # &#39;SVM linear - diskr&#39;,# &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, # # &#39;SVM linear - PCA&#39;,# &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, # &#39;SVM linear - Bbasis&#39;,# &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, # &#39;SVM linear - projection&#39;,# &#39;SVM poly - projection&#39;, # # &#39;SVM rbf - projection&#39;, # &#39;SVM linear - RKHS - radial&#39;#, &#39;SVM poly - RKHS - radial&#39;, # # &#39;SVM rbf - RKHS - radial&#39; # )) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) |&gt; ggplot(aes(x = shift, y = Err, colour = method, shape = method)) + geom_point(alpha = 0.7, size = 0.6) + theme_bw() + labs(x = &#39;shift&#39;, y = &#39;Trenovaci chybovost&#39;, colour = &#39;Klasifikační metoda&#39;, linetype = &#39;Klasifikační metoda&#39;, shape = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;right&#39;, plot.margin = unit(c(0.1, 0.7, 0.3, 0.3), &quot;cm&quot;), panel.grid.minor.x = element_blank()) + # guides(colour=guide_legend(direction = &#39;vertical&#39;), # linetype=guide_legend(direction = &#39;vertical&#39;), # shape=guide_legend(direction = &#39;vertical&#39;)) + # scale_y_log10() + geom_line(data = df_plot_smooth, aes(x = shift, y = value, col = method, linetype = method), linewidth = 0.95) + scale_colour_manual(values = rep(c(&#39;deepskyblue2&#39;, &#39;tomato&#39;), c(4, 4)), labels = methods_names) + scale_linetype_manual(values = rep(c(&#39;dotdash&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dotted&#39;), 2), labels = methods_names) + scale_shape_manual(values = rep(c(16, 1, 17, 4, 16, 1, 17, 4)), labels = methods_names) + guides(colour = guide_legend(override.aes = list(size = 1.2, alpha = 0.7)), linetype = guide_legend(override.aes = list(linewidth = 0.8))) Obrázek 7.9: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{x}\\). Code # ggsave(&quot;figures/kap6_sim_03sigma_curvesErr.tex&quot;, device = tikz, width = 9, height = 4.5) Nyní provedeme totéž pro testovací chybovosti. Code methods_subset &lt;- c(&#39;KNN&#39;, &#39;LR_functional&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM linear - RKHS - radial&#39;) Dat &lt;- SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; t() breaks &lt;- shift_vector rangeval &lt;- range(breaks) norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) lambda.vect &lt;- 10^seq(from = -3, to = 3, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(breaks, Dat, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(breaks, Dat, curv.fdPar) # smooth.pos XXfd &lt;- BSmooth$fd # Wfdobj fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, # eval.posfd evalarg = seq(min(breaks), max(breaks), length = 101)) # df_plot_smooth &lt;- data.frame( # method = rep(colnames(fdobjSmootheval), each = dim(fdobjSmootheval)[1]), # value = c(fdobjSmootheval), # sigma = rep(seq(min(sigma_vector), max(sigma_vector), length = 101), length(methods)) # ) |&gt; # mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) ## for positive smoothing # najdeme hodnotu minima # lambda.opt &lt;- 1e-4 # # curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) # BSmooth &lt;- smooth.pos(breaks, Dat, curv.fdPar, dbglev = 0, # wtvec = c(rep(1000, 5), rep(8, 5), rep(1, 20))) # smooth.pos # XXfd &lt;- BSmooth$Wfdobj # Wfdobj # # fdobjSmootheval &lt;- eval.posfd(Wfdobj = XXfd, # eval.posfd # evalarg = seq(min(sigma_vector), max(sigma_vector), # length = 101)) df_plot_smooth &lt;- data.frame( method = rep(methods, each = dim(fdobjSmootheval)[1]), value = c(fdobjSmootheval), shift = rep(seq(min(breaks), max(breaks), length = 101), length(methods)) ) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) Code # pro testovaci data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(shift_vector), names_to = &#39;shift&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), shift = as.numeric(shift)) |&gt; # filter(method %in% c(&#39;KNN&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, # &#39;RF_discr&#39;, #&#39;RF_score&#39;, &#39;RF_Bbasis&#39;, # &#39;SVM linear - diskr&#39;,# &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, # # &#39;SVM linear - PCA&#39;,# &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, # &#39;SVM linear - Bbasis&#39;,# &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, # &#39;SVM linear - projection&#39;,# &#39;SVM poly - projection&#39;, # # &#39;SVM rbf - projection&#39;, # &#39;SVM linear - RKHS - radial&#39;#, &#39;SVM poly - RKHS - radial&#39;, # # &#39;SVM rbf - RKHS - radial&#39; # )) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) |&gt; ggplot(aes(x = shift, y = Err, colour = method, shape = method)) + geom_point(alpha = 0.7, size = 0.6) + theme_bw() + labs(x = &#39;sigma&#39;, y = &#39;Testovaci chybovost&#39;, colour = &#39;Klasifikační metoda&#39;, linetype = &#39;Klasifikační metoda&#39;, shape = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;right&#39;, plot.margin = unit(c(0.1, 0.7, 0.3, 0.3), &quot;cm&quot;), panel.grid.minor.x = element_blank()) + # guides(colour=guide_legend(direction = &#39;vertical&#39;), # linetype=guide_legend(direction = &#39;vertical&#39;), # shape=guide_legend(direction = &#39;vertical&#39;)) + # scale_y_log10() + geom_line(data = df_plot_smooth, aes(x = shift, y = value, col = method, linetype = method), linewidth = 1.05) + scale_colour_manual(values = rep(c(&#39;deepskyblue2&#39;, &#39;tomato&#39;), c(4, 4)), labels = methods_names) + scale_linetype_manual(values = rep(c(&#39;dotdash&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dotted&#39;), 2), labels = methods_names) + scale_shape_manual(values = rep(c(16, 1, 17, 4, 16, 1, 17, 4)), labels = methods_names) + guides(colour = guide_legend(override.aes = list(size = 1.4, alpha = 0.6)), linetype = guide_legend(override.aes = list(linewidth = 0.7))) Obrázek 5.14: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(\\sigma_{x}\\). Code # ggsave(&quot;figures/kap6_sim_03shift_curvesErr.tex&quot;, device = tikz, width = 8, height = 4) Komentáře ke grafům lze najít v Diplomové práci. My si jen všimněme, že s rostoucí hodnotou \\(\\sigma_{shift}\\) roste u některých metod i testovací chybovost. Zároveň u metod, které dobře klasifikují data s vertikálním posunem, se při jeho zvětšující se velikosti neprojevuje zhoršení klasifikační síly. "],["simulace3diskr.html", "Kapitola 8 Závislost na diskretizaci 8.1 Simulace funkcionálních dat 8.2 Vyhlazení pozorovaných křivek 8.3 Klasifikace křivek 8.4 Simulační studie", " Kapitola 8 Závislost na diskretizaci V této části se budeme zabývat závislostí výsledků z předchozí sekce 5 na hodnotě \\(p\\), která definuje délku ekvidistantní posloupnosti bodů, které jsou použity k diskretizaci pozorovaných funkcionálních objektů. Bude nás zajímat, jak se mění chybovosti jednotlivých klasifikačních metod při zjemňování dělení intervalu, tedy při zvětšující se dimenzi diskretizovaných vektorů. Naším předpokladem je, že metoda podpůrných vektorů aplikovaná na diskretizovaná data by měla při zvětšujícím se \\(p\\) dávat menší chybovosti, neboť lépe aproximujeme integrály a tedy hodnoty jádrových funkcí funkcionálních analogií. Naopak bychom čekali, že rozhodovací stromy i náhodné lesy budou od jistého počtu \\(p\\) stagnovat. Jelikož se bavíme o diskretizaci intervalu, v této Kapitole se budeme zabývat pouze metodami pracujícími s diskretizovanými daty – tedy metodami: Rozhodovací strom, Náhodný les, metoda SVM. Kromě průměrné testovací chybovosti by nás také zajímala časová náročnost jednotlivých metod při zvětšující se hodnotě \\(p\\), neboť minimální chybovost není v praxi jediné kritérium pro použití dané klasifikační metody. Poměrně důležitou roli hraje právě i výpočetní obtížnost a s ní spojená časová náročnost. Tato kapitola sestává ze dvou částí. V první Sekci 8.3 zvolíme pevný počet \\(p=101\\) diskretizovaných hodnot a podíváme se na jednotlivé metody, jejich chybovost i časovou náročnost. Následně V Sekci 8.4 definujeme posloupnost hodnot \\(p\\) a pro každou několikrát celý proces zopakujeme tak, abychom mohli stanovit pro tuto hodnotu průměrné výsledky. Nakonec si vykreslíme závislost chybovosti a časové náročnosti na hodnotě \\(p\\) počtu bodů. 8.1 Simulace funkcionálních dat Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro simulaci nejprve: zvolíme vhodné funkce, generujeme body ze zvoleného intervalu, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 6]\\). Nyní vytvoříme funkce pomocí interpolačních polynomů. Nejprve si definujeme body, kterými má procházet naše křivka, a následně jimi proložíme interpolační polynom, který použijeme pro generování křivek pro klasifikaci. Code # definujici body pro tridu 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # definujici body pro tridu 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 1.1: Body definující interpolační polynomy. Pro výpočet interpolačních polynomů využijeme funkci poly.calc() z knihovny polynom. Dále definujeme funkce poly.0() a poly.1(), které budou počítat hodnoty polynomů v daném bodě intervalu. K jejich vytvoření použijeme funkci predict(), na jejíž vstup zadáme příslušný polynom a bod, ve kterám chceme polynom vyhodnotit. Code # vypocet polynomu polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # vykresleni polynomu xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 1.2: Znázornění dvou funkcí na intervalu \\(I = [0, 6]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce (parametr sigma_shift). Tento posun budeme generovat s normálního rozdělení s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 1.4: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 8.2 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Opět využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor t, standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) Obrázek 2.2: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.3: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Obrázek 6.1: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Tlustou čarou je zakreslen průměr pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Obrázek 5.2: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Tlustou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 8.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěšnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. 8.3.1 Rozhodovací stromy Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My samozřejmě nyní zvolíme poslední možnost. Nejprve si musíme definovat body z intervalu \\(I = [0, 6]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime p &lt;- 101 t.seq &lt;- seq(0, 6, length = p) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu start_time &lt;- Sys.time() clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) end_time &lt;- Sys.time() # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost klasifikátoru na testovacích datech je tedy 46.67 % a na trénovacích datech 33.57 %. Časovou náročnost spočítáme následovně. Code duration &lt;- end_time - start_time duration |&gt; print() ## Time difference of 0.435359 secs Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 8.1: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 3.4: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code RESULTS &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test, Duration = as.numeric(duration)) 8.3.2 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat diskretizaci intervalu. V tomto případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 6]\\). Code # sestrojeni modelu start_time &lt;- Sys.time() clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = F, nodesize = 1) end_time &lt;- Sys.time() # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 0 % a na testovacích datech 40 %. Časovou náročnost spočítáme následovně. Code duration &lt;- end_time - start_time duration |&gt; print() ## Time difference of 0.343611 secs Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test, Duration = as.numeric(duration)) RESULTS &lt;- rbind(RESULTS, Res) 8.3.3 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci. Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 6]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Nejprve však data znormujeme. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # sestrojeni modelu start_time &lt;- Sys.time() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) end_time &lt;- Sys.time() duration.l &lt;- end_time - start_time start_time &lt;- Sys.time() clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) end_time &lt;- Sys.time() duration.p &lt;- end_time - start_time start_time &lt;- Sys.time() clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) end_time &lt;- Sys.time() duration.r &lt;- end_time - start_time # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 5.71 % pro lineární jádro, 2.14 % pro polynomiální jádro a 3.57 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 16.67 % pro lineární jádro, 16.67 % pro polynomiální jádro a 13.33 % pro radiální jádro. Časovou náročnost spočítáme následovně. Code print(&#39;Linear kernel:&#39;, quote = F) ## [1] Linear kernel: Code duration.l |&gt; print() ## Time difference of 0.02797294 secs Code print(&#39;Polynomial kernel:&#39;, quote = F) ## [1] Polynomial kernel: Code duration.p |&gt; print() ## Time difference of 0.01821589 secs Code print(&#39;Radial kernel:&#39;, quote = F) ## [1] Radial kernel: Code duration.r |&gt; print() ## Time difference of 0.0297451 secs Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r), Duration = c(as.numeric(duration.l), as.numeric(duration.p), as.numeric(duration.r))) RESULTS &lt;- rbind(RESULTS, Res) 8.3.4 Tabulka výsledků Tabulka 8.1: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) Duration Tree - diskr. 0.3357 0.4667 0.4354 RForest - diskr 0.0000 0.4000 0.3436 SVM linear - diskr 0.0571 0.1667 0.0280 SVM poly - diskr 0.0214 0.1667 0.0182 SVM rbf - diskr 0.0357 0.1333 0.0297 8.4 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 50\\). Nyní zopakujeme celou předchozí část n.sim-krát a hodnoty chybovostí si budeme ukládat to objektu SIMUL_params. Přitom budeme měnit hodnotu parametru \\(p\\) a podíváme se, jak se mění výsledky jednotlivých vybraných klasifikačních metod v závislosti na této hodnotě. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci pro kazdou hodnotu simulacniho parametru n.sim &lt;- 50 methods &lt;- c(&#39;RF_discr&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;) # vektor delek p ekvidistantni posloupnosti p_vector &lt;- seq(4, 250, by = 2) # p_vector &lt;- seq(10, 250, by = 10) # vysledny objekt, do nehoz ukladame vysledky simulaci SIMUL_params &lt;- array(data = NA, dim = c(length(methods), 6, length(p_vector)), dimnames = list( method = methods, metric = c(&#39;ERRtrain&#39;, &#39;Errtest&#39;, &#39;SDtrain&#39;, &#39;SDtest&#39;, &#39;Duration&#39;, &#39;SDduration&#39;), sigma = paste0(p_vector))) # CV_res &lt;- data.frame(matrix(NA, ncol = 4, # nrow = length(p_vector), # dimnames = list(paste0(p_vector), # c(&#39;C.l&#39;, &#39;C.p&#39;, &#39;C.r&#39;, &#39;gamma&#39;)))) for (n_p in 1:length(p_vector)) { ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), duration = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))#, # CV = as.data.frame(matrix(NA, ncol = 4, # nrow = n.sim, # dimnames = list(1:n.sim, # c(&#39;C.l&#39;, &#39;C.p&#39;, &#39;C.r&#39;, &#39;gamma&#39;)))) ) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -4, to = 2, length.out = 40) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) Rozhodovací stromy # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = p_vector[n_p]) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu # start_time &lt;- Sys.time() # clf.tree &lt;- train(Y ~ ., data = grid.data, # method = &quot;rpart&quot;, # trControl = trainControl(method = &quot;CV&quot;, number = 10), # metric = &quot;Accuracy&quot;) # end_time &lt;- Sys.time() # duration &lt;- end_time - start_time # # # presnost na trenovacich datech # predictions.train &lt;- predict(clf.tree, newdata = grid.data) # presnost.train &lt;- table(Y.train, predictions.train) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # # presnost na trenovacich datech # predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) # presnost.test &lt;- table(Y.test, predictions.test) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # RESULTS &lt;- data.frame(model = &#39;Tree_discr&#39;, # Err.train = 1 - presnost.train, # Err.test = 1 - presnost.test, # Duration = as.numeric(duration)) ## 2) Náhodné lesy # sestrojeni modelu start_time &lt;- Sys.time() clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) end_time &lt;- Sys.time() duration &lt;- end_time - start_time # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test, Duration = as.numeric(duration)) # RESULTS &lt;- rbind(RESULTS, Res) ## 3) SVM # normovani dat norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() start_time &lt;- Sys.time() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) end_time &lt;- Sys.time() duration.l &lt;- end_time - start_time start_time &lt;- Sys.time() clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) end_time &lt;- Sys.time() duration.p &lt;- end_time - start_time start_time &lt;- Sys.time() clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) end_time &lt;- Sys.time() duration.r &lt;- end_time - start_time # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # # rozdelime trenovaci data na k casti # k_cv &lt;- 5 # folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # # kontrola, ze mame opravdu k = k_cv # while (length(folds) != k_cv) { # folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # } # # # ktere hodnoty gamma chceme uvazovat # gamma.cv &lt;- 10^seq(-4, 3, length = 8) # C.cv &lt;- 10^seq(-3, 5, length = 9) # p.cv &lt;- 3 # coef0 &lt;- 1 # # # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # # prazdna matice, do ktere vlozime jednotlive vysledky # # ve sloupcich budou hodnoty presnosti pro dane # # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds # CV.results &lt;- list( # SVM.l = array(NA, dim = c(length(C.cv), k_cv)), # SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), # SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) # ) # # # nejprve projdeme hodnoty C # for (C in C.cv) { # # projdeme jednotlive folds # for (index_cv in 1:k_cv) { # # definice testovaci a trenovaci casti pro CV # fold &lt;- folds[[index_cv]] # cv_sample &lt;- 1:dim(grid.data)[1] %in% fold # # data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) # data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) # # ## LINEARNI JADRO # # sestrojeni modelu # clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, # type = &#39;C-classification&#39;, # scale = TRUE, # cost = C, # kernel = &#39;linear&#39;) # # # presnost na validacnich datech # predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) # presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # # presnosti vlozime na pozice pro dane C a fold # CV.results$SVM.l[(1:length(C.cv))[C.cv == C], # index_cv] &lt;- presnost.test.l # # ## POLYNOMIALNI JADRO # for (p in p.cv) { # # sestrojeni modelu # clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, # type = &#39;C-classification&#39;, # scale = TRUE, # cost = C, # coef0 = 1, # degree = p, # kernel = &#39;polynomial&#39;) # # # presnost na validacnich datech # predictions.test.p &lt;- predict(clf.SVM.p, # newdata = data.grid.test.cv) # presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # # presnosti vlozime na pozice pro dane C, p a fold # CV.results$SVM.p[(1:length(C.cv))[C.cv == C], # (1:length(p.cv))[p.cv == p], # index_cv] &lt;- presnost.test.p # } # # ## RADIALNI JADRO # for (gamma in gamma.cv) { # # sestrojeni modelu # clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, # type = &#39;C-classification&#39;, # scale = TRUE, # cost = C, # gamma = gamma, # kernel = &#39;radial&#39;) # # # presnost na validacnich datech # predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) # presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # # presnosti vlozime na pozice pro dane C, gamma a fold # CV.results$SVM.r[(1:length(C.cv))[C.cv == C], # (1:length(gamma.cv))[gamma.cv == gamma], # index_cv] &lt;- presnost.test.r # } # } # } # # spocitame prumerne presnosti pro jednotliva C pres folds # ## Linearni jadro # CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) # ## Polynomialni jadro # CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) # ## Radialni jadro # CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) # # C.opt &lt;- c(which.max(CV.results$SVM.l), # which.max(CV.results$SVM.p) %% length(C.cv), # which.max(CV.results$SVM.r) %% length(C.cv)) # C.opt[C.opt == 0] &lt;- length(C.cv) # C.opt &lt;- C.cv[C.opt] # # gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) # gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) # gamma.opt &lt;- gamma.cv[gamma.opt] # # SIMULACE$CV[sim, ] &lt;- c(C.opt, gamma.opt) # # # sestrojeni modelu # start_time &lt;- Sys.time() # clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, # type = &#39;C-classification&#39;, # scale = TRUE, # cost = C.opt[1], # kernel = &#39;linear&#39;) # end_time &lt;- Sys.time() # # duration.l &lt;- end_time - start_time # # start_time &lt;- Sys.time() # clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, # type = &#39;C-classification&#39;, # scale = TRUE, # cost = C.opt[2], # degree = p.opt, # coef0 = 1, # kernel = &#39;polynomial&#39;) # end_time &lt;- Sys.time() # # duration.p &lt;- end_time - start_time # # start_time &lt;- Sys.time() # clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, # type = &#39;C-classification&#39;, # scale = TRUE, # cost = C.opt[3], # gamma = gamma.opt, # kernel = &#39;radial&#39;) # end_time &lt;- Sys.time() # # duration.r &lt;- end_time - start_time # # # presnost na trenovacich datech # predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) # presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) # presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) # presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # # presnost na testovacich datech # predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) # presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) # presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) # presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; # prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r), Duration = c(as.numeric(duration.l), as.numeric(duration.p), as.numeric(duration.r))) RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test SIMULACE$duration[sim, ] &lt;- RESULTS$Duration cat(&#39;\\r&#39;, paste0(n_p, &#39;: &#39;, sim)) } # Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé # klasifikační metody. # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd), Duration = apply(SIMULACE$duration, 2, mean), SD.dur = apply(SIMULACE$duration, 2, sd)) # CV_res[n_p, ] &lt;- apply(SIMULACE$CV, 2, median) SIMUL_params[, , n_p] &lt;- as.matrix(SIMULACE.df) } # ulozime vysledne hodnoty save(SIMUL_params, file = &#39;RData/simulace_params_diskretizace_03_cv.RData&#39;) 8.4.1 Grafický výstup Podívejme se na závislost simulovaných výsledků na hodnotě parametru \\(p\\). Začněme trénovacími chybovostmi. Code methods_names &lt;- c( # &#39;Rozhodovací strom&#39;, &#39;Náhodný les&#39;, &#39;SVM (linear)&#39;, &#39;SVM (poly)&#39;, &#39;SVM (radial)&#39; ) Code # pro trenovaci data SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(p_vector), names_to = &#39;p&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), p = as.numeric(p)) |&gt; filter(method %in% c(&quot;RF_discr&quot;, &quot;SVM linear - diskr&quot;, &quot;SVM poly - diskr&quot;, &quot;SVM rbf - diskr&quot;)) |&gt; ggplot(aes(x = p, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(p), y = expression(widehat(Err)[train]), colour = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;right&#39;) Obrázek 1.12: Graf závislosti trénovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(p\\). Následně pro testovací chybovosti, což je graf, který nás zajímá nejvíce. Code # pro testovaci data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(p_vector), names_to = &#39;p&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), p = as.numeric(p)) |&gt; filter(method %in% c(&quot;RF_discr&quot;, &quot;SVM linear - diskr&quot;, &quot;SVM poly - diskr&quot;, &quot;SVM rbf - diskr&quot;)) |&gt; ggplot(aes(x = p, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(p), y = expression(widehat(Err)[test]), colour = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;right&#39;) Obrázek 1.13: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(p\\). Nakonec si vykresleme graf pro časovou náročnost metod. Code SIMUL_params[, 5, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(p_vector), names_to = &#39;p&#39;, values_to = &#39;Duration&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), p = as.numeric(p)) |&gt; filter(method %in% c(&quot;RF_discr&quot;, &quot;SVM linear - diskr&quot;, &quot;SVM poly - diskr&quot;, &quot;SVM rbf - diskr&quot;)) |&gt; ggplot(aes(x = p, y = Duration, colour = method, linetype = method)) + geom_line() + theme_bw() + labs(x = expression(p), y = &quot;Duration [s]&quot;, colour = &#39;Klasifikační metoda&#39;, linetype = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;right&#39;) + scale_y_log10() + scale_colour_manual(values = c(&#39;deepskyblue2&#39;, &#39;deepskyblue2&#39;, &#39;tomato&#39;, &#39;tomato&#39;, &#39;tomato&#39;)) + scale_linetype_manual(values = c(&#39;solid&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dashed&#39;, &#39;dotted&#39;)) Obrázek 7.4: Graf závislosti časové náročnosti metod pro jednotlivé klasifikační metody na hodnotě parametru \\(p\\). Jelikož jsou výsledky zatíženy náhodnými výchylkami a zvýšení počtu opakování n.sim by bylo výpočetně velmi náročné, vyhlaďme nyní křivky závislosti průměrné testovací chybovosti na počtu \\(p\\). Code Dat &lt;- SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; t() breaks &lt;- p_vector rangeval &lt;- range(breaks) norder &lt;- 4 wtvec &lt;- rep(1, 124) # wtvec &lt;- c(seq(50, 10, length = 4), seq(0.1, 0.1, length = 10), rep(1, 110)) bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) lambda.vect &lt;- 10^seq(from = 3, to = 5, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(breaks, Dat, curv.Fdpar, wtvec = wtvec) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(breaks, Dat, curv.fdPar, wtvec = wtvec) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = seq(min(p_vector), max(p_vector), length = 101)) df_plot_smooth &lt;- data.frame( method = rep(colnames(fdobjSmootheval), each = dim(fdobjSmootheval)[1]), value = c(fdobjSmootheval), p = rep(seq(min(p_vector), max(p_vector), length = 101), length(methods)) ) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) Code # pro testovaci data p1 &lt;- SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(p_vector), names_to = &#39;p&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), p = as.numeric(p)) |&gt; filter(method %in% c(&quot;RF_discr&quot;, &quot;SVM linear - diskr&quot;, &quot;SVM poly - diskr&quot;, &quot;SVM rbf - diskr&quot;)) |&gt; ggplot(aes(x = p, y = Err, colour = method, shape = method)) + geom_point(alpha = 0.7, size = 0.6) + theme_bw() + labs(x = &#39;p&#39;, y = &#39;Testovaci chybovost&#39;, colour = &#39;Klasifikační metoda&#39;, linetype = &#39;Klasifikační metoda&#39;, shape = &#39;Klasifikační metoda&#39;) + theme(legend.position = &#39;none&#39;, plot.margin = unit(c(0.1, 0.1, 0.3, 0.5), &quot;cm&quot;), panel.grid.minor.x = element_blank()) + # scale_y_log10() + geom_line(data = df_plot_smooth, aes(x = p, y = value, col = method, linetype = method), linewidth = 0.95) + scale_colour_manual(values = c(&#39;deepskyblue2&#39;, &#39;deepskyblue2&#39;, #&#39;tomato&#39;, &#39;tomato&#39;, &#39;tomato&#39;), labels = methods_names) + scale_linetype_manual(values = c(&#39;solid&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dashed&#39;#, &#39;dotted&#39; ), labels = methods_names) + scale_shape_manual(values = c(16, 1, 16, 1), labels = methods_names) + guides(colour = guide_legend(override.aes = list(size = 1.2, alpha = 0.7)), linetype = guide_legend(override.aes = list(linewidth = 0.8))) p1 Obrázek 2.7: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(p\\). Code # ggsave(&quot;figures/kap6_sim_03diskr_curvesErr.tex&quot;, device = tikz, width = 4.5, height = 4.5) Přidáme ke grafu časovou náročnost. Code Dat &lt;- SIMUL_params[, 5, ] |&gt; as.data.frame() |&gt; t() breaks &lt;- p_vector rangeval &lt;- range(breaks) norder &lt;- 4 wtvec &lt;- rep(1, 124) # wtvec &lt;- c(seq(50, 10, length = 4), seq(0.1, 0.1, length = 10), rep(1, 110)) bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) lambda.vect &lt;- 10^seq(from = 3, to = 5, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(breaks, Dat, curv.Fdpar, wtvec = wtvec) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(breaks, Dat, curv.fdPar, wtvec = wtvec) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = seq(min(p_vector), max(p_vector), length = 101)) df_plot_smooth_2 &lt;- data.frame( method = rep(colnames(fdobjSmootheval), each = dim(fdobjSmootheval)[1]), value = c(fdobjSmootheval), p = rep(seq(min(p_vector), max(p_vector), length = 101), length(methods)) ) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) Code # pro testovaci data p2 &lt;- SIMUL_params[, 5, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(p_vector), names_to = &#39;p&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), p = as.numeric(p)) |&gt; filter(method %in% c(&quot;RF_discr&quot;, &quot;SVM linear - diskr&quot;, &quot;SVM poly - diskr&quot;, &quot;SVM rbf - diskr&quot;)) |&gt; ggplot(aes(x = p, y = Err, colour = method, shape = method)) + geom_point(alpha = 0.7, size = 0.6) + theme_bw() + labs(x = &#39;p&#39;, y = &#39;Testovaci chybovost&#39;, colour = &#39;Klasifikační metoda&#39;, linetype = &#39;Klasifikační metoda&#39;, shape = &#39;Klasifikační metoda&#39;) + theme(legend.position = c(0.7, 0.16),#&#39;right&#39;, plot.margin = unit(c(0.1, 0.1, 0.3, 0.5), &quot;cm&quot;), panel.grid.minor = element_blank(), legend.background = element_rect(fill=&#39;transparent&#39;), legend.key = element_rect(colour = NA, fill = NA)) + scale_y_log10() + geom_line(data = df_plot_smooth_2, aes(x = p, y = value, col = method, linetype = method), linewidth = 0.95) + scale_colour_manual(values = c(&#39;deepskyblue2&#39;, &#39;deepskyblue2&#39;, #&#39;tomato&#39;, &#39;tomato&#39;, &#39;tomato&#39;), labels = methods_names) + scale_linetype_manual(values = c(&#39;solid&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dashed&#39;#, &#39;dotted&#39; ), labels = methods_names) + scale_shape_manual(values = c(16, 1, 16, 1), labels = methods_names) + guides(colour = guide_legend(override.aes = list(size = 1.2, alpha = 0.7)), linetype = guide_legend(override.aes = list(linewidth = 0.8))) p2 Obrázek 5.8: Graf závislosti testovací chybovosti pro jednotlivé klasifikační metody na hodnotě parametru \\(p\\). Code # ggsave(&quot;figures/kap6_sim_03diskr_curvesTime.tex&quot;, device = tikz, width = 4.5, height = 4.5) Podrobné komentáře k výsledkům této simulační studie lze najít v diplomové práci. "],["simulace4.html", "Kapitola 9 Simulace 4 9.1 Klasifikace na základě první derivace 9.2 Klasifikace na základě druhé derivace", " Kapitola 9 Simulace 4 V této poslední sekci věnující se simulovaným datům se budeme zabývat stejnými daty jako v Kapitole 5 (případně také v kapitolách 6 nebo 7), tedy půjde o generování funkcionálních dat z funkcí vypočtených pomocí interpolačních polynomů. Jelikož jsme v sekci 5 generovali data s náhodným vertikálním posunem s parametrem směrodatné odchylky \\(\\sigma_{shift}\\), mohli bychom se pokusit odstranit tento posun a klasifikovat data po odstranění tohoto posunu. Viděli jsme totiž v sekci 7, že se zvětšující se hodnotou parametru směrodatné odchylky \\(\\sigma_{shift}\\) se úspěšnost, zejména klasických klasifikačních metod, poměrně dramaticky zhoršuje. Naopak klasifikační metody beroucí do úvahy funkcionální podstatu dat se zpravidla i se zvětšující se hodnotou \\(\\sigma_{shift}\\) chovají poměrně stabilně. Jednou z možností k odstranění vertikálního posunutí, kterou využijeme v následující části, je klasifikovat data na základě odhadu první derivace dané vygenerované a vyhlazené křivky, neboť jak známo \\[ \\frac{\\text d}{\\text d t} \\big( x(t) + c \\big) = \\frac{\\text d}{\\text d t} x(t)= x&#39;(t). \\] 9.1 Klasifikace na základě první derivace Nejprve si simulujeme funkce, které budeme následně chtít klasifikovat. Budeme uvažovat pro jednoduchost dvě klasifikační třídy. Pro simulaci nejprve: zvolíme vhodné funkce, generujeme body ze zvoleného intervalu, které obsahují, například gaussovský, šum, takto získané diskrétní body vyhladíme do podoby funkcionálního objektu pomocí nějakého vhodného bázového systému. Tímto postupem získáme funkcionální objekty společně s hodnotou kategoriální proměnné \\(Y\\), která rozlišuje příslušnost do klasifikační třídy. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) library(tikzDevice) set.seed(42) Uvažujme tedy dvě klasifikační třídy, \\(Y \\in \\{0, 1\\}\\), pro každou ze tříd stejný počet n generovaných funkcí. Definujme si nejprve dvě funkce, každá bude pro jednu třídu. Funkce budeme uvažovat na intervalu \\(I = [0, 6]\\). Nyní vytvoříme funkce pomocí interpolačních polynomů. Nejprve si definujeme body, kterými má procházet naše křivka, a následně jimi proložíme interpolační polynom, který použijeme pro generování křivek pro klasifikaci. Code # definujici body pro tridu 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # definujici body pro tridu 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # graf bodu dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 5.1: Body definující oba interpolační polynomy. Pro výpočet interpolačních polynomů využijeme funkci poly.calc() z knihovny polynom. Dále definujeme funkce poly.0() a poly.1(), které budou počítat hodnoty polynomů v daném bodě intervalu. K jejich vytvoření použijeme funkci predict(), na jejíž vstup zadáme příslušný polynom a bod, ve kterám chceme polynom vyhodnotit. Code # vypocet polynomu polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # vykresleni polynomu xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(colour = &#39;Klasifikační\\n třída&#39;) Obrázek 3.2: Znázornění dvou funkcí na intervalu \\(I = [0, 6]\\), ze kterých generujeme pozorování ze tříd 0 a 1. Code # generujici funkce pro Y = 0 a Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Nyní si vytvoříme funkci pro generování náhodných funkcí s přidaným šumem (resp. bodů na předem dané síti) ze zvolené generující funkce. Argument t označuje vektor hodnot, ve kterých chceme dané funkce vyhodnotit, fun značí generující funkci, n počet funkcí a sigma směrodatnou odchylku \\(\\sigma\\) normálního rozdělení \\(\\text{N}(\\mu, \\sigma^2)\\), ze kterého náhodně generujeme gaussovský bílý šum s \\(\\mu = 0\\). Abychom ukázali výhodu použití metod, které pracují s funkcionálními daty, přidáme při generování ke každému simulovanému pozorování navíc i náhodný člen, který bude mít význam vertikálního posunu celé funkce (parametr sigma_shift). Tento posun budeme generovat s normálního rozdělení s parametrem \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Nyní můžeme generovat funkce. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 2.2: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 9.1.1 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Opět využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor t, jelikož uvažujeme první derivaci, volíme norder = 5. Budeme penalizovat třetí derivaci funkcí, neboť nyní požadujeme hladké i první derivace. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 5 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(3) # penalizujeme 3. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) Obrázek 5.2: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 5.3: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Obrázek 1.6: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Tlustou čarou je zakreslen průměr pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Obrázek 5.4: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Tlustou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 9.1.2 Výpočet derivací K výpočtu derivace pro funkcionální objekt využijeme v R funkci deriv.fd() z balíčku fda. Jelikož chceme klasifikovat na základě první derivace, volíme argument Lfdobj = 1. Code XXder &lt;- deriv.fd(XXfd, 1) Nyní si vykresleme prvních několik prvních derivací pro obě klasifikační třídy. Všimněme si z obrázku níže, že se opravdu vertikální posun pomocí derivování opravdu podařilo odstranit. Ztratili jsme tím ale do jisté míry rozdílnost mezi křivkami, protože jak z obrázku vyplývá, křivky derivací pro obě třídy se liší primárně až ke konci intervalu, tedy pro argument v rozmezí přibližně \\([5, 6]\\). Code fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code abs.labs &lt;- paste(&quot;Klasifikační třída:&quot;, c(&quot;$Y = 0$&quot;, &quot;$Y = 1$&quot;)) names(abs.labs) &lt;- c(&#39;0&#39;, &#39;1&#39;) # fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(eval.fd(fdobj = mean.fd(XXder[1:n]), evalarg = t), eval.fd(fdobj = mean.fd(XXder[(n + 1):(2 * n)]), evalarg = t)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, #group = interaction(time, group), colour = group)) + geom_line(aes(group = time), linewidth = 0.05, alpha = 0.5) + theme_bw() + labs(x = &quot;$t$&quot;, # y = &quot;$\\\\frac{\\\\text d}{\\\\text d t} x_i(t)$&quot;, y =&quot;$x_i&#39;(t)$&quot;, colour = &#39;Klasifikační\\n třída&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;1&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;tomato&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;0&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;deepskyblue2&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + geom_line(data = DFmean |&gt; mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))), aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 0.8, linetype = &#39;dashed&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + facet_wrap(~group, labeller = labeller(group = abs.labs)) + scale_y_continuous(expand = c(0.02, 0.02)) + theme(legend.position = &#39;none&#39;, plot.margin = unit(c(0.1, 0.1, 0.3, 0.5), &quot;cm&quot;)) + coord_cartesian(ylim = c(-1.4, 3.5)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Obrázek 8.1: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou plnou čarou je zakreslen průměr pro každou třídu. Code # ggsave(&quot;figures/kap6_sim_04_curves_1der.tex&quot;, device = tikz, width = 8, height = 4) Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;grey3&#39;, linewidth = 0.7, linetype = &#39;dashed&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1.5, 2)) Obrázek 3.4: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou přerušovanou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 9.1.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 9.1.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # summary(neighb.model) # shrnuti modelu # plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K # neighb.model$max.prob # maximalni presnost (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 12 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) # CV.results Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 14 s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.2594. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 1.9: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # summary(neighb.model) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost # 1 - presnost Vidíme tedy, že chybovost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 14, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.3071 a na testovacích datech 0.1833. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 9.1.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 3, které dohromady vysvětlují 93.96 % variability v datech. První hlavní komponenta potom vysvětluje 50.6 % a druhá 33.44 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 1.10: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 3 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (31.43 %), tak i na testovacích datech (23.33 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.11: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak chybovost klasifikátoru na trénovacích (35 %), tak i na testovacích datech (20 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 2.7: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 9.1.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 1.14: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 10, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) # CV.results Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 14 s validační chybovostí 0.0684. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) Obrázek 9.1: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 14 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 5 %) i testovací chybovost (rovna 11.67 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Třída&#39;) Obrázek 1.16: Závislost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Obrázek 2.10: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 6]\\). Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy chybovost klasifikátoru na trénovacích (31.43 %) i na testovacích datech (23.33 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.19: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 9.1.3.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [0, 6]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost klasifikátoru na testovacích datech je tedy 18.33 % a na trénovacích datech 26.43 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 5.11: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 9.2: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 3 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na testovacích datech je tedy 23.33 % a na trénovacích datech 30 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 2.11: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 5.12: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na trénovacích datech je tedy 26.43 % a na testovacích datech 18.33 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 2.12: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 7.7: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 9.1.3.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 6]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 0 % a na testovacích datech 20 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 3 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost na trénovacích datech je tedy 1.43 % a na testovacích datech 30 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost tohoto klasifikátoru na trénovacích datech je 0 % a na testovacích datech 18.33 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 5.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 5.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 5.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 5.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 5.3.7.5. 9.1.3.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 6]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm &lt;- XXder XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Znormovaná data nyní klasifikujeme klasickou metodou SVM, parametry přitom volíme následovně. Pro jeden vygenerovaný datový soubor určíme parametry pomocí CV, tyto parametry pak použijeme i pro další nasimulovaná data. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 25, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.7, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 55, gamma = 0.0005, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 7.86 % pro lineární jádro, 12.86 % pro polynomiální jádro a 14.29 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 10 % pro lineární jádro, 23.33 % pro polynomiální jádro a 23.33 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 3 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.1, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.01, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1, gamma = 0.01, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 31.43 % pro lineární jádro, 31.43 % pro polynomiální jádro a 32.14 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 23.33 % pro lineární jádro, 23.33 % pro polynomiální jádro a 23.33 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 5.14: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 50, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, gamma = 0.001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 7.14 % pro lineární jádro, 20 % pro polynomiální jádro a 12.86 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 6.67 % pro lineární jádro, 21.67 % pro polynomiální jádro a 20 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), # matice diskretnich dat argvals = t.seq, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- t(Projection) # subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 12 0.1559432 ## poly 8 0.1982189 ## radial 8 0.1918956 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 12 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1559, 8 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1982 a 8 pro radiální jádro s hodnotou chybovosti 0.1919. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 6.6: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), # matice diskretnich dat argvals = t.seq, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 13.57 % pro lineární jádro, 13.57 % pro polynomiální jádro a 13.57 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 21.67 % pro lineární jádro, 26.67 % pro polynomiální jádro a 26.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 9.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 9.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 9.1.3.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 9.1.3.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku4 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 9.1.3.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). 9.1.3.7.5.4 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0000 0.4833 SVM poly - RKHS 0.0000 0.4167 SVM rbf - RKHS 0.0214 0.3000 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 3, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 9.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 33 1000.0000 0.1278 linear poly 22 7.1969 0.1965 polynomial radial 12 0.2683 0.1782 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 33 a \\(\\gamma={}\\) 1000 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1278, \\(d={}\\) 22 a \\(\\gamma={}\\) 7.1969 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1965 a \\(d={}\\) 12 a \\(\\gamma={}\\) 0.2683 pro radiální jádro s hodnotou chybovosti 0.1782. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 9.3: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.5: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.1071 0.3000 SVM poly - RKHS - radial 0.1071 0.2500 SVM rbf - RKHS - radial 0.1357 0.3333 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 10.71 % pro lineární jádro, 10.71 % pro polynomiální jádro a 13.57 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 30 % pro lineární jádro, 25 % pro polynomiální jádro a 33.33 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7.5.5 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 1.7: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 21 3 0.1945 linear poly 7 5 0.1715 polynomial radial 8 5 0.2013 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 21 a \\(p={}\\) 3 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1945, \\(d={}\\) 7 a \\(p={}\\) 5 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1715 a \\(d={}\\) 8 a \\(p={}\\) 5 pro radiální jádro s hodnotou chybovosti 0.2013. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 5.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.1714 0.2333 SVM poly - RKHS - poly 0.1500 0.2833 SVM rbf - RKHS - poly 0.1786 0.3000 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 17.14 % pro lineární jádro, 15 % pro polynomiální jádro a 17.86 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 23.33 % pro lineární jádro, 28.33 % pro polynomiální jádro a 30 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.7.5.6 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 5.5: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 9 0.3417 linear poly 17 0.3395 polynomial radial 19 0.3257 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 9 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.3417, \\(d={}\\) 17 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.3395 a \\(d={}\\) 19 pro radiální jádro s hodnotou chybovosti 0.3257. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.3000 0.2833 SVM poly - RKHS - linear 0.2857 0.2333 SVM rbf - RKHS - linear 0.3071 0.2667 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 30 % pro lineární jádro, 28.57 % pro polynomiální jádro a 30.71 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 28.33 % pro lineární jádro, 23.33 % pro polynomiální jádro a 26.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.1.3.8 Tabulka výsledků Tabulka 5.6: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3071 0.1833 LDA 0.3143 0.2333 QDA 0.3500 0.2000 LR functional 0.0500 0.1167 LR score 0.3143 0.2333 Tree - diskr. 0.2643 0.1833 Tree - score 0.3000 0.2333 Tree - Bbasis 0.2643 0.1833 RForest - diskr 0.0000 0.2000 RForest - score 0.0143 0.3000 RForest - Bbasis 0.0000 0.1833 SVM linear - diskr 0.0786 0.1000 SVM poly - diskr 0.1286 0.2333 SVM rbf - diskr 0.1429 0.2333 SVM linear - PCA 0.3143 0.2333 SVM poly - PCA 0.3143 0.2333 SVM rbf - PCA 0.3214 0.2333 SVM linear - Bbasis 0.0714 0.0667 SVM poly - Bbasis 0.2000 0.2167 SVM rbf - Bbasis 0.1286 0.2000 SVM linear - projection 0.1357 0.2167 SVM poly - projection 0.1357 0.2667 SVM rbf - projection 0.1357 0.2667 SVM linear - RKHS - radial 0.1071 0.3000 SVM poly - RKHS - radial 0.1071 0.2500 SVM rbf - RKHS - radial 0.1357 0.3333 SVM linear - RKHS - poly 0.1714 0.2333 SVM poly - RKHS - poly 0.1500 0.2833 SVM rbf - RKHS - poly 0.1786 0.3000 SVM linear - RKHS - linear 0.3000 0.2833 SVM poly - RKHS - linear 0.2857 0.2333 SVM rbf - RKHS - linear 0.3071 0.2667 9.1.4 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 5 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(3) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd # vypocet derivace XXder &lt;- deriv.fd(XXfd, 1) fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- 1:20 #c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # normovani dat norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm &lt;- XXder XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 25, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.7, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 55, gamma = 0.0005, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.1, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.01, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1, gamma = 0.01, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 50, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, gamma = 0.001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 #length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- t(Projection) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 5) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_04_cv.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/simulace_04_res_cv.RData&#39;) 9.1.4.1 Výsledky Tabulka 9.4: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.2288 0.2557 0.0418 0.0633 LDA 0.2295 0.2383 0.0415 0.0645 QDA 0.2234 0.2485 0.0440 0.0620 LR_functional 0.0384 0.1067 0.0273 0.0433 LR_score 0.2284 0.2400 0.0429 0.0635 Tree_discr 0.1823 0.2493 0.0477 0.0701 Tree_score 0.2092 0.2848 0.0461 0.0693 Tree_Bbasis 0.1854 0.2447 0.0542 0.0723 RF_discr 0.0106 0.2323 0.0085 0.0706 RF_score 0.0359 0.2693 0.0179 0.0694 RF_Bbasis 0.0111 0.2315 0.0082 0.0676 SVM linear - diskr 0.0649 0.1060 0.0277 0.0427 SVM poly - diskr 0.1147 0.2187 0.0537 0.0678 SVM rbf - diskr 0.1393 0.1868 0.0481 0.0611 SVM linear - PCA 0.2289 0.2427 0.0422 0.0646 SVM poly - PCA 0.2389 0.2860 0.0478 0.0720 SVM rbf - PCA 0.2292 0.2477 0.0434 0.0660 SVM linear - Bbasis 0.0584 0.0958 0.0271 0.0397 SVM poly - Bbasis 0.1506 0.2205 0.0492 0.0605 SVM rbf - Bbasis 0.1165 0.1747 0.0491 0.0617 SVM linear - projection 0.1192 0.1522 0.0428 0.0564 SVM poly - projection 0.0994 0.2043 0.0527 0.0632 SVM rbf - projection 0.1284 0.1995 0.0508 0.0676 SVM linear - RKHS - radial 0.1069 0.1782 0.0444 0.0581 SVM poly - RKHS - radial 0.0744 0.1973 0.0443 0.0680 SVM rbf - RKHS - radial 0.1091 0.1958 0.0473 0.0644 SVM linear - RKHS - poly 0.1430 0.2552 0.0626 0.0921 SVM poly - RKHS - poly 0.1032 0.2527 0.0747 0.0826 SVM rbf - RKHS - poly 0.1465 0.2355 0.0499 0.0796 SVM linear - RKHS - linear 0.2829 0.3630 0.0868 0.0907 SVM poly - RKHS - linear 0.2543 0.3568 0.0889 0.0855 SVM rbf - RKHS - linear 0.2802 0.3433 0.0768 0.0893 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # nastavime jinak nazvy klasifikacnich metod methods_names &lt;- c( &#39;$K$ nejbližších sousedů&#39;, &#39;Lineární diskriminační analýza&#39;, &#39;Kvadratická diskriminační analýza&#39;, &#39;Funkcionální logistická regrese&#39;, &#39;Logistické regrese s fPCA&#39;, &#39;Rozhodovací strom -- diskretizace&#39;, &#39;Rozhodovací strom -- fPCA&#39;, &#39;Rozhodovací strom -- bázové koeficienty&#39;, &#39;Náhodný les -- diskretizace&#39;, &#39;Náhodný les -- fPCA&#39;, &#39;Náhodný les -- bázové koeficienty&#39;, &#39;SVM (linear) -- diskretizace&#39;, &#39;SVM (poly) -- diskretizace&#39;, &#39;SVM (radial) -- diskretizace&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (poly) -- fPCA&#39;, &#39;SVM (radial) -- fPCA&#39;, &#39;SVM (linear) -- bázové koeficienty&#39;, &#39;SVM (poly) -- bázové koeficienty&#39;, &#39;SVM (radial) -- bázové koeficienty&#39;, &#39;SVM (linear) -- projekce&#39;, &#39;SVM (poly) -- projekce&#39;, &#39;SVM (radial) -- projekce&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39;, &#39;RKHS (radial SVR) $+$ SVM (poly)&#39;, &#39;RKHS (radial SVR) $+$ SVM (radial)&#39;, &#39;RKHS (poly SVR) $+$ SVM (linear)&#39;, &#39;RKHS (poly SVR) $+$ SVM (poly)&#39;, &#39;RKHS (poly SVR) $+$ SVM (radial)&#39;, &#39;RKHS (linear SVR) $+$ SVM (linear)&#39;, &#39;RKHS (linear SVR) $+$ SVM (poly)&#39;, &#39;RKHS (linear SVR) $+$ SVM (radial)&#39; ) # barvy pro boxploty box_col &lt;- c(&#39;#4dd2ff&#39;, &#39;#0099cc&#39;, &#39;#00ace6&#39;, &#39;#00bfff&#39;, &#39;#1ac5ff&#39;, rep(&#39;#33ccff&#39;, 3), rep(&#39;#0086b3&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff6347&#39;, 3), rep(&#39;#ff7961&#39;, 3), rep(&#39;#ff4d2e&#39;, 3), rep(&#39;#fa2600&#39;, 9)) # box_col &lt;- c(&#39;#CA0A0A&#39;, &#39;#fa2600&#39;, &#39;#fa2600&#39;, &#39;#D15804&#39;, # &#39;#D15804&#39;, rep(&#39;#D3006D&#39;, 3), rep(&#39;#BE090F&#39;, 3), c(&quot;#12DEE8&quot;, &quot;#4ECBF3&quot;, &quot;#127DE8&quot;, &quot;#4C3CD3&quot;, &quot;#4E65F3&quot;, &quot;#4E9EF3&quot;, &quot;#081D58&quot;) |&gt; rep(each = 3)) # alpha pro boxploty box_alpha &lt;- c(0.9, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, seq(0.9, 0.6, length = 9)) #- 0.3 Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 9.4: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, # y = &quot;$\\\\widehat{\\\\textnormal{Err}}_{test}$&quot; y = expression(widehat(Err)[test]) ) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 50, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 3, color = &quot;black&quot;, alpha = 0.9) + # scale_x_discrete(labels = methods_names) + # theme(plot.margin = unit(c(0.5, 0.5, 2, 2), &quot;cm&quot;), # panel.grid.minor = element_blank()) + # scale_fill_manual(values = box_col) + # coord_cartesian(ylim = c(0, 0.45)) + # scale_alpha_manual(values = box_alpha) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;gray20&#39;, alpha = 0.8) Obrázek 9.5: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # ggsave(&quot;figures/kap6_sim_04_boxplot_test_1der.tex&quot;, device = tikz, width = 9, height = 7) Chtěli bychom nyní formálně otestovat, zda jsou některé klasifikační metody na základě předchozí simulace na těchto datech lepší než jiné, případně ukázat, že je můžeme považovat za stejně úspěšné. Vzhledem k nesplnění předpokladu normality nemůžeme použít klasický párový t-test. Využijeme jeho neparametrickou alternativu - párový Wilcoxonův test. Musíme si však v tomto případě dávat pozor na interpretaci. Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.9942506 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - Bbasis&#39;], alternative = &#39;g&#39;, paired = T)$p.value ## [1] 0.007509998 Code wilcox.test(SIMULACE$test[, &#39;SVM linear - diskr&#39;], SIMULACE$test[, &#39;SVM linear - Bbasis&#39;], alternative = &#39;g&#39;, paired = T)$p.value ## [1] 0.001004391 Testujeme přitom na adjustované hladině významnosti \\(\\alpha_{adj} = 0.05 / 3 = 0.0167\\). Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 5.7: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 11.0 nharm 3.0 LR_func_n_basis 12.0 SVM_d_Linear 11.5 SVM_d_Poly 11.0 SVM_d_Radial 10.5 SVM_RKHS_radial_gamma1 3.2 SVM_RKHS_radial_gamma2 3.2 SVM_RKHS_radial_gamma3 3.2 SVM_RKHS_radial_d1 20.0 SVM_RKHS_radial_d2 15.0 SVM_RKHS_radial_d3 15.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 4.0 SVM_RKHS_poly_p3 4.0 SVM_RKHS_poly_d1 25.0 SVM_RKHS_poly_d2 25.0 SVM_RKHS_poly_d3 25.0 SVM_RKHS_linear_d1 15.0 SVM_RKHS_linear_d2 15.0 SVM_RKHS_linear_d3 15.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.18: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.19: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.20: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.21: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.22: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. 9.2 Klasifikace na základě druhé derivace V předchozí části jsme uvažovali první derivaci křivek. Nyní zopakujme celý proces na druhých derivacích. V každé ze dvou tříd budeme uvažovat 100 pozorování, tedy n = 100. Code # pocet vygenerovanych pozorovani pro kazdou tridu set.seed(42) n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) Vykreslíme vygenerované (ještě nevyhlazené) funkce barevně v závislosti na třídě (pouze prvních 10 pozorování z každé třídy pro přehlednost). Code n_curves_plot &lt;- 10 # pocet krivek, ktere chceme vykreslit z kazde skupiny DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 9.6: Prvních 10 vygenerovaných pozorování z každé ze dvou klasifikačních tříd. Pozorovaná data nejsou vyhlazená. 9.2.1 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Opět využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor t, jelikož uvažujeme první derivaci, volíme norder = 6. Budeme penalizovat čtvrtou derivaci funkcí, neboť nyní požadujeme hladké i druhé derivace. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # penalizujeme 4. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -4, to = -2, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) Obrázek 9.7: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce a opět znázorníme graficky prvních 10 pozorovaných křivek z každé klasifikační třídy. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Obrázek 9.8: Prvních 10 vyhlazených křivek z každé klasifikační třídy. Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Obrázek 9.9: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Tlustou čarou je zakreslen průměr pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Obrázek 9.10: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Tlustou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 9.2.2 Výpočet derivací K výpočtu derivace pro funkcionální objekt využijeme v R funkci deriv.fd() z balíčku fda. Jelikož chceme klasifikovat na základě 2. derivace, volíme argument Lfdobj = 2. Code XXder &lt;- deriv.fd(XXfd, 2) Nyní si vykresleme prvních několik prvních derivací pro obě klasifikační třídy. Všimněme si z obrázku níže, že se opravdu vertikální posun pomocí derivování opravdu podařilo odstranit. Ztratili jsme tím ale do jisté míry rozdílnost mezi křivkami, protože jak z obrázku vyplývá, křivky derivací pro obě třídy se liší primárně až ke konci intervalu, tedy pro argument v rozmezí přibližně \\([5, 6]\\). Code fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code abs.labs &lt;- paste(&quot;Klasifikační třída:&quot;, c(&quot;$Y = 0$&quot;, &quot;$Y = 1$&quot;)) names(abs.labs) &lt;- c(&#39;0&#39;, &#39;1&#39;) tt &lt;- seq(min(t), max(t), length = 91) fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = tt) DFsmooth &lt;- data.frame( t = rep(tt, 2 * n), time = rep(rep(1:n, each = length(tt)), 2), Smooth = c(fdobjSmootheval), group = factor(rep(c(0, 1), each = n * length(tt))) ) DFmean &lt;- data.frame( t = rep(tt, 2), Mean = c(eval.fd(fdobj = mean.fd(XXder[1:n]), evalarg = tt), eval.fd(fdobj = mean.fd(XXder[(n + 1):(2 * n)]), evalarg = tt)), group = factor(rep(c(0, 1), each = length(tt))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, #group = interaction(time, group), colour = group)) + geom_line(aes(group = time), linewidth = 0.05, alpha = 0.5) + theme_bw() + labs(x = &quot;$t$&quot;, # y = &quot;$\\\\frac{\\\\text d}{\\\\text d t} x_i(t)$&quot;, y =&quot;$x_i&#39;(t)$&quot;, colour = &#39;Klasifikační\\n třída&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;1&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;tomato&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;0&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;deepskyblue2&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + geom_line(data = DFmean |&gt; mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))), aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 0.8, linetype = &#39;dashed&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + facet_wrap(~group, labeller = labeller(group = abs.labs)) + scale_y_continuous(expand = c(0.02, 0.02)) + theme(legend.position = &#39;none&#39;, plot.margin = unit(c(0.1, 0.1, 0.3, 0.5), &quot;cm&quot;)) + coord_cartesian(ylim = c(-20, 20)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Obrázek 9.11: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čarou je zakreslen průměr pro každou třídu. Code # ggsave(&quot;figures/kap6_sim_04_curves_2der.tex&quot;, device = tikz, width = 8, height = 4) Code fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;grey3&#39;, linewidth = 0.7, linetype = &#39;dashed&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-12, 10)) Obrázek 9.12: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čarou je zakreslen průměr pro každou třídu. Přiblížený pohled. 9.2.3 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Abychom mohli jednotlivé klasifikátory porovnat, rozdělíme množinu vygenerovaných pozorování na dvě části v poměru 70:30, a to na trénovací a testovací (validační) část. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 9.2.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # summary(neighb.model) # shrnuti modelu # plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K # neighb.model$max.prob # maximalni presnost (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 12 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) # CV.results Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 12 s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1617. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 9.13: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # summary(neighb.model) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost # 1 - presnost Vidíme tedy, že chybovost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 12, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.15 a na testovacích datech 0.15. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 9.2.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 3, které dohromady vysvětlují 90.88 % variability v datech. První hlavní komponenta potom vysvětluje 45.83 % a druhá 36.91 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 9.14: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 3 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (13.57 %), tak i na testovacích datech (18.33 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 9.15: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak chybovost klasifikátoru na trénovacích (14.29 %), tak i na testovacích datech (15 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 9.16: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 9.2.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze # basis1 &lt;- X.train$basis nbasis.x &lt;- 20 basis1 &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, nbasis = nbasis.x) Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 9.17: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 20, tedy menší hodnota než 30. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, nbasis = nbasis.x) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) # CV.results Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 15 s validační chybovostí 0.1065. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) Obrázek 9.18: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 15 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 5 %) i testovací chybovost (rovna 6.67 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Třída&#39;) Obrázek 9.19: Závislost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Obrázek 9.20: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [0, 6]\\). Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy chybovost klasifikátoru na trénovacích (13.57 %) i na testovacích datech (18.33 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 9.21: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 9.2.3.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [0, 6]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost klasifikátoru na testovacích datech je tedy 15 % a na trénovacích datech 8.57 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.22: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 9.23: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 3 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na testovacích datech je tedy 28.33 % a na trénovacích datech 16.43 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.24: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 9.25: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na trénovacích datech je tedy 10 % a na testovacích datech 11.67 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.26: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 9.27: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 9.2.3.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 6]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 0.71 % a na testovacích datech 13.33 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 3 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost lesu na trénovacích datech je tedy 2.86 % a na testovacích datech 20 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost tohoto klasifikátoru na trénovacích datech je 0.71 % a na testovacích datech 10 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. 9.2.3.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 6]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm &lt;- XXder XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.04, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 3, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 12, gamma = 0.001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 7.86 % pro lineární jádro, 2.86 % pro polynomiální jádro a 7.86 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 6.67 % pro lineární jádro, 15 % pro polynomiální jádro a 8.33 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 3 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.01, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 25, gamma = 0.0001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 13.57 % pro lineární jádro, 12.86 % pro polynomiální jádro a 13.57 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 18.33 % pro lineární jádro, 18.33 % pro polynomiální jádro a 16.67 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 9.28: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 55, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.2, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 110, gamma = 0.001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 2.86 % pro lineární jádro, 5 % pro polynomiální jádro a 3.57 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 8.33 % pro lineární jádro, 10 % pro polynomiální jádro a 6.67 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), # matice diskretnich dat argvals = t.seq, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- t(Projection) # subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 15 0.07711081 ## poly 14 0.10463828 ## radial 15 0.10325549 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 15 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0771, 14 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1046 a 15 pro radiální jádro s hodnotou chybovosti 0.1033. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 9.29: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), # matice diskretnich dat argvals = t.seq, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 6.43 % pro lineární jádro, 3.57 % pro polynomiální jádro a 6.43 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 8.33 % pro lineární jádro, 8.33 % pro polynomiální jádro a 8.33 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 9.2 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 9.2 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 9.2.3.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 9.2.3.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku5 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 9.2.3.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). 9.2.3.7.5.4 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.5: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0000 0.3833 SVM poly - RKHS 0.0000 0.2500 SVM rbf - RKHS 0.0143 0.2333 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 1, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 9.6: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 38 1.3895 0.0782 linear poly 38 1.3895 0.0978 polynomial radial 9 2.2758 0.0792 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 38 a \\(\\gamma={}\\) 1.3895 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0782, \\(d={}\\) 38 a \\(\\gamma={}\\) 1.3895 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0978 a \\(d={}\\) 9 a \\(\\gamma={}\\) 2.2758 pro radiální jádro s hodnotou chybovosti 0.0792. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 9.30: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.7: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.00 0.1833 SVM poly - RKHS - radial 0.00 0.1500 SVM rbf - RKHS - radial 0.05 0.0667 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 0 % pro lineární jádro, 0 % pro polynomiální jádro a 5 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 18.33 % pro lineární jádro, 15 % pro polynomiální jádro a 6.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7.5.5 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 9.8: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 20 2 0.0772 linear poly 39 3 0.0907 polynomial radial 33 3 0.1127 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 20 a \\(p={}\\) 2 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0772, \\(d={}\\) 39 a \\(p={}\\) 3 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0907 a \\(d={}\\) 33 a \\(p={}\\) 3 pro radiální jádro s hodnotou chybovosti 0.1127. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.9: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.0643 0.2333 SVM poly - RKHS - poly 0.0286 0.2000 SVM rbf - RKHS - poly 0.0571 0.1167 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 6.43 % pro lineární jádro, 2.86 % pro polynomiální jádro a 5.71 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 23.33 % pro lineární jádro, 20 % pro polynomiální jádro a 11.67 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.7.5.6 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 9.10: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 17 0.1407 linear poly 27 0.1665 polynomial radial 16 0.1575 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 17 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1407, \\(d={}\\) 27 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1665 a \\(d={}\\) 16 pro radiální jádro s hodnotou chybovosti 0.1575. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 9.11: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.0929 0.2000 SVM poly - RKHS - linear 0.0500 0.1667 SVM rbf - RKHS - linear 0.0929 0.1833 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 9.29 % pro lineární jádro, 5 % pro polynomiální jádro a 9.29 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 20 % pro lineární jádro, 16.67 % pro polynomiální jádro a 18.33 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 9.2.3.8 Tabulka výsledků Tabulka 9.12: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.1500 0.1500 LDA 0.1357 0.1833 QDA 0.1429 0.1500 LR functional 0.0500 0.0667 LR score 0.1357 0.1833 Tree - diskr. 0.0857 0.1500 Tree - score 0.1643 0.2833 Tree - Bbasis 0.1000 0.1167 RForest - diskr 0.0071 0.1333 RForest - score 0.0286 0.2000 RForest - Bbasis 0.0071 0.1000 SVM linear - diskr 0.0786 0.0667 SVM poly - diskr 0.0286 0.1500 SVM rbf - diskr 0.0786 0.0833 SVM linear - PCA 0.1357 0.1833 SVM poly - PCA 0.1286 0.1833 SVM rbf - PCA 0.1357 0.1667 SVM linear - Bbasis 0.0286 0.0833 SVM poly - Bbasis 0.0500 0.1000 SVM rbf - Bbasis 0.0357 0.0667 SVM linear - projection 0.0643 0.0833 SVM poly - projection 0.0357 0.0833 SVM rbf - projection 0.0643 0.0833 SVM linear - RKHS - radial 0.0000 0.1833 SVM poly - RKHS - radial 0.0000 0.1500 SVM rbf - RKHS - radial 0.0500 0.0667 SVM linear - RKHS - poly 0.0643 0.2333 SVM poly - RKHS - poly 0.0286 0.2000 SVM rbf - RKHS - poly 0.0571 0.1167 SVM linear - RKHS - linear 0.0929 0.2000 SVM poly - RKHS - linear 0.0500 0.1667 SVM rbf - RKHS - linear 0.0929 0.1833 9.2.4 Simulační studie V celé předchozí části jsme se zabývali pouze jedním náhodně vygenerovaným souborem funkcí ze dvou klasifikačních tříd, který jsme následně opět náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se vygenerovaná data (a jejich rozdělení na dvě části) mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé vygenerované soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -4.5, to = -1.5, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd # vypocet derivace XXder &lt;- deriv.fd(XXfd, 2) fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- 1:20 #c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze # basis1 &lt;- X.train$basis nbasis.x &lt;- 20 rangeval &lt;- range(tt) norder &lt;- 6 basis1 &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, nbasis = nbasis.x) ### 10-fold cross-validation n.basis.max &lt;- 15 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] # B-spline baze # basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # normovani dat norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm &lt;- XXder XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.04, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 3, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 12, gamma = 0.001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.01, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 25, gamma = 0.0001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 55, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.2, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 110, gamma = 0.001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 # length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- t(Projection) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 3, length = 6) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_04_2der_cv.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/simulace_04_res_2der_cv.RData&#39;) 9.2.4.1 Výsledky Tabulka 9.13: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.1775 0.1895 0.0539 0.0618 LDA 0.1589 0.1610 0.0688 0.0736 QDA 0.1541 0.1665 0.0677 0.0756 LR_functional 0.0518 0.0985 0.0272 0.0431 LR_score 0.1594 0.1640 0.0681 0.0722 Tree_discr 0.1060 0.1443 0.0446 0.0731 Tree_score 0.1690 0.2282 0.0583 0.0812 Tree_Bbasis 0.1031 0.1453 0.0461 0.0765 RF_discr 0.0105 0.1315 0.0077 0.0646 RF_score 0.0311 0.1902 0.0193 0.0867 RF_Bbasis 0.0094 0.1303 0.0071 0.0653 SVM linear - diskr 0.1031 0.1215 0.0473 0.0617 SVM poly - diskr 0.0311 0.1612 0.0249 0.0675 SVM rbf - diskr 0.0962 0.1268 0.0499 0.0666 SVM linear - PCA 0.1656 0.1772 0.0732 0.0918 SVM poly - PCA 0.1696 0.1923 0.0737 0.0941 SVM rbf - PCA 0.2364 0.2755 0.1167 0.1536 SVM linear - Bbasis 0.0396 0.0932 0.0231 0.0386 SVM poly - Bbasis 0.0764 0.1278 0.0437 0.0676 SVM rbf - Bbasis 0.0586 0.1070 0.0309 0.0523 SVM linear - projection 0.0815 0.1062 0.0259 0.0413 SVM poly - projection 0.0547 0.1463 0.0325 0.0598 SVM rbf - projection 0.0821 0.1397 0.0481 0.0656 SVM linear - RKHS - radial 0.0729 0.1227 0.0312 0.0544 SVM poly - RKHS - radial 0.0424 0.1465 0.0297 0.0572 SVM rbf - RKHS - radial 0.0676 0.1428 0.0362 0.0642 SVM linear - RKHS - poly 0.0830 0.1585 0.0388 0.0635 SVM poly - RKHS - poly 0.0465 0.1660 0.0268 0.0650 SVM rbf - RKHS - poly 0.0864 0.1552 0.0335 0.0637 SVM linear - RKHS - linear 0.1194 0.2052 0.0563 0.0636 SVM poly - RKHS - linear 0.0755 0.2047 0.0468 0.0644 SVM rbf - RKHS - linear 0.1177 0.1972 0.0526 0.0655 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 9.31: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, # y = &quot;$\\\\widehat{\\\\textnormal{Err}}_{test}$&quot; y = expression(widehat(Err)[test]) ) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 50, hjust = 1), panel.grid.minor = element_blank()) + geom_jitter(position = position_jitter(0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 3, color = &quot;black&quot;, alpha = 0.9) + # scale_x_discrete(labels = methods_names) + # theme(plot.margin = unit(c(0.5, 0.5, 2, 2), &quot;cm&quot;)) + # scale_fill_manual(values = box_col) + # scale_alpha_manual(values = box_alpha) + # coord_cartesian(ylim = c(0, 0.45)) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;gray20&#39;, alpha = 0.8) Obrázek 9.32: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # ggsave(&quot;figures/kap6_sim_04_boxplot_test_2der.tex&quot;, device = tikz, width = 9, height = 7) Chtěli bychom nyní formálně otestovat, zda jsou některé klasifikační metody na základě předchozí simulace na těchto datech lepší než jiné, případně ukázat, že je můžeme považovat za stejně úspěšné. Vzhledem k nesplnění předpokladu normality nemůžeme použít klasický párový t-test. Využijeme jeho neparametrickou alternativu - párový Wilcoxonův test. Musíme si však v tomto případě dávat pozor na interpretaci. Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - Bbasis&#39;], alternative = &#39;g&#39;, paired = T)$p.value ## [1] 0.1154435 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - projection&#39;], alternative = &#39;l&#39;, paired = T)$p.value ## [1] 0.04565301 Code wilcox.test(SIMULACE$test[, &#39;SVM linear - projection&#39;], SIMULACE$test[, &#39;SVM linear - Bbasis&#39;], alternative = &#39;g&#39;, paired = T)$p.value ## [1] 0.002090352 Testujeme přitom na adjustované hladině významnosti \\(\\alpha_{adj} = 0.05 / 3 = 0.0167\\). Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 9.14: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 8.5 nharm 3.0 LR_func_n_basis 9.0 SVM_d_Linear 10.0 SVM_d_Poly 10.0 SVM_d_Radial 12.0 SVM_RKHS_radial_gamma1 10.0 SVM_RKHS_radial_gamma2 10.0 SVM_RKHS_radial_gamma3 10.0 SVM_RKHS_radial_d1 17.5 SVM_RKHS_radial_d2 20.0 SVM_RKHS_radial_d3 15.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 4.0 SVM_RKHS_poly_p3 4.0 SVM_RKHS_poly_d1 22.5 SVM_RKHS_poly_d2 30.0 SVM_RKHS_poly_d3 30.0 SVM_RKHS_linear_d1 20.0 SVM_RKHS_linear_d2 25.0 SVM_RKHS_linear_d3 25.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.33: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.34: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.35: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.36: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 9.37: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. Porovnejme na závěr nejlepší klasifikátory z každé derivace, tj. lineární SVM aplikovanou na koeficienty ortogonální projekce na systém B-splinových funkcí pro nederivovaná data (chybovost \\(9.55\\,\\%\\)) a aplikovanou na bázové koeficienty pro první (chybovost \\(9.58\\,\\%\\)) i druhou (chybovost \\(9.32\\,\\%\\)) derivaci. Jelikož ve všech třech simulačních studiích nastavujeme generátor pseudonáhodných čísel na stejnou hodnotu, vygenerované diskrétní vektory \\(\\boldsymbol y_i\\) a jejich rozdělení na množiny \\(\\mathcal T_1\\) a \\(\\mathcal T_2\\) jsou u daného opakování z celkového počtu \\(N=100\\) totožné. Proto pro porovnání mediánů testovacích chybovostí nejúspěšnějších metod využijeme opět Wilcoxonův párový test. Code # nejprve si data ze všech simulací načteme load(&#39;RData/simulace_03_cv.RData&#39;, verbose = F) data_0der &lt;- SIMULACE load(&#39;RData/simulace_04_cv.RData&#39;, verbose = F) data_1der &lt;- SIMULACE load(&#39;RData/simulace_04_2der_cv.RData&#39;, verbose = F) data_2der &lt;- SIMULACE Ověříme, že jsme načetli správná data. Code mean(data_0der$test$`SVM linear - projection`) ## [1] 0.0955 Code mean(data_1der$test$`SVM linear - Bbasis`) ## [1] 0.09583333 Code mean(data_2der$test$`SVM linear - Bbasis`) ## [1] 0.09316667 Konečně provedeme formální testy. Code wilcox.test(data_0der$test$`SVM linear - projection`, data_1der$test$`SVM linear - Bbasis`, alternative = &#39;t&#39;, paired = T)#$p.value ## ## Wilcoxon signed rank test with continuity correction ## ## data: data_0der$test$`SVM linear - projection` and data_1der$test$`SVM linear - Bbasis` ## V = 2033.5, p-value = 0.8151 ## alternative hypothesis: true location shift is not equal to 0 Code wilcox.test(data_0der$test$`SVM linear - projection`, data_2der$test$`SVM linear - Bbasis`, alternative = &#39;t&#39;, paired = T)#$p.value ## ## Wilcoxon signed rank test with continuity correction ## ## data: data_0der$test$`SVM linear - projection` and data_2der$test$`SVM linear - Bbasis` ## V = 2325.5, p-value = 0.8672 ## alternative hypothesis: true location shift is not equal to 0 Code wilcox.test(data_1der$test$`SVM linear - Bbasis`, data_2der$test$`SVM linear - Bbasis`, alternative = &#39;t&#39;, paired = T)#$p.value ## ## Wilcoxon signed rank test with continuity correction ## ## data: data_1der$test$`SVM linear - Bbasis` and data_2der$test$`SVM linear - Bbasis` ## V = 2012, p-value = 0.8235 ## alternative hypothesis: true location shift is not equal to 0 Vidíme, že všechny tři \\(p\\)-hodnoty jsou výrazně nad hladinou významnosti 0.05 (případně i po adjustaci), můžeme tedy říci, že klasifikační síla těchto metod je srovnatelná. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["aplikace1.html", "Kapitola 10 Aplikace na reálných datech 1 10.1 Vyhlazení pozorovaných křivek 10.2 Klasifikace křivek 10.3 Tabulka výsledků 10.4 Simulační studie", " Kapitola 10 Aplikace na reálných datech 1 Ve druhé části tohoto dokumentu se budeme zabývat aplikací dříve popsaných metod (pro více podrobností viz například sekci 1) na reálná data. Budeme pracovat s daty growth, která jsou dostupná v knihovně fda. Jedná se o datový soubor obsahující růstové křivky 39 chlapců a 54 děvčat, přičemž subjekty sledujeme v intervalu jednoho až 18. roku života, a to v celkem 31 časových bodech. Naší úlohou bude klasifikovat růstové křivky podle pohlaví, tedy bude nás zajímat predikce pro novou růstovou křivku, zda se jedná o chlapce, či dívku. Nejprve si data načteme a vykreslíme. Code # nacteni dat library(fda) library(ggplot2) library(dplyr) library(tidyr) data &lt;- growth data.gr &lt;- cbind(data.frame(age = data$age), data$hgtf, data$hgtm) n_girls &lt;- 54 n_boys &lt;- 39 Code gender.labs &lt;- c(&quot;dívky&quot;, &quot;chlapci&quot;) names(gender.labs) &lt;- c(&#39;girl&#39;, &#39;boy&#39;) pivot_longer(data.gr, cols = girl01:boy39, names_to = &#39;sample&#39;, values_to = &#39;height&#39;, cols_vary = &#39;slowest&#39;) |&gt; mutate(Individual = as.factor(sample), Gender = factor(rep(rep(c(&#39;girl&#39;, &#39;boy&#39;), c(n_girls, n_boys)), each = length(data.gr$age)), levels = c(&#39;girl&#39;, &#39;boy&#39;))) |&gt; ggplot(aes(x = age, y = height, colour = Gender, group = Individual)) + geom_line() + theme_bw() + facet_wrap(~Gender, labeller = labeller(Gender = gender.labs)) + labs(x = &quot;Věk [v letech]&quot;, y = &quot;Výška [v cm]&quot;, colour = &quot;Pohlaví&quot;) + theme(legend.position = &#39;none&#39;) + scale_color_discrete(labels = c(&quot;dívky&quot;, &quot;chlapci&quot;)) Obrázek 10.1: Růstové křivky zlášť pro dívky a chlapce. Vidíme, že růstové křivky dívek i chlapců začínají přibližně ve srovnatelných hodnotách, počáteční průběh je podobný, liší se zejména po 13. roku, kde u dívek začínají růstové křivky konvergovat ke konstantní funkci, zatímco u chlapců růst pokračuje dále, přičemž se zastavuje kolem 15. roku. Navíc mezi 13. a 15. rokem u chlapců můžeme vidět, jak křivky ještě zrychlí svůj růst předtím, než dojde k ukončení růstu. Čekali bychom, že tyto znaky budou hrát klíčovou roli při klasifikaci podle pohlaví, neboť v nich se dívky a chlapci liší nejvíce. 10.1 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nejedná o periodické křivky na intervalu \\(I = [1, 18]\\), využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor age, standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code t &lt;- data.gr$age rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě pohlaví stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\) máme v případě rozdílné volby pro každou třídu volit. Code # spojeni pozorovani do jedne matice XX &lt;- data.gr[, -1] |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -5, to = -0.5, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.7) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 3.1: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále při základu 10. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code n &lt;- n_girls + n_boys names(gender.labs) &lt;- c(&#39;girls&#39;, &#39;boys&#39;) DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n_girls], 1, mean), n_girls), rep(apply(fdobjSmootheval[ , (n_girls + 1) : n], 1, mean), n_boys)), Gender = factor(rep(c(&#39;girls&#39;, &#39;boys&#39;), c(n_girls, n_boys) * length(t)), levels = c(&#39;girls&#39;, &#39;boys&#39;)) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n_girls], 1, mean), apply(fdobjSmootheval[ , (n_girls + 1) : n], 1, mean)), group = factor(rep(c(&#39;girls&#39;, &#39;boys&#39;), each = length(t)), levels = c(&#39;girls&#39;, &#39;boys&#39;)) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, color = Gender, group = time)) + geom_line(linewidth = 0.25) + theme_bw() + facet_wrap(~Gender, labeller = labeller(Gender = gender.labs)) + labs(x = &quot;Věk [v letech]&quot;, y = &quot;Výška [v cm]&quot;, colour = &quot;Pohlaví&quot;) + theme(legend.position = &#39;none&#39;) + scale_color_discrete(labels = c(&quot;dívky&quot;, &quot;chlapci&quot;)) + geom_line(aes(x = t, y = Mean), colour = &#39;black&#39;, linewidth = 1, linetype = &#39;twodash&#39;) Obrázek 7.1: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle pohlaví. Černou čerchovanou čarou je zakreslen průměr pro každou třídu. 10.2 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les set.seed(42) Rozdělíme data v poměru 30:70 na testovací a trénovací část, abychom mohli stanovit úspěšnost klasifikace jednotlivých metod. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěšnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast set.seed(42) split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro divky a 1 pro chlapce Y &lt;- rep(c(0, 1), c(n_girls, n_boys)) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 33 32 Code table(Y.test) ## Y.test ## 0 1 ## 21 7 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5076923 0.4923077 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.75 0.25 Vidíme, že v obou množinách (testovací a trénovací) jsou chlapci i dívky zastoupeni přibližně ve stejném poměru, přičemž bereme do úvahy, že chlapců je méně než dívek. Konkrétně dívek je přibližně 58.06 \\(\\%\\) a chlapců 41.94 \\(\\%\\). 10.2.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) neighb.model$max.prob # maximalni presnost ## [1] 0.9538462 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 3 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; t() ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## K 1.0000000 2.0000000 3.0000000 4.0000000 5.0000000 6.0000000 7.0000000 ## CV 0.9071429 0.8946429 0.9514286 0.9314286 0.9514286 0.9028571 0.9414286 ## [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## K 8.0000000 9.0000000 10.0000000 11.0000000 12.0000000 13.0000000 14.0000000 ## CV 0.9071429 0.9271429 0.9371429 0.9389286 0.9246429 0.9289286 0.9289286 ## [,15] [,16] [,17] [,18] ## K 15.0000000 16.0000000 17.0000000 18.0000000 ## CV 0.9246429 0.9271429 0.8946429 0.8971429 Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 3 s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0486. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 2.3: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() Vidíme tedy, že chybovost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 3, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.0462 a na testovacích datech 0.0357. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 10.2.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme využít principu filtrace, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p=\\) 2, které dohromady vysvětlují 94.92 \\(\\%\\) variability v datech. První hlavní komponenta potom vysvětluje 81.34 % a druhá 13.58 \\(\\%\\) variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Pohlaví&#39;) + scale_color_discrete(labels = c(&quot;dívky&quot;, &quot;chlapci&quot;)) + theme_bw() Obrázek 1.6: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (1.54 %), tak i na testovacích datech (3.57 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Pohlaví&#39;) + scale_color_discrete(labels = c(&quot;dívky&quot;, &quot;chlapci&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 2.4: Skóre prvních dvou hlavních komponent, barevně odlišené podle pohlaví. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme chybovost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak chybovost klasifikátoru na trénovacích (1.54 %), tak i na testovacích datech (3.57 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Pohlaví&#39;) + scale_color_discrete(labels = c(&quot;dívky&quot;, &quot;chlapci&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.5: Skóre prvních dvou hlavních komponent, barevně odlišené podle pohlaví. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola, avšak se jen (alespoň opticky) velmi málo liší od přímky. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 10.2.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # potrebujeme zvolit bazi pro funkcionalni pozorovani, klasicky volime # stejnou bazi jako je ta pouzita pro vyhlazeni krivek. Tato volba vsak # vede k numericke chybe, zvolime proto bazi s mensim poctem bazovych funkci. # Po vyzkouseni nekolika moznosti se zda, ze 15 funkci je dostatecne mnoho. nbasis.x &lt;- 15 # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 1.9: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 4, tedy výrazně menší hodnota než 30. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 4 s validační chybovostí 0.0688. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) Obrázek 2.6: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 4 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b, maxit = 1000, epsilon = 1e-2) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 3.08 %) i testovací chybovost (rovna 10.71 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_color_discrete(labels = c(&quot;dívky&quot;, &quot;chlapci&quot;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Pohlaví&#39;) Obrázek 10.2: Závislost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(min(t), max(t), length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line() + theme_bw() + labs(x = expression(x[1]), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Obrázek 6.2: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [1, 18]\\). Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro časy \\(t\\) ze začátku intervalu \\([1, 18]\\), zatímco pro pozdějsí časy jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd zejména ke konci intervalu, zatímco z počátku intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy chybovost klasifikátoru na trénovacích (1.54 %) i na testovacích datech (3.57 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 3.7: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 10.2.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [1, 18]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní můžeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační metoda není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost klasifikátoru na testovacích datech je tedy 7.14 % a na trénovacích datech 13.85 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 6.3: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.14: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 2 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na testovacích datech je tedy 10.71 % a na trénovacích datech 9.23 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.1: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.15: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na trénovacích datech je tedy 13.85 % a na testovacích datech 7.14 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.17: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.18: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 10.2.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [1, 18]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 1.54 % a na testovacích datech 3.57 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 1.54 % a na testovacích datech 3.57 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost tohoto klasifikátoru na trénovacích datech je 1.54 % a na testovacích datech 3.57 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 10.2.7 Support Vector Machines Nyní se podívejme na klasifikaci našich pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního a radiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Ukazuje se, že volba \\(\\alpha_0^{default} = 0\\) není vhodná, volíme proto \\(\\alpha_0^{default} = 1\\). V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 10.2.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 10.2.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 10.2.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 10.2.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 10.2.7.5. 10.2.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [1, 18]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 1.54 % pro lineární jádro, 1.54 % pro polynomiální jádro a 1.54 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 3.57 % pro lineární jádro, 3.57 % pro polynomiální jádro a 3.57 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 10.2.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 1.54 % pro lineární jádro, 1.54 % pro polynomiální jádro a 1.54 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 3.57 % pro lineární jádro, 3.57 % pro polynomiální jádro a 3.57 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) # + Obrázek 10.3: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code # geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), # colour = &#39;black&#39;) + # geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), # colour = &#39;black&#39;) Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 10.2.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 1.54 % pro lineární jádro, 1.54 % pro polynomiální jádro a 1.54 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 3.57 % pro lineární jádro, 3.57 % pro polynomiální jádro a 3.57 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 10.2.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 8 0.01428571 ## poly 23 0.04857143 ## radial 23 0.03428571 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 8 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0143, 23 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0486 a 23 pro radiální jádro s hodnotou chybovosti 0.0343. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &#39;Jádro&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 1.21: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 98.46 % pro lineární jádro, 98.46 % pro polynomiální jádro a 98.46 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 96.43 % pro lineární jádro, 96.43 % pro polynomiální jádro a 96.43 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 10.2.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 10.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 10.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 10.2.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 10.2.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku6 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 10.2.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). Code # hodnoty z clanku # A. Muñoz, J. González / Pattern Recognition Letters 31 (2010) 511–516 eps &lt;- 0.01 C &lt;- 1 # 100 dava spatne vysledky 10.2.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Na rozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, cost = C, coef0 = 1, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 10.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0000 0.2500 SVM poly - RKHS 0.0000 0.2143 SVM rbf - RKHS 0.0154 0.2143 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:30 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 2.3: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 21 3.7276 0.0668 linear poly 3 0.0373 0.0611 polynomial radial 3 0.0373 0.0611 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 21 a \\(\\gamma={}\\) 3.7276 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0668, \\(d={}\\) 3 a \\(\\gamma={}\\) 0.0373 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0611 a \\(d={}\\) 3 a \\(\\gamma={}\\) 0.0373 pro radiální jádro s hodnotou chybovosti 0.0611. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 5.14: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.2: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0000 0.2500 SVM poly - RKHS - radial 0.0615 0.0714 SVM rbf - RKHS - radial 0.0615 0.1071 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 0 % pro lineární jádro, 6.15 % pro polynomiální jádro a 6.15 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 25 % pro lineární jádro, 7.14 % pro polynomiální jádro a 10.71 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 10.2.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 2:30 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, coef0 = 1, cost = C, epsilon = eps, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, gamma = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 5.1: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 13 3 0.0577 linear poly 9 2 0.0595 polynomial radial 2 2 0.0863 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 13 a \\(p={}\\) 3 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0577, \\(d={}\\) 9 a \\(p={}\\) 2 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0595 a \\(d={}\\) 2 a \\(p={}\\) 2 pro radiální jádro s hodnotou chybovosti 0.0863. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, cost = C, gamma = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = C, gamma = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 5.2: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.0615 0.1071 SVM poly - RKHS - poly 0.0000 0.0357 SVM rbf - RKHS - poly 0.0769 0.0714 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 6.15 % pro lineární jádro, 0 % pro polynomiální jádro a 7.69 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 10.71 % pro lineární jádro, 3.57 % pro polynomiální jádro a 7.14 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 10.2.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 2:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 9.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 16 0.1087 linear poly 19 0.0736 polynomial radial 9 0.1079 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 16 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1087, \\(d={}\\) 19 pro polynomiální jádro s hodnotou spočtenou pomocí 10-násobné CV 0.0736 a \\(d={}\\) 9 pro radiální jádro s hodnotou 0.1079. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 10.2: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.0615 0.0000 SVM poly - RKHS - linear 0.0000 0.0357 SVM rbf - RKHS - linear 0.0000 0.2500 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 6.15 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 0 % pro lineární jádro, 3.57 % pro polynomiální jádro a 25 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 10.3 Tabulka výsledků Tabulka 10.3: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.0462 0.0357 LDA 0.0154 0.0357 QDA 0.0154 0.0357 LR functional 0.0308 0.1071 LR score 0.0154 0.0357 Tree - diskr. 0.1385 0.0714 Tree - score 0.0923 0.1071 Tree - Bbasis 0.1385 0.0714 RForest - diskr 0.0154 0.0357 RForest - score 0.0154 0.0357 RForest - Bbasis 0.0154 0.0357 SVM linear - diskr 0.0154 0.0357 SVM poly - diskr 0.0154 0.0357 SVM rbf - diskr 0.0154 0.0357 SVM linear - PCA 0.0154 0.0357 SVM poly - PCA 0.0154 0.0357 SVM rbf - PCA 0.0154 0.0357 SVM linear - Bbasis 0.0154 0.0357 SVM poly - Bbasis 0.0154 0.0357 SVM rbf - Bbasis 0.0154 0.0357 SVM linear - projection 0.0154 0.0357 SVM poly - projection 0.0154 0.0357 SVM rbf - projection 0.0154 0.0357 SVM linear - RKHS - radial 0.0000 0.2500 SVM poly - RKHS - radial 0.0615 0.0714 SVM rbf - RKHS - radial 0.0615 0.1071 SVM linear - RKHS - poly 0.0615 0.1071 SVM poly - RKHS - poly 0.0000 0.0357 SVM rbf - RKHS - poly 0.0769 0.0714 SVM linear - RKHS - linear 0.0615 0.0000 SVM poly - RKHS - linear 0.0000 0.0357 SVM rbf - RKHS - linear 0.0000 0.2500 10.4 Simulační studie V celé předchozí části jsme se zabývali souborem funkcí ze dvou klasifikačních tříd, který jsme následně náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se toto rozdělení na dvě části může při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho trénovacího datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různé trénovací a testovací soubory. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), c(n_girls, n_boys)) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze nbasis.x &lt;- 15 basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(1, 18, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS C &lt;- 1 eps &lt;- 0.01 ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 30, by =2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 10, by = 1) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = C, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/aplikace_01.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/aplikace_01_res.RData&#39;) 10.4.1 Výsledky Tabulka 10.4: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.0438 0.0507 0.0221 0.0500 LDA 0.0231 0.0207 0.0091 0.0234 QDA 0.0298 0.0296 0.0151 0.0274 LR_functional 0.0197 0.0921 0.0209 0.0666 LR_score 0.0312 0.0354 0.0154 0.0350 Tree_discr 0.1043 0.1407 0.0194 0.0640 Tree_score 0.0720 0.1318 0.0220 0.0667 Tree_Bbasis 0.1069 0.1443 0.0340 0.0728 RF_discr 0.0195 0.0982 0.0117 0.0582 RF_score 0.0149 0.0746 0.0112 0.0500 RF_Bbasis 0.0195 0.0911 0.0109 0.0528 SVM linear - diskr 0.0177 0.0525 0.0144 0.0401 SVM poly - diskr 0.0222 0.0446 0.0108 0.0346 SVM rbf - diskr 0.0245 0.0461 0.0120 0.0364 SVM linear - PCA 0.0257 0.0286 0.0116 0.0292 SVM poly - PCA 0.0246 0.0357 0.0120 0.0341 SVM rbf - PCA 0.0234 0.0250 0.0104 0.0280 SVM linear - Bbasis 0.0206 0.0400 0.0108 0.0345 SVM poly - Bbasis 0.0214 0.0461 0.0100 0.0356 SVM rbf - Bbasis 0.0228 0.0350 0.0099 0.0292 SVM linear - projection 0.0240 0.0400 0.0116 0.0330 SVM poly - projection 0.0208 0.0554 0.0108 0.0432 SVM rbf - projection 0.0238 0.0436 0.0121 0.0331 SVM linear - RKHS - radial 0.0326 0.1025 0.0203 0.0661 SVM poly - RKHS - radial 0.0263 0.0925 0.0175 0.0533 SVM rbf - RKHS - radial 0.0400 0.0861 0.0168 0.0577 SVM linear - RKHS - poly 0.0578 0.1096 0.0242 0.0641 SVM poly - RKHS - poly 0.0271 0.1321 0.0258 0.0704 SVM rbf - RKHS - poly 0.0374 0.1089 0.0279 0.0545 SVM linear - RKHS - linear 0.0188 0.1214 0.0243 0.0590 SVM poly - RKHS - linear 0.0203 0.1032 0.0123 0.0554 SVM rbf - RKHS - linear 0.0280 0.0793 0.0134 0.0522 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 10.4: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[test])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 10.5: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 5.4: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 4 nharm 2 LR_func_n_basis 6 SVM_d_Linear 6 SVM_d_Poly 7 SVM_d_Radial 6 SVM_RKHS_radial_gamma1 0 SVM_RKHS_radial_gamma2 0 SVM_RKHS_radial_gamma3 0 SVM_RKHS_radial_d1 10 SVM_RKHS_radial_d2 10 SVM_RKHS_radial_d3 10 SVM_RKHS_poly_p1 3 SVM_RKHS_poly_p2 3 SVM_RKHS_poly_p3 2 SVM_RKHS_poly_d1 5 SVM_RKHS_poly_d2 5 SVM_RKHS_poly_d3 4 SVM_RKHS_linear_d1 23 SVM_RKHS_linear_d2 25 SVM_RKHS_linear_d3 25 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 2.18: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 2.19: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.26: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.27: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.28: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["aplikace2.html", "Kapitola 11 Aplikace na reálných datech 2 11.1 Vyhlazení pozorovaných křivek 11.2 Klasifikace křivek 11.3 Tabulka výsledků 11.4 Klasifikace dalších fonémů", " Kapitola 11 Aplikace na reálných datech 2 V této části dokumentu se budeme zabývat aplikací dříve popsaných metod (pro více podrobností viz například Kapitolu 5) na reálná data phoneme, která jsou dostupná například na této adrese. Podrobný popis dat pak můžeme nalézt zde. Jedná se o datový soubor obsahující log-periodogramy (zvukový záznam řeči) celkem 50 mužů, přičemž od každého jedince máme k dispozici několik zvukových záznamů. V datovém souboru rozlišujeme celkem pět různých hlásek – aa, ao, dcl, iy, sh. Naší úlohou bude klasifikovat log-periodogramy do těchto skupin, tedy bude nás zajímat predikce pro nový log-periodogram. Nejprve si data načteme z textového souboru phoneme.txt a upravíme datový soubor tak, aby byl vhodný pro další analýzu. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(tidyverse) set.seed(42) # nacteni dat data &lt;- read.delim2(&#39;phoneme.txt&#39;, header = T, sep = &#39;,&#39;) # zmenime dve promenne na typ factor data &lt;- data |&gt; mutate(g = factor(g), speaker = factor(speaker)) # numericke promenne prevedeme opravdu na numericke data[, 2:257] &lt;- as.numeric(data[, 2:257] |&gt; as.matrix()) Podíváme se na popisné statistiky faktorových proměnných, tedy zejména nás zajímají absolutní a relativní četnosti hlásek v datovém souboru. Code data.frame(phoneme = table(data$g) |&gt; names(), count = as.numeric(table(data$g)), proportion = as.numeric(table(data$g) / sum(table(data$g)))) ## phoneme count proportion ## 1 aa 695 0.1541362 ## 2 ao 1022 0.2266578 ## 3 dcl 757 0.1678864 ## 4 iy 1163 0.2579286 ## 5 sh 872 0.1933910 V posledním sloupci datového souboru phoneme jsou uvedeny názvy jednotlivých záznamů, přičemž kromě dalších obsahují i informaci o tom, zda se jedná o trénovací (označení train) nebo testovací (označení test) pozorování. To má pro nás klíčovou roli, neboť podle tohoto označení rozdělíme záznamy na testovací a trénovací množinu. Code tr_vs_test &lt;- str_split(data$speaker, &#39;\\\\.&#39;) |&gt; unlist() tr_vs_test &lt;- tr_vs_test[seq(1, length(tr_vs_test), by = 4)] data$train &lt;- ifelse(tr_vs_test == &#39;train&#39;, TRUE, FALSE) data.frame(count = tr_vs_test |&gt; factor() |&gt; summary(), proportion = tr_vs_test |&gt; factor() |&gt; summary() / length(tr_vs_test)) ## count proportion ## test 1169 0.2592593 ## train 3340 0.7407407 Vykresleme si pro lepší představu log-periodogramy jednotlivých fonémů. Z obrázku níže můžeme vidět, že nejvíce si podobné co do průběhu jsou log-periodogramy pro fonémy aa a ao. Vybereme si pro klasifikaci tyto dva fonémy a naším cílem bude porovnat metody popsané v předchozích kapitolách z hlediska úspěšnosti klasifikace. Code n_plot &lt;- 5 data[c(1, 2, 3, 5, 9), ] |&gt; pivot_longer(cols = x.1:x.256, names_to = &#39;time&#39;, values_to = &#39;value&#39;) |&gt; mutate(time = rep(1:256, n_plot)) |&gt; ggplot(aes(x = time, y = value, colour = g, group = interaction(speaker, g))) + geom_line() + theme_bw() + facet_grid(~ g) + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) + # scale_colour_manual(values = rep(&#39;deepskyblue2&#39;, 5)) + theme(aspect.ratio = 1, legend.position = &quot;none&quot;, plot.margin = unit(c(0.1, 0.1, 0.1, 0.3), &quot;cm&quot;)) Obrázek 3.1: Log-periodogramy fonémů pro vybrané záznamy. Code # ggsave(&#39;figures/app_02_log_periodogramy.tex&#39;, device = tikz, width = 9, height = 2.5) Vytvořme si ještě vhodné proměnné v R, do kterých uložíme trénovací a testovací záznamy a také informaci o druhu fonému. Code # nastaveni generatoru cisel set.seed(42) # pocet trenovacich a testovacich dat # n_train &lt;- 500 # n_test &lt;- 300 # vybrane fonemy ke klasifikaci phoneme_subset &lt;- c(&#39;aa&#39;, &#39;ao&#39;) # testovaci a trenovaci data data_train &lt;- data |&gt; filter(train) |&gt; filter(g %in% phoneme_subset) data_test &lt;- data |&gt; filter(!train) |&gt; filter(g %in% phoneme_subset) # omezime se pouze na nejakou podmnozinu # data_train &lt;- data_train[sample(1:dim(data_train)[1], n_train), ] # data_test &lt;- data_test[sample(1:dim(data_test)[1], n_test), ] # odstranime sloupce, ktere nenesou informaci o frekvenci a # transponujeme tak, aby ve sloupcich byly jednotlive zaznamy X_train &lt;- data_train[, -c(1, 258, 259, 260)] |&gt; t() X_test &lt;- data_test[, -c(1, 258, 259, 260)] |&gt; t() # prejmenujeme radky a sloupce rownames(X_train) &lt;- 1:256 colnames(X_train) &lt;- paste0(&#39;train&#39;, data_train$row.names) rownames(X_test) &lt;- 1:256 colnames(X_test) &lt;- paste0(&#39;test&#39;, data_test$row.names) # definujeme vektor fonemu y_train &lt;- data_train[, 258] |&gt; factor(levels = phoneme_subset) y_test &lt;- data_test[, 258] |&gt; factor(levels = phoneme_subset) # absolutni cetnosti data.frame(phoneme = phoneme_subset, train = table(y_train) |&gt; as.numeric(), test = table(y_test) |&gt; as.numeric()) ## phoneme train test ## 1 aa 519 176 ## 2 ao 759 263 11.1 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nejedná o periodické křivky, využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor frekvencí (1 až 256), standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = 1, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 1.3: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Ještě znázorněme křivky včetně průměru zvlášť pro každou třídu. Code # library(tikzDevice) n &lt;- dim(XX)[2] y &lt;- c(y_train, y_test) DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Phoneme = rep(y, each = length(t))) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(eval.fd(fdobj = mean.fd(XXfd[y == phoneme_subset[1]]), evalarg = t), eval.fd(fdobj = mean.fd(XXfd[y == phoneme_subset[2]]), evalarg = t)), Phoneme = factor(rep(phoneme_subset, each = length(t)), levels = levels(y)) ) DFsmooth |&gt; filter(time %in% as.character(1:50)) |&gt; ggplot(aes(x = t, y = Smooth, color = Phoneme)) + geom_line(aes(group = time), linewidth = 0.05, alpha = 0.7) + theme_bw() + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Phoneme&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean, group = Phoneme), linewidth = 1.25, linetype = &#39;solid&#39;, colour = &#39;grey2&#39;) + facet_wrap(~Phoneme) + theme(legend.position = &#39;none&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), labels = phoneme_subset) Obrázek 1.5: Vykreslení prvních 50 vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čarou je zakreslen průměr pro každou třídu. Code # ggsave(&quot;figures/kap7_phoneme_curves_mean.tex&quot;, device = tikz, width = 9, height = 4.5) Nakonec této podkapitoly si ještě vykresleme vyhlazené křivky pro vybrané záznamy z grafu pro všechny fonémy výše. Code n_plot &lt;- 5 data[c(1, 2, 3, 5, 9), ] |&gt; pivot_longer(cols = x.1:x.256, names_to = &#39;time&#39;, values_to = &#39;value&#39;) |&gt; mutate(time = rep(1:256, n_plot)) |&gt; filter(g %in% phoneme_subset) |&gt; ggplot(aes(x = time, y = value, colour = g, group = interaction(speaker, g))) + geom_line(alpha = 0.4) + theme_bw() + facet_grid(~ g) + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) + theme(aspect.ratio = 1, legend.position = &quot;none&quot;) + geom_line( data = DFsmooth |&gt; filter(time %in% as.character(c(1, 3))) |&gt; mutate(g = Phoneme), aes(x = t, y = Smooth, color = g, group = time), linewidth = 0.8 ) Obrázek 2.1: Log-periodogramy fonémů pro vybrané záznamy. Code # ggsave(&#39;figures/app_02_log_periodogramy_plus_smooth.pdf&#39;, width = 10, height = 5) 11.2 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Code # rozdeleni na testovaci a trenovaci cast split &lt;- ifelse(substr(XXfd$fdnames$reps, 1, 2) == &#39;tr&#39;, TRUE, FALSE) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(y, split == TRUE) Y.test &lt;- subset(y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## aa ao ## 519 759 Code table(Y.test) ## Y.test ## aa ao ## 176 263 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## aa ao ## 0.4061033 0.5938967 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## aa ao ## 0.4009112 0.5990888 11.2.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme hodnotu \\(k = {5}\\), neboť výpočet je nyní velmi časově náročný. Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) summary(neighb.model) # shrnuti modelu ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.6782274 0.8853755 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 352 167 ## 2 87 672 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 1 2 3 4 5 6 7 8 9 10 11 ## 0.7363 0.7113 0.7574 0.7480 0.7668 0.7746 0.7833 0.7786 0.7879 0.7895 0.7895 ## 12 13 14 15 16 17 18 19 20 21 22 ## 0.7926 0.7934 0.7864 0.7848 0.7903 0.7887 0.7848 0.7911 0.7919 0.7950 0.8013 ## 23 24 25 26 27 28 29 30 31 32 33 ## 0.7981 0.7958 0.7934 0.7919 0.7926 0.7950 0.7950 0.7911 0.7887 0.7903 0.7903 ## 34 35 36 ## 0.7911 0.7934 0.7911 ## ## -Optimal number of neighbors: knn.opt= 22 ## with highest probability of correct classification max.prob= 0.801252 ## ## -Probability of correct classification: 0.8013 Code plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K Code neighb.model$max.prob # maximalni presnost ## [1] 0.801252 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 22 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 5 # k-fold CV neighbours &lt;- seq(2, 35, by = 5) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[(1:length(neighbours))[neighbour == neighbours], index] &lt;- presnost #cat(&#39;\\r&#39;, paste0(index, &#39;: &#39;, neighbour)) } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- neighbours[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 22 s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.202. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 5.4: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) summary(neighb.model) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.6782274 0.8853755 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 352 167 ## 2 87 672 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 22 ## 0.8013 ## ## -Optimal number of neighbors: knn.opt= 22 ## with highest probability of correct classification max.prob= 0.801252 ## ## -Probability of correct classification: 0.8013 Code # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() Vidíme tedy, že chybovost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 22, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.1987 a na testovacích datech 0.2073. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 11.2.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 9, které dohromady vysvětlují 90.26 % variability v datech. První hlavní komponenta potom vysvětluje 44.16 % a druhá 13.79 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() Obrázek 8.1: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 9 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (19.95 %), tak i na testovacích datech (20.96 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pokud p = 9 if(dim(data.PCA.train)[2] == 10) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1], V6 = data.PCA.train$V5[1], V7 = data.PCA.train$V5[1], V8 = data.PCA.train$V5[1], V9 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 5.5: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{\\texttt{aa}, \\texttt{ao}\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak chybovost klasifikátoru na trénovacích (20.19 %), tak i na testovacích datech (20.96 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 7.3: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 11.2.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) - 1 colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze # potrebujeme zvolit bazi pro funkcionalni pozorovani, klasicky volime # stejnou bazi jako je ta pouzita pro vyhlazeni krivek. Tato volba vsak # vede k numericke chybe, zvolime proto bazi s mensim poctem bazovych funkci. # Po vyzkouseni nekolika moznosti se zda, ze 100 funkci je dostatecne mnoho. nbasis.x &lt;- 100 basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 10.2: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 14, tedy výrazně menší hodnota než 30. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() - 1 x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() - 1 dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 20 s validační chybovostí 0.1719. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) Obrázek 5.7: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 20 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train - 1) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 16.2 %) i testovací chybovost (rovna 19.36 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;, linewidth = 0.3) + geom_point(aes(x = linear.predictor, y = response + rnorm(length(response), sd = 0.02), colour = Y), size = 0.5, alpha = 0.75) + geom_line(aes(x = linear.predictor, y = response), colour = &#39;grey2&#39;) + # scale_colour_discrete(labels = phoneme_subset) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Foném&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), labels = phoneme_subset) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = c(0.85, 0.18)) + scale_y_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1)#, # labels = c(&#39;0&#39;, &#39;0.25&#39;, &#39;0.5&#39;, &#39;0.75&#39;, &#39;1&#39;) ) + scale_x_continuous(breaks = c(-6, -3, 0, 3, 6)) Obrázek 1.13: Závislost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Code # library(tikzDevice) # ggsave(&quot;figures/DP_kap3_linearpredictor.tex&quot;, width = 4, height = 3.5, device = tikz) Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(min(tt), max(tt), length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line(colour = &#39;deepskyblue2&#39;, linewidth = 0.8) + theme_bw() + labs(x = expression(t), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Obrázek 7.4: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [1, 256]\\). Code # ggsave(&quot;figures/DP_kap3_betahat.tex&quot;, width = 4, height = 3.5, # device = tikz) Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro frekvence \\(t\\) z prostředka a konce intervalu \\([1, 256]\\), zatímco pro počáteční frekvence jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd zejména na začátku intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy chybovost klasifikátoru na trénovacích (20.03 %) i na testovacích datech (21.18 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.14: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 11.2.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [1, 256]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(1, 256, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační metoda není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost klasifikátoru na testovacích datech je tedy 21.41 % a na trénovacích datech 17.76 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.15: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.16: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 9 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na testovacích datech je tedy 31.21 % a na trénovacích datech 25.27 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.17: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.18: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na trénovacích datech je tedy 15.88 % a na testovacích datech 22.1 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.19: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 1.20: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 11.2.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [1, 256]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 0.23 % a na testovacích datech 19.82 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 9 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 0.31 % a na testovacích datech 22.32 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost tohoto klasifikátoru na trénovacích datech je 0.08 % a na testovacích datech 18.91 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.2.7 Support Vector Machines Nyní se podívejme na klasifikaci našich pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního a radiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách, avšak lepší volbou se zdá být \\(\\alpha_0^{default} = 1\\). V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 11.2.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 11.2.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 11.2.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 11.2.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 11.2.7.5. 11.2.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [1, 256]\\)), přičemž budeme uvažovat všechny tři výše zmíněné jádrové funkce. Nejprve funkcionální pozorování normujeme, čímž dosáhneme ještě o něco menší testovací chybovosti. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(BSmooth$fd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast split_norm &lt;- ifelse(substr(XXfd_norm$fdnames$reps, 1, 2) == &#39;tr&#39;, TRUE, FALSE) X.train_norm &lt;- subset(XXfd_norm, split_norm == TRUE) X.test_norm &lt;- subset(XXfd_norm, split_norm == FALSE) Y.train_norm &lt;- subset(y, split_norm == TRUE) Y.test_norm &lt;- subset(y, split_norm == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Nyní se pokusme, na rozdíl od postupu v předchozích kapitolách, hyperparametry klasifikátorů odhadnout z dat pomocí 10-násobné cross-validace. Vzhledem k tomu, že každé jádro má ve své definici jiné hyperparametry, budeme ke každé jádrové funkci přistupovat zvlášť. Nicméně hyperparametr \\(C\\) vystupuje u všech jádrových funkcí, přičemž ale připouštíme, že se může jeho optimální hodnota mezi jádry lišit. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split_norm), k = k_cv, time = 1) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 10) C.cv &lt;- 10^seq(-3, 3, length = 10) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.1, pro polynomiální jádro je \\(C\\) rovno 0.1 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 2.1544 a pro \\(\\gamma\\) je to 0.001. Validační chybovosti jsou postupně 0.1651206 pro lineární, 0.1846703 pro polynomiální a 0.1729331 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 15.6495 % pro lineární jádro, 13.928 % pro polynomiální jádro a 16.4319 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 20.0456 % pro lineární jádro, 18.9066 % pro polynomiální jádro a 18.9066 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 11.2.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 9 hlavních komponent. Nyní se pokusme, na rozdíl od postupu v předchozích kapitolách, hyperparametry klasifikátorů odhadnout z dat pomocí 10-násobné cross-validace. Vzhledem k tomu, že každé jádro má ve své definici jiné hyperparametry, budeme ke každé jádrové funkci přistupovat zvlášť. Nicméně hyperparametr \\(C\\) vystupuje u všech jádrových funkcí, přičemž ale připouštíme, že se může jeho optimální hodnota mezi jádry lišit. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 10) C.cv &lt;- 10^seq(-3, 3, length = 10) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.0046, pro polynomiální jádro je \\(C\\) rovno 0.1 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 1000 a pro \\(\\gamma\\) je to 0.001. Validační chybovosti jsou postupně 0.2034633 pro lineární, 0.2065822 pro polynomiální a 0.1987635 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 19.95 % pro lineární jádro, 17.14 % pro polynomiální jádro a 18.39 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 20.7289 % pro lineární jádro, 18.6788 % pro polynomiální jádro a 19.8178 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 11.1: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 11.2.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 10) C.cv &lt;- 10^seq(-3, 3, length = 10) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.1, pro polynomiální jádro je \\(C\\) rovno 0.1 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 0.4642 a pro \\(\\gamma\\) je to 0.001. Validační chybovosti jsou postupně 0.1627891 pro lineární, 0.1886011 pro polynomiální a 0.1831139 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 15.26 % pro lineární jádro, 15.65 % pro polynomiální jádro a 17.21 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 19.59 % pro lineární jádro, 19.1344 % pro polynomiální jádro a 19.1344 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 11.2.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 30. Code set.seed(42) k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- 32 # norder n_basis_max &lt;- 50 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # hodnoty hyperparametru pro jadrove funkce C &lt;- 1 p &lt;- 3 gamma &lt;- 0.001 # 1/ncol(data.projection.train) # # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 40 0.1643270 ## poly 43 0.1925320 ## radial 38 0.1690453 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 40 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1643, 43 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1925 a 38 pro radiální jádro s hodnotou chybovosti 0.169. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &#39;Jádro&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 2.13: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, degree = p, coef0 = coef0, gamma = gamma, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 15.1 % pro lineární jádro, 16.12 % pro polynomiální jádro a 16.12 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 19.59 % pro lineární jádro, 18.45 % pro polynomiální jádro a 17.54 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 11.2.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 11.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 11.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 11.2.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 11.2.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku7 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 11.2.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). Code # defaultni hodnoty v R C &lt;- 1 eps &lt;- 0.1 11.2.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.001 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 11.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.2872 0.3713 SVM poly - RKHS 0.0344 0.3622 SVM rbf - RKHS 0.1354 0.3417 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 25, by = 4) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 1, length = 7) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), (1:length(dimensions))[dimensions == d.RKHS], index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), (1:length(dimensions))[dimensions == d.RKHS], index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), (1:length(dimensions))[dimensions == d.RKHS], index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 5.1: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 15 0.3162 0.1737 linear poly 19 3.1623 0.1924 polynomial radial 23 10.0000 0.1862 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 15 a \\(\\gamma={}\\) 0.3162 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1737, \\(d={}\\) 19 a \\(\\gamma={}\\) 3.1623 pro polynomiální jádro s hodnotou spočtenou pomocí 10-násobné CV 0.1924 a \\(d={}\\) 23 a \\(\\gamma={}\\) 10 pro radiální jádro s hodnotou 0.1862. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 11.2: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 2.5: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.1714 0.2187 SVM poly - RKHS - radial 0.0736 0.2255 SVM rbf - RKHS - radial 0.1150 0.2073 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 17.14 % pro lineární jádro, 7.36 % pro polynomiální jádro a 11.5 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 21.87 % pro lineární jádro, 22.55 % pro polynomiální jádro a 20.73 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 11.2.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 5) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, cost = C, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), (1:length(dimensions))[dimensions == d.RKHS], index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), (1:length(dimensions))[dimensions == d.RKHS], index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), (1:length(dimensions))[dimensions == d.RKHS], index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 2.6: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 33 2 0.1674 linear poly 13 3 0.1956 polynomial radial 28 2 0.1792 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 33 a \\(p={}\\) 2 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1674, \\(d={}\\) 13 a \\(p={}\\) 3 pro polynomiální jádro s hodnotou spočtenou pomocí 10-násobné CV 0.1956 a \\(d={}\\) 28 a \\(p={}\\) 2 pro radiální jádro s hodnotou 0.1792. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, cost = C, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.5: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.1612 0.2323 SVM poly - RKHS - poly 0.1432 0.2392 SVM rbf - RKHS - poly 0.1236 0.2164 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 16.12 % pro lineární jádro, 14.32 % pro polynomiální jádro a 12.36 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 23.23 % pro lineární jádro, 23.92 % pro polynomiální jádro a 21.64 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 11.2.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 1.7: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 39 0.1870 linear poly 40 0.2066 polynomial radial 37 0.1941 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 39 pro lineární jádro s hodnotou chybovosti spočtené pomocí 10-násobné CV 0.187, \\(d={}\\) 40 pro polynomiální jádro s hodnotou chybovosti spočtené pomocí 10-násobné CV 0.2066 a \\(d={}\\) 37 pro radiální jádro s hodnotou 0.1941. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 5.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.1784 0.2255 SVM poly - RKHS - linear 0.0696 0.2346 SVM rbf - RKHS - linear 0.1291 0.2255 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 17.84 % pro lineární jádro, 6.96 % pro polynomiální jádro a 12.91 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 22.55 % pro lineární jádro, 23.46 % pro polynomiální jádro a 22.55 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 11.3 Tabulka výsledků Nakonec se ještě podívejme na celkové výsledky na trénovacích a zejména na testovacích datech. Vidíme z tabulky níže, že nejlepší metodou pro klasifikaci je projekce na B-splinovou bázi v kombinaci s metodou SVM s radiálním jádrem. Dobře si také vede klasifikátor Random Forest na diskretizovaných datech a také funkcionální logistická regrese. Také SVM aplikované na diskretizovaná data se chová velmi úspěšně, a to bez ohledu na zvolenou jádrovou funkci. Naopak klasifikátor kombinující projekci na RKHS a SVM pro klasifikaci není na tato data nejlepší volbou, nicméně při volbě radiálního jádra jak pro projekci, tak pro klasifikaci vychází stejné výsledky jako pro LDA a QDA na skórech hlavních komponent. Doplňme, že v tomto případě využijí analýzy hlavních komponent pro klasifikaci není optimální volbou, neboť k vysvětlení 90 % variability v datech je potřebných alespoň 9 hlavních komponent, což je již poměrně hodně. Tabulka 1.8: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.1987 0.2073 LDA 0.1995 0.2096 QDA 0.2019 0.2096 LR functional 0.1620 0.1936 LR score 0.2003 0.2118 Tree - diskr. 0.1776 0.2141 Tree - score 0.2527 0.3121 Tree - Bbasis 0.1588 0.2210 RForest - diskr 0.0023 0.1982 RForest - score 0.0031 0.2232 RForest - Bbasis 0.0008 0.1891 SVM linear - diskr 0.1565 0.2005 SVM poly - diskr 0.1393 0.1891 SVM rbf - diskr 0.1643 0.1891 SVM linear - PCA 0.1995 0.2073 SVM poly - PCA 0.1714 0.1868 SVM rbf - PCA 0.1839 0.1982 SVM linear - Bbasis 0.1526 0.1959 SVM poly - Bbasis 0.1565 0.1913 SVM rbf - Bbasis 0.1721 0.1913 SVM linear - projection 0.1510 0.1959 SVM poly - projection 0.1612 0.1845 SVM rbf - projection 0.1612 0.1754 SVM linear - RKHS - radial 0.1714 0.2187 SVM poly - RKHS - radial 0.0736 0.2255 SVM rbf - RKHS - radial 0.1150 0.2073 SVM linear - RKHS - poly 0.1612 0.2323 SVM poly - RKHS - poly 0.1432 0.2392 SVM rbf - RKHS - poly 0.1236 0.2164 SVM linear - RKHS - linear 0.1784 0.2255 SVM poly - RKHS - linear 0.0696 0.2346 SVM rbf - RKHS - linear 0.1291 0.2255 Doplňme, že jelikož nyní máme datový soubor již rozdělený na testovací a trénovací data, nemá velký význam provádět celý postup několikrát, tak jako jsme to dělali například v předchozí Kapitole 10.4. Proto se spokojíme s výsledky uvedenými v tabulce výše. 11.4 Klasifikace dalších fonémů Doposud jsme se zabývali klasifikací dvou fonémů, a to aa a ao, neboť ty si byly nejpodobnější a tudíž výsledky klasifikace nejzajímavější. Podívejme se nyní na jinou možnou volbu dvou fonémů a zjistěme, jak si jednotlivé klasifikační metody povedou mezi sebou. Vzhledem k výpočetní náročnosti některých metod budeme uvažovat pouze vybrané metody. Vyjdeme z předchozí situace a zvolíme ty metody, které pro předchozí volbu fonémů dávaly nejlepší výsledky. 11.4.1 ao proti dcl Code # nastaveni generatoru cisel set.seed(42) # pocet trenovacich a testovacich dat # n_train &lt;- 500 # n_test &lt;- 300 # vybrane fonemy ke klasifikaci phoneme_subset &lt;- c(&#39;dcl&#39;, &#39;ao&#39;) # testovaci a trenovaci data data_train &lt;- data |&gt; filter(train) |&gt; filter(g %in% phoneme_subset) data_test &lt;- data |&gt; filter(!train) |&gt; filter(g %in% phoneme_subset) # omezime se pouze na nejakou podmnozinu # data_train &lt;- data_train[sample(1:dim(data_train)[1], n_train), ] # data_test &lt;- data_test[sample(1:dim(data_test)[1], n_test), ] # odstranime sloupce, ktere nenesou informaci o frekvenci a # transponujeme tak, aby ve sloupcich byly jednotlive zaznamy X_train &lt;- data_train[, -c(1, 258, 259, 260)] |&gt; t() X_test &lt;- data_test[, -c(1, 258, 259, 260)] |&gt; t() # prejmenujeme radky a sloupce rownames(X_train) &lt;- 1:256 colnames(X_train) &lt;- paste0(&#39;train&#39;, data_train$row.names) rownames(X_test) &lt;- 1:256 colnames(X_test) &lt;- paste0(&#39;test&#39;, data_test$row.names) # definujeme vektor fonemu y_train &lt;- data_train[, 258] |&gt; factor(levels = phoneme_subset) y_test &lt;- data_test[, 258] |&gt; factor(levels = phoneme_subset) # absolutni cetnosti data.frame(phoneme = phoneme_subset, train = table(y_train) |&gt; as.numeric(), test = table(y_test) |&gt; as.numeric()) ## phoneme train test ## 1 dcl 562 195 ## 2 ao 759 263 11.4.1.0.1 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nejedná o periodické křivky, využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor frekvencí (1 až 256 Hz), standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -2, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 2.19: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Ještě znázorněme křivky včetně průměru zvlášť pro každou třídu. Code n &lt;- dim(XX)[2] y &lt;- c(y_train, y_test) DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Phoneme = rep(y, each = length(t))) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , y == phoneme_subset[1]], 1, mean), apply(fdobjSmootheval[ , y == phoneme_subset[2]], 1, mean)), Phoneme = factor(rep(phoneme_subset, each = length(t)), levels = levels(y)) ) DFsmooth |&gt; filter(time %in% as.character(1:100)) |&gt; ggplot(aes(x = t, y = Smooth, color = Phoneme)) + geom_line(aes(group = time), linewidth = 0.2) + theme_bw() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;log-periodogram&#39;, colour = &#39;Phoneme&#39;) + scale_colour_discrete(labels = phoneme_subset) + geom_line(data = DFmean, aes(x = t, y = Mean, group = Phoneme), linewidth = 1, linetype = &#39;solid&#39;, colour = &#39;grey2&#39;) + facet_wrap(~Phoneme) + theme(legend.position = &#39;none&#39;) Obrázek 1.26: Vykreslení prvních 100 vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čarou je zakreslen průměr pro každou třídu. Nakonec této podkapitoly si ještě vykresleme vyhlazené křivky pro vybrané záznamy z grafu pro všechny fonémy výše. Code n_plot &lt;- 5 data[c(1, 2, 3, 5, 9), ] |&gt; pivot_longer(cols = x.1:x.256, names_to = &#39;time&#39;, values_to = &#39;value&#39;) |&gt; mutate(time = rep(1:256, n_plot)) |&gt; filter(g %in% phoneme_subset) |&gt; ggplot(aes(x = time, y = value, colour = g, group = interaction(speaker, g))) + geom_line(alpha = 0.4) + theme_bw() + facet_grid(~ g) + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) + theme(aspect.ratio = 1, legend.position = &quot;none&quot;) + geom_line( data = DFsmooth |&gt; filter(time %in% as.character(c(1, 3))) |&gt; mutate(g = Phoneme), aes(x = t, y = Smooth, color = g, group = time), linewidth = 0.8 ) Obrázek 11.3: Log-periodogramy fonémů pro vybrané záznamy. Code # ggsave(&#39;figures/app_02_log_periodogramy_plus_smooth.pdf&#39;, # width = 10, height = 5) 11.4.1.0.2 Klasifikace křivek Code # rozdeleni na testovaci a trenovaci cast split &lt;- ifelse(substr(XXfd$fdnames$reps, 1, 2) == &#39;tr&#39;, TRUE, FALSE) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(y, split == TRUE) Y.test &lt;- subset(y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## dcl ao ## 562 759 Code table(Y.test) ## Y.test ## dcl ao ## 195 263 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## dcl ao ## 0.4254353 0.5745647 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## dcl ao ## 0.4257642 0.5742358 11.4.1.0.2.1 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 4, které dohromady vysvětlují 90.93 % variability v datech. První hlavní komponenta potom vysvětluje 80.98 % a druhá 5.06 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() Obrázek 5.16: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 4 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (0 %), tak i na testovacích datech (0.22 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pokud p = 9 if(dim(data.PCA.train)[2] == 10) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1], V6 = data.PCA.train$V5[1], V7 = data.PCA.train$V5[1], V8 = data.PCA.train$V5[1], V9 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Foném&#39;) + scale_colour_discrete(labels = phoneme_subset) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.30: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code RESULTS &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) 11.4.1.0.2.2 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 11.4.1.0.2.2.1 Funkcionální logistická regrese Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) - 1 colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze nbasis.x &lt;- 100 basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 11.4: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 9, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() - 1 x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() - 1 dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 10 s validační chybovostí 0. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) Obrázek 5.18: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 10 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train - 1) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 0 %) i testovací chybovost (rovna 0 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = .5) + scale_colour_discrete(labels = phoneme_subset) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Foném&#39;) Obrázek 5.19: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(min(tt), max(tt), length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line() + theme_bw() + labs(x = expression(t), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Obrázek 11.5: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [1, 256]\\). Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro frekvence \\(t\\) z prostředka a konce intervalu \\([1, 256]\\), zatímco pro počáteční frekvence jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd zejména na začátku intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 11.4.1.0.2.3 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. 11.4.1.0.2.3.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [1, 256]\\)), přičemž budeme uvažovat všechny tři výše zmíněné jádrové funkce. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 0 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 0 % pro lineární jádro, 0.22 % pro polynomiální jádro a 0.87 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 11.4.1.0.2.3.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 4 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 0.08 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 0 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Klasifikační\\n třída&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 11.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 11.4.1.0.2.3.3 Bázové koeficienty Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 0 % pro lineární jádro, 0.22 % pro polynomiální jádro a 0.87 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 11.4.1.0.2.3.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Code set.seed(42) k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- 32 # norder n_basis_max &lt;- 50 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # hodnoty hyperparametru pro jadrove funkce C &lt;- 1 p &lt;- 3 gamma &lt;- 1/ncol(data.projection.train) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, degree = p, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 33 0.0000000000 ## poly 36 0.0000000000 ## radial 45 0.0007518797 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 33 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0, 36 pro polynomiální jádro s hodnotou spočtenou pomocí 10-násobné CV 0 a 45 pro radiální jádro s hodnotou 8^{-4}. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &#39;Jádro&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 9.8: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, degree = p, coef0 = coef0, gamma = gamma, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 0.22 % pro lineární jádro, 0.22 % pro polynomiální jádro a 0.87 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 11.4.1.0.3 Tabulka výsledků Tabulka 11.2: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) LDA 0e+00 0.0022 LR functional 0e+00 0.0000 SVM linear - diskr 0e+00 0.0000 SVM poly - diskr 0e+00 0.0022 SVM rbf - diskr 0e+00 0.0087 SVM linear - PCA 8e-04 0.0000 SVM poly - PCA 0e+00 0.0000 SVM rbf - PCA 0e+00 0.0000 SVM linear - Bbasis 0e+00 0.0000 SVM poly - Bbasis 0e+00 0.0022 SVM rbf - Bbasis 0e+00 0.0087 SVM linear - projection 0e+00 0.0022 SVM poly - projection 0e+00 0.0022 SVM rbf - projection 0e+00 0.0087 Protože podobně by dopadla klasifikace i ostatních dvojic fonémů, nebudeme uvádět další výsledky. Navíc většina metod klasifikuje tyto dvojice s minimální testovací chybovostí, výsledky jsou tudíž nezajímavé. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["aplikace3.html", "Kapitola 12 Aplikace na reálných datech 3 12.1 Vyhlazení pozorovaných křivek 12.2 Výpočet derivací 12.3 Klasifikace křivek 12.4 Tabulka výsledků 12.5 Klasifikace pomocí druhé derivace 12.6 Tabulka výsledků 12.7 Simulační studie", " Kapitola 12 Aplikace na reálných datech 3 Stejně jako v předchozí kapitole se budeme v této části dokumentu zabývat aplikací dříve popsaných metod (pro více podrobností viz například sekci 1) na reálná data tecator, která jsou dostupná například v balíčku ddalpha. Podrobný popis dat pak můžeme nalézt zde. Jedná se o datový soubor obsahující spektrometrické křivky (absorbanční křivky měřené ve 100 vlnových délkách). Pro každý kus jemně nasekaného masa pozorujeme jednu spektrometrickou křivku, která odpovídá absorbanci naměřené při 100 vlnových délkách. Kusy jsou rozděleny podle Ferratyho a Vieu (2006) do dvou tříd: s malým (\\(&lt; 20\\,\\%\\)) a velkým (\\(\\geq 20\\,\\%\\)) obsahem tuku získaným analytickým chemickým zpracováním. Naším cílem bude klasifikovat spektrometrické křivky na intervalu \\(I = [850 \\text{ nm}, 1050 \\text{ nm}]\\) na základě obsahu tuku. Jak uvidíme z výsledků v části 12.5, je výhodné uvažovat druhou derivaci křivek. Začněme nejprve s načtením a vykreslením dat. Data jsou uložena poněkud složitě, proto pro lepší budou práci s nimi si je uložíme do praktičtějšího formátu. Pojmenujeme si také příslušné sloupce podle toho, zda obsah tuku je malý (small) nebo velký (large). Code # nacteni dat library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) data &lt;- ddalpha::dataf.tecator() data.gr &lt;- data$dataf[[1]]$vals for(i in 2:length(data$labels)) { data.gr &lt;- rbind(data.gr, data$dataf[[i]]$vals) } data.gr &lt;- cbind(data.frame(wave = data$dataf[[1]]$args), t(data.gr)) # vektor trid labels &lt;- data$labels |&gt; unlist() # prejmenovani podle tridy colnames(data.gr) &lt;- c(&#39;wavelength&#39;, paste0(labels, 1:length(data$labels))) Spektrometrické křivky si vykresleme podle skupiny. Code abs.labs &lt;- c(&quot;Obsah tuku &lt; 20 %&quot;, &quot;Obsah tuku &gt; 20 %&quot;) names(abs.labs) &lt;- c(&#39;small&#39;, &#39;large&#39;) pivot_longer(data.gr, cols = large1:large215, names_to = &#39;sample&#39;, values_to = &#39;absorbance&#39;, cols_vary = &#39;slowest&#39;) |&gt; mutate(sample = as.factor(sample), Abs = factor(rep(labels, each = length(data.gr$wavelength)), levels = c(&#39;small&#39;, &#39;large&#39;))) |&gt; ggplot(aes(x = wavelength, y = absorbance, colour = Abs, group = sample)) + geom_line(linewidth = 0.5) + theme_bw() + facet_wrap(~Abs, labeller = labeller(Abs = abs.labs)) + labs(x = &quot;Vlnová délka [v nm]&quot;, y = &quot;Absorbance&quot;, colour = &quot;Obsah tuku&quot;) + theme(legend.position = &#39;none&#39;) + scale_color_discrete(labels = abs.labs) Obrázek 10.1: Absorpční křivky podle skupiny. 12.1 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nejedná o periodické křivky na intervalu \\(I = [850, 1050]\\), využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor wavelength, standardně bychom uvažovali kubické spliny, protože ale budeme chtít pracovat s druhou derivací, volíme norder = 6. Ze stejného důvodu budeme penalizovat čtvrtou derivaci funkcí. Code t &lt;- data.gr$wavelength rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # penalizujeme 4. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě pohlaví stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\) máme v případě rozdílné volby pro každou třídu volit. Code # spojeni pozorovani do jedne matice XX &lt;- data.gr[, -1] |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -1, to = 0.5, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.7) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 3.1: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále při základu 10. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code library(tikzDevice) n &lt;- dim(XX)[2] DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Fat = factor(rep(labels, each = length(t)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(eval.fd(fdobj = mean.fd(XXfd[labels == &#39;small&#39;]), evalarg = t), eval.fd(fdobj = mean.fd(XXfd[labels == &#39;large&#39;]), evalarg = t)), # c(apply(fdobjSmootheval[ , labels == &#39;small&#39;], 1, mean), # apply(fdobjSmootheval[ , labels == &#39;large&#39;], 1, mean)), Fat = factor(rep(c(&#39;small&#39;, &#39;large&#39;), each = length(t)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, color = Fat)) + geom_line(linewidth = 0.05, aes(group = time), alpha = 0.5) + theme_bw() + facet_wrap(~Fat, labeller = labeller(Fat = abs.labs) ) + labs(x = &quot;Vlnová délka&quot;, y = &quot;Absorbance&quot;, colour = &quot;Obsah tuku&quot;) + theme(legend.position = &#39;none&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Obrázek 7.1: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle třídy Černou čarou je zakreslen průměr pro každou třídu. Code # ggsave(&quot;figures/kap7_tecator_curves_mean.tex&quot;, device = tikz, width = 9, height = 4.5) Vidíme, že křivky pro obě skupiny (podle obsahu tuku) jsou poměrně podobné, černou čarou je znázorněn průměr. Křivky se liší zejména uprostřed intervalu, kde u tučnějších vzorků nastává o jeden lokální extrém více, naopak u méně tučných vzorků vypadají křivky jednodušeji pouze s jedním extrémem. 12.2 Výpočet derivací Jak jsme již zmínili výše, bude výhodné klasifikovat křivky na základě druhé derivace. K výpočtu derivace pro funkcionální objekt využijeme v R funkci deriv.fd() z balíčku fda. Jelikož chceme klasifikovat na základě druhé derivace, volíme argument Lfdobj = 2. Využití těchto dat bude ukázáno v Sekci 12.5. Code XXder &lt;- deriv.fd(XXfd, 2) ttt &lt;- seq(min(t), max(t), length = 501) fdobjSmootheval_der2 &lt;- eval.fd(fdobj = XXder, evalarg = ttt) Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code DFsmooth &lt;- data.frame( t = rep(ttt, n), time = factor(rep(1:n, each = length(ttt))), Smooth = c(fdobjSmootheval_der2), Fat = factor(rep(labels, each = length(ttt)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFmean &lt;- data.frame( t = rep(ttt, 2), Mean = c(eval.fd(fdobj = mean.fd(XXder[labels == &#39;small&#39;]), evalarg = ttt), eval.fd(fdobj = mean.fd(XXder[labels == &#39;large&#39;]), evalarg = ttt)), Fat = factor(rep(c(&#39;small&#39;, &#39;large&#39;), each = length(ttt)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, color = Fat)) + geom_line(linewidth = 0.05, aes(group = time), alpha = 0.5) + theme_bw() + facet_wrap(~Fat#, #labeller = labeller(Fat = abs.labs) ) + labs(x = &quot;Vlnová délka&quot;, y = &quot;Absorbance&quot;, colour = &quot;Obsah tuku&quot;) + theme(legend.position = &#39;none&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Obrázek 1.3: Vykreslení všech vyhlazených pozorovaných křivek, barevně jsou odlišeny křivky podle příslušnosti do klasifikační třídy. Černou čarou je zakreslen průměr pro každou třídu. Code # ggsave(&quot;figures/kap7_tecator_curves_derivatives.tex&quot;, device = tikz, width = 9, height = 4.5) Vidíme z obrázku výše, že nyní se průměrné křivky mezi oběma skupinami vzorků liší mnohem výrazněji než v případě původních nederivovaných křivek. 12.3 Klasifikace křivek V první části této kapitoly se budeme věnovat klasifikaci původních nederivovaných křivek. Klasifikaci na základě druhé derivace původních křivek uvidíme dále v Sekci 12.5. Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les set.seed(42) Rozdělíme data v poměru 30:70 na testovací a trénovací část, abychom mohli stanovit úspěšnost klasifikace jednotlivých metod. Trénovací část použijeme při konstrukci klasifikátoru a testovací na výpočet chyby klasifikace a případně dalších charakteristik našeho modelu. Výsledné klasifikátory podle těchto spočtených charakteristik můžeme následně porovnat mezi sebou z pohledu jejich úspěšnosti klasifikace. Code # rozdeleni na testovaci a trenovaci cast set.seed(42) split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 91 59 Code table(Y.test) ## Y.test ## 0 1 ## 47 18 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.6066667 0.3933333 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.7230769 0.2769231 12.3.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) 1 - neighb.model$max.prob # minimalni chybovost ## [1] 0.1466667 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 1 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 1 s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.1478. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 5.2: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() Vidíme tedy, že chybovost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 1, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.1467 a na testovacích datech 0.1692. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 12.3.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p=\\) 2, které dohromady vysvětlují 99.57 \\(\\%\\) variability v datech. První hlavní komponenta potom vysvětluje 98.47 % a druhá 1.09 \\(\\%\\) variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() Obrázek 3.3: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní funkce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (32 %), tak i na testovacích datech (29.23 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 8.1: Skóre prvních dvou hlavních komponent, barevně odlišené podle klasifikační třídy Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak chybovost klasifikátoru na trénovacích (32 %), tak i na testovacích datech (30.77 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 7.2: Skóre prvních dvou hlavních komponent, barevně odlišené podle klasifikační třídy Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 12.3.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # potrebujeme zvolit bazi pro funkcionalni pozorovani, klasicky volime # stejnou bazi jako je ta pouzita pro vyhlazeni krivek. Tato volba vsak # vede k numericke chybe, zvolime proto bazi s mensim poctem bazovych funkci. # Po vyzkouseni nekolika moznosti se zda, ze 7 funkci je dostatecne mnoho. nbasis.x &lt;- 7 # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 3.6: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 6, tedy výrazně menší hodnota než 30. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 24 s validační chybovostí 0.0488. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) Obrázek 10.2: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 24 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b, maxit = 1000, epsilon = 1e-2) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 0 %) i testovací chybovost (rovna 6.15 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Obsah tuku&#39;) Obrázek 1.11: Závoslost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(min(t), max(t), length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line() + theme_bw() + labs(x = expression(x[1]), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Obrázek 5.7: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [850, 1050]\\). Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sestrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy chybovost klasifikátoru na trénovacích (30.67 %) i na testovacích datech (29.23 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_colour_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 2.8: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 12.3.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [850, 1050]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní můžeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost klasifikátoru na testovacích datech je tedy 35.38 % a na trénovacích datech 29.33 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 2.9: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 12.1: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 2 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na testovacích datech je tedy 38.46 % a na trénovacích datech 32.67 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 1.16: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 2.10: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na trénovacích datech je tedy 22.67 % a na testovacích datech 24.62 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 7.5: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 12.2: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 12.3.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [850, 1050]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 2 % a na testovacích datech 12.31 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 4.67 % a na testovacích datech 30.77 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost tohoto klasifikátoru na trénovacích datech je 1.33 % a na testovacích datech 12.31 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.3.7 Support Vector Machines Nyní se podívejme na klasifikaci křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního a radiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Volíme přitom \\(\\alpha_0^{default} = 1\\), neboť tato hodnota dává výrazně lepší výsledky. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 12.3.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 12.3.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 12.3.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 12.3.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 12.3.7.5. 12.3.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [850, 1050]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(BSmooth$fd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Nyní se pokusme, na rozdíl od postupu v předchozích kapitolách, hyperparametry klasifikátorů odhadnout z dat pomocí 10-násobné cross-validace. Vzhledem k tomu, že každé jádro má ve své definici jiné hyperparametry, budeme ke každé jádrové funkci přistupovat zvlášť. Nicméně hyperparametr \\(C\\) vystupuje u všech jádrových funkcí, přičemž ale připouštíme, že se může jeho optimální hodnota mezi jádry lišit. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.0379, pro polynomiální jádro je \\(C\\) rovno 1.4384 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 12.7427 a pro \\(\\gamma\\) je to 0.0052. Validační chybovosti jsou postupně 0.0066667 pro lineární, 0.0195833 pro polynomiální a 0.0133333 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 0.6667 % pro lineární jádro, 1.3333 % pro polynomiální jádro a 1.3333 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 7.6923 % pro lineární jádro, 7.6923 % pro polynomiální jádro a 4.6154 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 12.3.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Nyní se pokusme, na rozdíl od postupu v předchozích kapitolách, hyperparametry klasifikátorů odhadnout z dat pomocí 10-násobné cross-validace. Vzhledem k tomu, že každé jádro má ve své definici jiné hyperparametry, budeme ke každé jádrové funkci přistupovat zvlášť. Nicméně hyperparametr \\(C\\) vystupuje u všech jádrových funkcí, přičemž ale připouštíme, že se může jeho optimální hodnota mezi jádry lišit. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.1624, pro polynomiální jádro je \\(C\\) rovno 0.0785 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 26.3665 a pro \\(\\gamma\\) je to 1.6379. Validační chybovosti jsou postupně 0.328869 pro lineární, 0.328869 pro polynomiální a 0.2959524 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 32 % pro lineární jádro, 33.33 % pro polynomiální jádro a 16 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 29.2308 % pro lineární jádro, 27.6923 % pro polynomiální jádro a 36.9231 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 12.3: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 12.3.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 6.1585, pro polynomiální jádro je \\(C\\) rovno 54.5559 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 26.3665 a pro \\(\\gamma\\) je to 0.0118. Validační chybovosti jsou postupně 0.0195833 pro lineární, 0.0325 pro polynomiální a 0.0258333 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0.67 % pro lineární jádro, 0.67 % pro polynomiální jádro a 1.33 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 6.1538 % pro lineární jádro, 9.2308 % pro polynomiální jádro a 6.1538 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 12.3.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 20. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 9 0.01958333 ## poly 7 0.03297619 ## radial 6 0.14125000 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 9 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0196, 7 pro polynomiální jádro s hodnotou spočtenou pomocí 10-násobné CV 0.033 a 6 pro radiální jádro s hodnotou chybovosti 0.1412. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &#39;Jádro&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 12.4: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 2 % pro lineární jádro, 2.67 % pro polynomiální jádro a 9.33 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 6.15 % pro lineární jádro, 6.15 % pro polynomiální jádro a 10.77 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.3.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice 1.1 (Jádro) Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\end{equation*}\\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma 12.1 Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[\\begin{equation*} \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\end{equation*}\\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení 12.1 Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[\\begin{equation*} K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\end{equation*}\\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 12.3.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 1.2 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (1.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 12.3.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku8 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení 1.3 (Muñoz-González) Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 12.3.7.5.3 Implementace metody v R Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). Jako dobrá volba hyperparametrů se po vyzkoušení zdají být hodnoty \\(\\varepsilon = 0.01\\) a \\(C = 1\\). Vzhledem k výpočetní náročnosti nebudeme tyto hyperparametry odhadovat pomocí CV. Code eps &lt;- 0.01 C &lt;- 1 12.3.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, cost = C, coef0 = coef0, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 12.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0133 0.1231 SVM poly - RKHS 0.0267 0.0308 SVM rbf - RKHS 0.0400 0.0308 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není pro lineární jádro dobrá. Proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:30 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 11.1: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 11 1.0000 0 linear poly 27 1.0000 0 polynomial radial 30 3.7276 0 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 11 a \\(\\gamma={}\\) 1 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0, \\(d={}\\) 27 a \\(\\gamma={}\\) 1 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0 a \\(d={}\\) 30 a \\(\\gamma={}\\) 3.7276 pro radiální jádro s hodnotou chybovosti 0. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 12.5: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 12.2: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0 0.0308 SVM poly - RKHS - radial 0 0.0308 SVM rbf - RKHS - radial 0 0.0154 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 0 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 3.08 % pro lineární jádro, 3.08 % pro polynomiální jádro a 1.54 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.3.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 2:30 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, coef0 = 1, cost = C, epsilon = eps, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, gamma = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 12.3: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 20 5 0.0474 linear poly 10 3 0.0461 polynomial radial 7 5 0.0403 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 20 a \\(p={}\\) 5 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0474, \\(d={}\\) 10 a \\(p={}\\) 3 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0461 a \\(d={}\\) 7 a \\(p={}\\) 5 pro radiální jádro s hodnotou chybovosti 0.0403. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, cost = C, gamma = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = C, gamma = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 2.6: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.0333 0.0615 SVM poly - RKHS - poly 0.0267 0.1077 SVM rbf - RKHS - poly 0.0333 0.1077 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 3.33 % pro lineární jádro, 2.67 % pro polynomiální jádro a 3.33 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 6.15 % pro lineární jádro, 10.77 % pro polynomiální jádro a 10.77 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.3.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 2:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 2.7: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 15 0.0667 linear poly 16 0.0454 polynomial radial 25 0.0526 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 15 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0667, \\(d={}\\) 16 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0454 a \\(d={}\\) 25 pro radiální jádro s hodnotou chybovosti 0.0526. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 1.7: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.0400 0.0923 SVM poly - RKHS - linear 0.0133 0.0308 SVM rbf - RKHS - linear 0.0200 0.0308 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 4 % pro lineární jádro, 1.33 % pro polynomiální jádro a 2 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 9.23 % pro lineární jádro, 3.08 % pro polynomiální jádro a 3.08 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.4 Tabulka výsledků Vidíme z tabulky níže, že jednotlivé klasifikační metody mají mezi sebou výrazné rozdíly co do úspěšnosti klasifikace. Zejména klasické metody, jako je KNN, LDA nebo QDA si vedou velmi bídně. Můžeme si všimnout, že všechny metody postavené na funkcionální analýze hlavních komponent nedosahují zdaleka podobných výsledků jako některé jiné metody. Naopak nyní se vymyká svou dobrou klasifikační schopností metoda RKHS společně s SVM. Poznamenejme, že také klasická SVM s lineárním jádrem si vede velmi obstojně. Obecně je lineární jádro dobrou volbou (jak jsme se ostatně mohli přesvědčit již dříve), neboť pro dostatečně hustou síť bodů dobře aproximuje určitý integrál na uvažovaném intervalu \\(I\\). Tabulka 2.8: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.1467 0.1692 LDA 0.3200 0.2923 QDA 0.3200 0.3077 LR functional 0.0000 0.0615 LR score 0.3067 0.2923 Tree - diskr. 0.2933 0.3538 Tree - score 0.3267 0.3846 Tree - Bbasis 0.2267 0.2462 RForest - diskr 0.0200 0.1231 RForest - score 0.0467 0.3077 RForest - Bbasis 0.0133 0.1231 SVM linear - diskr 0.0067 0.0769 SVM poly - diskr 0.0133 0.0769 SVM rbf - diskr 0.0133 0.0462 SVM linear - PCA 0.3200 0.2923 SVM poly - PCA 0.3333 0.2769 SVM rbf - PCA 0.1600 0.3692 SVM linear - Bbasis 0.0067 0.0615 SVM poly - Bbasis 0.0067 0.0923 SVM rbf - Bbasis 0.0133 0.0615 SVM linear - projection 0.0200 0.0615 SVM poly - projection 0.0267 0.0615 SVM rbf - projection 0.0933 0.1077 SVM linear - RKHS - radial 0.0000 0.0308 SVM poly - RKHS - radial 0.0000 0.0308 SVM rbf - RKHS - radial 0.0000 0.0154 SVM linear - RKHS - poly 0.0333 0.0615 SVM poly - RKHS - poly 0.0267 0.1077 SVM rbf - RKHS - poly 0.0333 0.1077 SVM linear - RKHS - linear 0.0400 0.0923 SVM poly - RKHS - linear 0.0133 0.0308 SVM rbf - RKHS - linear 0.0200 0.0308 12.5 Klasifikace pomocí druhé derivace Jak jsme již avizovali dříve, pro tato data je vhodné ke klasifikaci uvažovat jejich druhou derivaci. Tu jsme si již spočetli výše, proto už se nyní můžeme pustit rovnou do konstrukce modelů. Proveďme obdobnou analýzu jako v situaci výše, následně (jelikož data náhodně rozdělujeme na testovací a trénovací část), provedeme simulační studii, pomocí které budeme jednotlivé klasifikační metody schopni lépe a s mnohem větší silou porovnat. Code # rozdeleni na testovaci a trenovaci cast set.seed(42) split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 91 59 Code table(Y.test) ## Y.test ## 0 1 ## 47 18 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.6066667 0.3933333 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.7230769 0.2769231 12.5.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) neighb.model$max.prob # maximalni presnost ## [1] 0.9866667 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 3 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 3 s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0103. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) Obrázek 1.25: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() Vidíme tedy, že chybovost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 3, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.0133 a na testovacích datech 0.0769. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 12.5.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p=\\) 2, které dohromady vysvětlují 93.12 \\(\\%\\) variability v datech. První hlavní komponenta potom vysvětluje 77.7 % a druhá 15.42 \\(\\%\\) variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() Obrázek 11.3: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak chybovost klasifikátoru na trénovacích (4 %), tak i na testovacích datech (9.23 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 1.28: Skóre prvních dvou hlavních komponent, barevně odlišené podle klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak chybovost klasifikátoru na trénovacích (0.67 %), tak i na testovacích datech (1.54 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 12.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola, avšak se jen (alespoň opticky) velmi málo liší od přímky. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 12.5.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; nbasis.x &lt;- 7 # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Obrázek 12.7: Závislost testovací a trénovací chybovosti na počtu bázových funkcí pro \\(\\beta\\). Červeným bodem je znázorněn optimální počet \\(n_{optimal}\\) zvolený jako minimum testovací chybovosti, černou čarou je vykreslena testovací a modrou přerušovanou čarou je vykreslen průběh trénovací chybovosti. Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno 5, tedy výrazně menší hodnota než 30. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou 7 s validační chybovostí 0.0574. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) Obrázek 5.19: Závislost validační chybovosti na hodnotě \\(n_{basis}\\), tedy na počtu bází. Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s 7 bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b, maxit = 1000, epsilon = 1e-2) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna 0 %) i testovací chybovost (rovna 7.69 %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_color_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Obsah tuku&#39;) Obrázek 5.20: Závislost odhadnutých pravděpodobností na hodnotách lineárního prediktoru. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(min(t), max(t), length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line() + theme_bw() + labs(x = expression(x[1]), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Obrázek 12.8: Průběh odhadu parametrické funkce \\(\\beta(t), t \\in [850, 1050]\\). Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro časy \\(t\\) z prostředka a začátku intervalu \\([850, 1050]\\), zatímco pro pozdějsí časy jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd na začátku a konci intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred Code # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy chybovost klasifikátoru na trénovacích (0.67 %) i na testovacích datech (4.62 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;) + scale_colour_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 9.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 12.5.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [850, 1050]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost klasifikátoru na testovacích datech je tedy 1.54 % a na trénovacích datech 0.67 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 9.8: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 9.9: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 2 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na testovacích datech je tedy 6.15 % a na trénovacích datech 0.67 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 12.9: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 12.10: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost rozhodovacího stromu na trénovacích datech je tedy 0.67 % a na testovacích datech 1.54 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 12.11: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 12.12: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 12.5.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [850, 1050]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 0 % a na testovacích datech 4.62 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost náhodného lesu na trénovacích datech je tedy 0.67 % a na testovacích datech 4.62 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost tohoto klasifikátoru na trénovacích datech je 0 % a na testovacích datech 3.08 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 12.5.7 Support Vector Machines Nyní se podívejme na klasifikaci našich křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního a radiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci (sekce 12.5.7.1). Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace 12.5.7.2. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi (sekce 12.5.7.3). Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme využít projekce funkcí na podprostor generovaný, např. B-splinovými, funkcemi (sekce 12.5.7.4). Poslední metoda, kterou použijeme pro klasifikaci funkcionálních dat, spočívá v kombinaci projekce na určitý podprostor generovaný funkcemi (Reproducing Kernel Hilbert Space, RKHS) a klasifikace příslušné reprezentace. Tato metoda využívá kromě klasického SVM i SVM pro regresi, více uvádíme v sekci RKHS + SVM 12.5.7.5. 12.5.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [850, 1050]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm_der &lt;- XXder XXfd_norm_der$coefs &lt;- XXfd_norm_der$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm_der, split == TRUE) X.test_norm &lt;- subset(XXfd_norm_der, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Nyní se pokusme, na rozdíl od postupu v předchozích kapitolách, hyperparametry klasifikátorů odhadnout z dat pomocí 10-násobné cross-validace. Vzhledem k tomu, že každé jádro má ve své definici jiné hyperparametry, budeme ke každé jádrové funkci přistupovat zvlášť. Nicméně hyperparametr \\(C\\) vystupuje u všech jádrových funkcí, přičemž ale připouštíme, že se může jeho optimální hodnota mezi jádry lišit. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.001, pro polynomiální jádro je \\(C\\) rovno 0.0785 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 0.336 a pro \\(\\gamma\\) je to 0.0052. Validační chybovosti jsou postupně 0.0066667 pro lineární, 0.0066667 pro polynomiální a 0.0066667 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM na trénovacích datech je tedy 0.6667 % pro lineární jádro, 0.6667 % pro polynomiální jádro a 0.6667 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 1.5385 % pro lineární jádro, 6.1538 % pro polynomiální jádro a 1.5385 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 12.5.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních \\(p =\\) 2 hlavních komponent. Nyní se pokusme, na rozdíl od postupu v předchozích kapitolách, hyperparametry klasifikátorů odhadnout z dat pomocí 10-násobné cross-validace. Vzhledem k tomu, že každé jádro má ve své definici jiné hyperparametry, budeme ke každé jádrové funkci přistupovat zvlášť. Nicméně hyperparametr \\(C\\) vystupuje u všech jádrových funkcí, přičemž ale připouštíme, že se může jeho optimální hodnota mezi jádry lišit. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 2.9764, pro polynomiální jádro je \\(C\\) rovno 0.6952 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 1000 a pro \\(\\gamma\\) je to 1.6379. Validační chybovosti jsou postupně 0.0066667 pro lineární, 0.0066667 pro polynomiální a 0.0066667 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 0.67 % pro lineární jádro, 0.67 % pro polynomiální jádro a 0.67 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 4.6154 % pro lineární jádro, 1.5385 % pro polynomiální jádro a 3.0769 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Obsah tuku&#39;, linetype = &#39;Jádro&#39;) + scale_colour_discrete(labels = c(&quot;malý&quot;, &quot;velký&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 9.16: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 12.5.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Podívejme se, jak dopadly optimální hodnoty. Pro lineární jádro máme optimální hodnotu \\(C\\) rovnu 0.0089, pro polynomiální jádro je \\(C\\) rovno 0.1624 a pro radiální jádro máme dvě optimální hodnoty, pro \\(C\\) je optimální hodnota 2.9764 a pro \\(\\gamma\\) je to 0.0118. Validační chybovosti jsou postupně 0.0066667 pro lineární, 0.0066667 pro polynomiální a 0.0066667 pro radiální jádro. Konečně můžeme sestrojit finální klasifikátory na celých trénovacích datech s hodnotami hyperparametrů určenými pomocí 10-násobné CV. Určíme také chybovosti na testovacích a také na trénovacích datech. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 0.67 % pro lineární jádro, 0.67 % pro polynomiální jádro a 0.67 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 7.6923 % pro lineární jádro, 7.6923 % pro polynomiální jádro a 9.2308 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 12.5.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 20. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 9 0.01958333 ## poly 7 0.03297619 ## radial 6 0.14125000 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 9 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0196, 7 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.033 a 6 pro radiální jádro s hodnotou chybovosti 0.1412. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;lineární&#39;, &#39;polynomiální&#39;, &#39;radiální&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;, colour = &#39;Jádro&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 9.18: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Chybovost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 2 % pro lineární jádro, 2.67 % pro polynomiální jádro a 9.33 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 6.15 % pro lineární jádro, 6.15 % pro polynomiální jádro a 10.77 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.5.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Z poslední části Tvrzení 1.3 vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). Code # hodnoty hypermarametru stejne jako v minule casti eps &lt;- 0.01 C &lt;- 1 12.5.7.5.0.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 101, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, cost = C, coef0 = coef0, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 12.4: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0 0 SVM poly - RKHS 0 0 SVM rbf - RKHS 0 0 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:30 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 12.5: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 12 1.0000 0 linear poly 18 1.0000 0 polynomial radial 17 1.9307 0 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 12 a \\(\\gamma={}\\) 1 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0, \\(d={}\\) 18 a \\(\\gamma={}\\) 1 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0 a \\(d={}\\) 17 a \\(\\gamma={}\\) 1.9307 pro radiální jádro s hodnotou chybovosti 0. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 12.13: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 12.6: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0 0.0308 SVM poly - RKHS - radial 0 0.0769 SVM rbf - RKHS - radial 0 0.0308 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 0 % pro lineární jádro, 0 % pro polynomiální jádro a 0 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 3.08 % pro lineární jádro, 7.69 % pro polynomiální jádro a 3.08 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.5.7.5.0.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 2:30 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, coef0 = 1, cost = C, epsilon = eps, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, gamma = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 12.7: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 30 3 0.0077 linear poly 17 2 0.0077 polynomial radial 3 4 0.0254 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 30 a \\(p={}\\) 3 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0077, \\(d={}\\) 17 a \\(p={}\\) 2 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0077 a \\(d={}\\) 3 a \\(p={}\\) 4 pro radiální jádro s hodnotou chybovosti 0.0254. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, cost = C, gamma = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = C, gamma = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 12.8: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.00 0.0308 SVM poly - RKHS - poly 0.00 0.0154 SVM rbf - RKHS - poly 0.02 0.0308 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 0 % pro lineární jádro, 0 % pro polynomiální jádro a 2 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 3.08 % pro lineární jádro, 1.54 % pro polynomiální jádro a 3.08 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.5.7.5.0.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 2:40 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 12.9: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 31 0.0267 linear poly 21 0.0267 polynomial radial 6 0.0592 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 31 pro lineární jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0267, \\(d={}\\) 21 pro polynomiální jádro s hodnotou chybovosti spočtenou pomocí 10-násobné CV 0.0267 a \\(d={}\\) 6 pro radiální jádro s hodnotou chybovosti 0.0592. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 12.10: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.00 0.0462 SVM poly - RKHS - linear 0.00 0.0308 SVM rbf - RKHS - linear 0.02 0.1231 Chybovost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 0 % pro lineární jádro, 0 % pro polynomiální jádro a 2 % pro gaussovské jádro. Na testovacích datech je potom chybovost metody 4.62 % pro lineární jádro, 3.08 % pro polynomiální jádro a 12.31 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 12.6 Tabulka výsledků Z tabulky níže si všimněme zejména dvou podstatných věcí. Tou první je, že metody klasifikují data podstatně lépe než v situaci původních nederivovaných dat. U některých metod je zlepšení i v řádech desítek procent. Druhou podstatnou věcí je fakt, že nyní není takový výrazný rozdíl mezi výsledky jednotlivých metod. Tabulka 12.11: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.0133 0.0769 LDA 0.0400 0.0923 QDA 0.0067 0.0154 LR functional 0.0000 0.0769 LR score 0.0067 0.0462 Tree - diskr. 0.0067 0.0154 Tree - score 0.0067 0.0615 Tree - Bbasis 0.0067 0.0154 RForest - diskr 0.0000 0.0462 RForest - score 0.0067 0.0462 RForest - Bbasis 0.0000 0.0308 SVM linear - diskr 0.0067 0.0154 SVM poly - diskr 0.0067 0.0615 SVM rbf - diskr 0.0067 0.0154 SVM linear - PCA 0.0067 0.0462 SVM poly - PCA 0.0067 0.0154 SVM rbf - PCA 0.0067 0.0308 SVM linear - Bbasis 0.0067 0.0769 SVM poly - Bbasis 0.0067 0.0769 SVM rbf - Bbasis 0.0067 0.0923 SVM linear - projection 0.0200 0.0615 SVM poly - projection 0.0267 0.0615 SVM rbf - projection 0.0933 0.1077 SVM linear - RKHS - radial 0.0000 0.0308 SVM poly - RKHS - radial 0.0000 0.0769 SVM rbf - RKHS - radial 0.0000 0.0308 SVM linear - RKHS - poly 0.0000 0.0308 SVM poly - RKHS - poly 0.0000 0.0154 SVM rbf - RKHS - poly 0.0200 0.0308 SVM linear - RKHS - linear 0.0000 0.0462 SVM poly - RKHS - linear 0.0000 0.0308 SVM rbf - RKHS - linear 0.0200 0.1231 12.7 Simulační studie V celé předchozí části jsme se zabývali souborem funkcí ze dvou klasifikačních tříd, který jsme následně náhodně rozdělili na testovací a trénovací část. Poté jsme jednotlivé klasifikátory získané pomocí uvažovaných metod ohodnotili na základě testovací a trénovací chybovosti. Jelikož se rozdělení dat na dvě části mohou při každém zopakování výrazně lišit, budou se i chybovosti jednotlivých klasifikačních algoritmů výrazně lišit. Proto dělat jakékoli závěry o metodách a porovnávat je mezi sebou může být na základě jednoho vygenerovaného trénovacího datového souboru velmi zavádějící. Z tohoto důvodu se v této části zaměříme na opakování celého předchozího postupu pro různá rozdělení. Výsledky si budeme ukládat do tabulky a nakonec spočítáme průměrné charakteristiky modelů přes jednotlivá opakování. Aby byly naše závěry dostatečně obecné, zvolíme počet opakování \\(n_{sim} = 100\\). 12.7.1 Simulace pro nederivovaná data Nejprve se podívejme na simulaci původních, tedy nederivovaných, dat. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:10) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze nbasis.x &lt;- 7 basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) } # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-2, 2, length = 5) C.cv &lt;- 10^seq(-3, 2, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], coef0 = 1, degree = p.opt, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], coef0 = 1, degree = p.opt, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS C &lt;- 1 eps &lt;- 0.01 ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 30, by = 2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, coef0 = 1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 10, by = 1) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/aplikace_03neder.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/aplikace_03neder_res.RData&#39;) 12.7.1.1 Výsledky Tabulka 12.12: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.1730 0.1777 0.0265 0.0504 LDA 0.2996 0.3163 0.0207 0.0446 QDA 0.3017 0.3182 0.0217 0.0441 LR_functional 0.0105 0.0482 0.0278 0.0467 LR_score 0.2919 0.3103 0.0207 0.0447 Tree_discr 0.1857 0.2937 0.0449 0.0583 Tree_score 0.2470 0.3360 0.0451 0.0574 Tree_Bbasis 0.1855 0.2918 0.0456 0.0565 RF_discr 0.0121 0.2072 0.0091 0.0502 RF_score 0.0360 0.3100 0.0110 0.0507 RF_Bbasis 0.0121 0.2066 0.0087 0.0453 SVM linear - diskr 0.0036 0.0162 0.0058 0.0227 SVM poly - diskr 0.0113 0.0455 0.0129 0.0234 SVM rbf - diskr 0.0053 0.0346 0.0063 0.0229 SVM linear - PCA 0.2989 0.3285 0.0234 0.0495 SVM poly - PCA 0.2833 0.3515 0.0329 0.0452 SVM rbf - PCA 0.1553 0.3469 0.1128 0.0479 SVM linear - Bbasis 0.0113 0.0277 0.0074 0.0194 SVM poly - Bbasis 0.0137 0.0495 0.0104 0.0287 SVM rbf - Bbasis 0.0153 0.0532 0.0063 0.0252 SVM linear - projection 0.0312 0.0425 0.0103 0.0244 SVM poly - projection 0.0339 0.0577 0.0135 0.0331 SVM rbf - projection 0.1395 0.2034 0.0289 0.0554 SVM linear - RKHS - radial 0.0008 0.0197 0.0024 0.0188 SVM poly - RKHS - radial 0.0009 0.0132 0.0023 0.0178 SVM rbf - RKHS - radial 0.0036 0.0195 0.0047 0.0163 SVM linear - RKHS - poly 0.0543 0.0832 0.0138 0.0321 SVM poly - RKHS - poly 0.0305 0.0889 0.0152 0.0305 SVM rbf - RKHS - poly 0.0302 0.0642 0.0094 0.0294 SVM linear - RKHS - linear 0.0448 0.0725 0.0149 0.0349 SVM poly - RKHS - linear 0.0433 0.0754 0.0134 0.0358 SVM rbf - RKHS - linear 0.0803 0.1188 0.0217 0.0434 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Můžeme také formálně otestovat, zda jsou některé metody úspěšnější než jiné. Vzhledem k nesplnění předpokladu normality nemůžeme použít klasický párový t-test. Využijeme jeho neparametrickou alternativu - Wilcoxonův test. Code wilcox.test(SIMULACE$test[, &#39;SVM poly - RKHS - radial&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.3874749 Code wilcox.test(SIMULACE$test[, &#39;SVM rbf - RKHS - radial&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.201061 Code wilcox.test(SIMULACE$test[, &#39;SVM linear - RKHS - radial&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.1067453 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;greater&#39;, paired = T)$p.value ## [1] 2.09865e-11 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM poly - RKHS - radial&#39;], alternative = &#39;greater&#39;, paired = T)$p.value ## [1] 2.355944e-13 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM rbf - RKHS - radial&#39;], alternative = &#39;greater&#39;, paired = T)$p.value ## [1] 1.332949e-10 Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # nastavime jinak nazvy klasifikacnich metod methods_names &lt;- c( &#39;$K$ nejbližších sousedů&#39;, &#39;Lineární diskriminační analýza&#39;, &#39;Kvadratická diskriminační analýza&#39;, &#39;Funkcionální logistická regrese&#39;, &#39;Logistické regrese s fPCA&#39;, &#39;Rozhodovací strom -- diskretizace&#39;, &#39;Rozhodovací strom -- fPCA&#39;, &#39;Rozhodovací strom -- bázové koeficienty&#39;, &#39;Náhodný les -- diskretizace&#39;, &#39;Náhodný les -- fPCA&#39;, &#39;Náhodný les -- bázové koeficienty&#39;, &#39;SVM (linear) -- diskretizace&#39;, &#39;SVM (poly) -- diskretizace&#39;, &#39;SVM (radial) -- diskretizace&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (poly) -- fPCA&#39;, &#39;SVM (radial) -- fPCA&#39;, &#39;SVM (linear) -- bázové koeficienty&#39;, &#39;SVM (poly) -- bázové koeficienty&#39;, &#39;SVM (radial) -- bázové koeficienty&#39;, &#39;SVM (linear) -- projekce&#39;, &#39;SVM (poly) -- projekce&#39;, &#39;SVM (radial) -- projekce&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39;, &#39;RKHS (radial SVR) $+$ SVM (poly)&#39;, &#39;RKHS (radial SVR) $+$ SVM (radial)&#39;, &#39;RKHS (poly SVR) $+$ SVM (linear)&#39;, &#39;RKHS (poly SVR) $+$ SVM (poly)&#39;, &#39;RKHS (poly SVR) $+$ SVM (radial)&#39;, &#39;RKHS (linear SVR) $+$ SVM (linear)&#39;, &#39;RKHS (linear SVR) $+$ SVM (poly)&#39;, &#39;RKHS (linear SVR) $+$ SVM (radial)&#39; ) # barvy pro boxploty box_col &lt;- c(&#39;#4dd2ff&#39;, &#39;#0099cc&#39;, &#39;#00ace6&#39;, &#39;#00bfff&#39;, &#39;#1ac5ff&#39;, rep(&#39;#33ccff&#39;, 3), rep(&#39;#0086b3&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff6347&#39;, 3), rep(&#39;#ff7961&#39;, 3), rep(&#39;#ff4d2e&#39;, 3), rep(&#39;#fa2600&#39;, 9)) # box_col &lt;- c(&#39;#CA0A0A&#39;, &#39;#fa2600&#39;, &#39;#fa2600&#39;, &#39;#D15804&#39;, # &#39;#D15804&#39;, rep(&#39;#D3006D&#39;, 3), rep(&#39;#BE090F&#39;, 3), c(&quot;#12DEE8&quot;, &quot;#4ECBF3&quot;, &quot;#127DE8&quot;, &quot;#4C3CD3&quot;, &quot;#4E65F3&quot;, &quot;#4E9EF3&quot;, &quot;#081D58&quot;) |&gt; rep(each = 3)) # alpha pro boxploty box_alpha &lt;- c(0.9, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, seq(0.9, 0.6, length = 9)) #- 0.3 Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 60, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 12.14: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = &quot;$\\\\widehat{\\\\textnormal{Err}}_{test}$&quot; # y = expression(widehat(Err)[test]) ) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 50, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 3, color = &quot;black&quot;, alpha = 0.9) + scale_x_discrete(labels = methods_names) + theme(plot.margin = unit(c(0.5, 0.5, 2, 2), &quot;cm&quot;)) + scale_fill_manual(values = box_col) + scale_alpha_manual(values = box_alpha) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;gray25&#39;, alpha = 0.8) Obrázek 9.29: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # ggsave(&quot;figures/kap7_tecator_box_test_neder.tex&quot;, device = tikz, width = 9, height = 7) Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 12.13: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 1 nharm 1 LR_func_n_basis 19 SVM_d_Linear 6 SVM_d_Poly 6 SVM_d_Radial 6 SVM_RKHS_radial_gamma1 1 SVM_RKHS_radial_gamma2 1 SVM_RKHS_radial_gamma3 1 SVM_RKHS_radial_d1 17 SVM_RKHS_radial_d2 16 SVM_RKHS_radial_d3 20 SVM_RKHS_poly_p1 4 SVM_RKHS_poly_p2 4 SVM_RKHS_poly_p3 5 SVM_RKHS_poly_d1 8 SVM_RKHS_poly_d2 7 SVM_RKHS_poly_d3 7 SVM_RKHS_linear_d1 20 SVM_RKHS_linear_d2 17 SVM_RKHS_linear_d3 13 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.15: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.16: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.17: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.18: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.19: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. 12.7.2 Simulace pro derivovaná data Nyní se konečně podívejme na simulaci derivovaných dat (určili jsme druhou derivaci křivek). Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Nyní zopakujeme celou předchozí část 100-krát a hodnoty chybovostí si budeme ukládat to listu SIMULACE. Do datové tabulky CV_RESULTS si potom budeme ukládat hodnoty optimálních hyperparametrů – pro metodu \\(K\\) nejbližších sousedů a pro SVM hodnotu dimenze \\(d\\) v případě projekce na B-splinovou bázi. Také uložíme všechny hodnoty hyperparametrů pro metodu SVM + RKHS. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:10) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze nbasis.x &lt;- 7 basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm_der, split == TRUE) X.test_norm &lt;- subset(XXfd_norm_der, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) } # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-2, 2, length = 5) C.cv &lt;- 10^seq(-3, 2, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], coef0 = coef0, degree = p.opt, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS C &lt;- 1 eps &lt;- 0.01 ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 30, by = 2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 10, by = 1) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/aplikace_03der.RData&#39;) Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # ulozime vysledne hodnoty save(SIMULACE.df, file = &#39;RData/aplikace_03der_res.RData&#39;) 12.7.2.1 Výsledky Tabulka 12.14: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.0147 0.0212 0.0066 0.0170 LDA 0.0558 0.0626 0.0089 0.0287 QDA 0.0105 0.0145 0.0064 0.0129 LR_functional 0.0009 0.0405 0.0032 0.0327 LR_score 0.0081 0.0145 0.0059 0.0157 Tree_discr 0.0109 0.0263 0.0454 0.0603 Tree_score 0.0171 0.0229 0.0063 0.0176 Tree_Bbasis 0.0107 0.0251 0.0455 0.0595 RF_discr 0.0003 0.0117 0.0015 0.0122 RF_score 0.0057 0.0168 0.0032 0.0164 RF_Bbasis 0.0001 0.0089 0.0007 0.0103 SVM linear - diskr 0.0033 0.0091 0.0052 0.0137 SVM poly - diskr 0.0013 0.0152 0.0032 0.0164 SVM rbf - diskr 0.0025 0.0148 0.0040 0.0135 SVM linear - PCA 0.0097 0.0197 0.0062 0.0203 SVM poly - PCA 0.0080 0.0174 0.0060 0.0167 SVM rbf - PCA 0.0073 0.0174 0.0058 0.0144 SVM linear - Bbasis 0.0033 0.0249 0.0065 0.0216 SVM poly - Bbasis 0.0031 0.0234 0.0044 0.0174 SVM rbf - Bbasis 0.0033 0.0220 0.0068 0.0197 SVM linear - projection 0.0297 0.0449 0.0086 0.0274 SVM poly - projection 0.0339 0.0560 0.0142 0.0393 SVM rbf - projection 0.1454 0.1954 0.0306 0.0605 SVM linear - RKHS - radial 0.0007 0.0238 0.0020 0.0152 SVM poly - RKHS - radial 0.0024 0.0238 0.0037 0.0170 SVM rbf - RKHS - radial 0.0038 0.0203 0.0046 0.0148 SVM linear - RKHS - poly 0.0142 0.0386 0.0071 0.0215 SVM poly - RKHS - poly 0.0070 0.0488 0.0094 0.0254 SVM rbf - RKHS - poly 0.0127 0.0535 0.0102 0.0232 SVM linear - RKHS - linear 0.0063 0.0442 0.0089 0.0219 SVM poly - RKHS - linear 0.0035 0.0397 0.0058 0.0250 SVM rbf - RKHS - linear 0.0061 0.0426 0.0075 0.0233 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Můžeme také formálně otestovat, zda jsou některé metody úspěšnější než jiné. Vzhledem k nesplnění předpokladu normality nemůžeme použít klasický párový t-test. Využijeme jeho neparametrickou alternativu - Wilcoxonův test. Code wilcox.test(SIMULACE$test[, &#39;RF_Bbasis&#39;], SIMULACE$test[, &#39;RF_discr&#39;], alternative = &#39;less&#39;, paired = T)$p.value ## [1] 0.0005059073 Code wilcox.test(SIMULACE$test[, &#39;RF_Bbasis&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.8449667 Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 12.20: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = &quot;$\\\\widehat{\\\\textnormal{Err}}_{test}$&quot; # y = expression(widehat(Err)[train]) ) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 50, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 3, color = &quot;black&quot;, alpha = 0.9) + scale_x_discrete(labels = methods_names) + theme(plot.margin = unit(c(0.5, 0.5, 2, 2), &quot;cm&quot;)) + coord_cartesian(ylim = c(0, 0.15)) + scale_fill_manual(values = box_col) + scale_alpha_manual(values = box_alpha) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;grey20&#39;, alpha = 0.8) Obrázek 12.21: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # ggsave(&quot;figures/kap7_tecator_box_test_der.tex&quot;, device = tikz, width = 9, height = 7) Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 9.9: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. Mediánová hodnota hyperparametru KNN_K 5.0 nharm 2.0 LR_func_n_basis 6.0 SVM_d_Linear 6.0 SVM_d_Poly 6.0 SVM_d_Radial 6.0 SVM_RKHS_radial_gamma1 0.5 SVM_RKHS_radial_gamma2 0.3 SVM_RKHS_radial_gamma3 0.3 SVM_RKHS_radial_d1 14.0 SVM_RKHS_radial_d2 12.0 SVM_RKHS_radial_d3 10.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 4.0 SVM_RKHS_poly_p3 4.0 SVM_RKHS_poly_d1 5.0 SVM_RKHS_poly_d2 5.5 SVM_RKHS_poly_d3 4.0 SVM_RKHS_linear_d1 21.0 SVM_RKHS_linear_d2 21.0 SVM_RKHS_linear_d3 24.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.22: Histogramy hodnot hyperparametrů pro KNN, funkcionální logistickou regresi a také histogram pro počet hlavních komponent. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.23: Histogramy hodnot hyperparametrů metody SVM s projekcí na B-splinovou bázi. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.24: Histogramy hodnot hyperparametrů metody RKHS + SVM s radiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.25: Histogramy hodnot hyperparametrů metody RKHS + SVM s polynomiálním jádrem. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 12.26: Histogramy hodnot hyperparametrů metody RKHS + SVM s lineárním jádrem. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["aplikace-na-reálných-datech-4.html", "Kapitola 13 Aplikace na reálných datech 4 13.1 handwrit data 13.2 growth data", " Kapitola 13 Aplikace na reálných datech 4 V této části se podíváme podrobněji na klasifikaci pomocí postupu, při kterém nejprve data projektujeme na Reproducing Kernel Hilbert Space definovaný jádrem \\(K\\) a následně pomocí koeficientů této projekce klasifikujeme data pomocí metody SVM. K získání koeficientů projekce se také využívá metoda SVM, avšak její analogie pro řešení regresních preblémů (SVM for regression). Tento postup je podrobněji popsán ve článku9, jehož autory jsou Muñoz a González. Autoři ve svém článku popisují mimo jiné i chování koeficientů z vyjádření pomocí jádrových funkcí (kernel expansion) a zlepšení stability koeficientů pomocí RKHS. Tuto vlastnost si nyní ilustrujme na analogickém příkladě, jako uvádějí autoři. Code library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(patchwork) library(e1071) library(caret) library(kernlab) 13.1 handwrit data Nejprve si načteme data, která představují souřadnice ručně psaných slov fda. Pro naše účely, kdy chceme porovnat dvě různá pozorování, si načtěme pouze první dva záznamy. Code # nacteni dat samples &lt;- c(4, 5) + 10 hand &lt;- handwrit[, samples, ] Pro lepší orientaci si data nejprve vykresleme. Nejprve celkový záznam nápisu fda, následně jednotlivé souřadnice \\(x\\) a \\(y\\) proti času. Code p1 &lt;- ggplot(data = data.frame(x = c(hand[, 1, 1], hand[, 2, 1]), y = c(hand[, 1, 2], hand[, 2, 2]), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = 1401)), aes(x = x, y = y, colour = Sample)) + geom_path() + theme_bw() + labs(x = &#39;X&#39;, y = &#39;Y&#39;) + theme(aspect.ratio = 1, legend.position = &#39;none&#39;) p2 &lt;- ggplot(data = data.frame(x = c(handwritTime, handwritTime), y = c(hand[, 1, 1], hand[, 2, 1]), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = 1401)), aes(x = x, y = y, colour = Sample)) + geom_path() + theme_bw() + labs(x = &#39;Time&#39;, y = &#39;X&#39;) + theme(aspect.ratio = 1, legend.position = &#39;none&#39;) p3 &lt;- ggplot(data = data.frame(x = c(handwritTime, handwritTime), y = c(hand[, 1, 2], hand[, 2, 2]), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = 1401)), aes(x = x, y = y, colour = Sample)) + geom_path() + theme_bw() + labs(x = &#39;Time&#39;, y = &#39;Y&#39;) + theme(aspect.ratio = 1, legend.position = &#39;none&#39;) (p1 | p2 | p3) Nyní se podívejme, jak dopadnou vektory koeficientů \\(\\boldsymbol \\alpha_1\\), \\(\\boldsymbol \\alpha_2\\) z jádrového vyjádření získaného pomocí SVM pro regresi a také jak vypadají \\(\\boldsymbol \\lambda_1\\), \\(\\boldsymbol \\lambda_2\\) pro RKHS reprezentaci. Vektory \\(\\boldsymbol \\alpha_1\\) a \\(\\boldsymbol \\alpha_2\\) definují reprezentaci křivek \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R, l= 1, 2.\\). Podobně vektory \\(\\boldsymbol \\lambda_1\\) a \\(\\boldsymbol \\lambda_2\\) definují vyjádření křivek v bázi RKHS \\(\\mathcal H_K\\) \\[ c_l^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_{jl}^* \\phi_j(\\boldsymbol x), \\] které můžeme odhadnout z dat pomocí \\[ \\hat\\lambda_{jl}^* = \\hat\\lambda_{jl} \\sum_{i = 1}^m \\alpha_{il}\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d. \\] Uvažme Gaussovské jádro s parametrem \\(\\gamma = 0.5\\) (analogicky jako ve výše zmíněném článku). Code gamma &lt;- 0.5 # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;)^2)) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Nyní si spočítejme pomocí funkce svm() z knihovny e1071 vektory koeficientů \\(\\boldsymbol \\alpha_1\\) a \\(\\boldsymbol \\alpha_2\\). Uvědomme si, že funkce svm() vrací koeficienty pouze pro podpůrné vektory, pro ostatní jsou tyto koeficienty nulové. Další možností pro získání koeficientů je využít funkci train() z balíčku caret s volbou method = 'svmRadial'. Syntaxe této funkce je poněkud složitější, proto jsme v celém předchozím postupu uvažovali první zmíněnou funkci. Code # urceni koeficientu alpha z SVM alpha &lt;- matrix(0, nrow = length(handwritTime), ncol = length(samples)) # prazdny objekt # model for(i in 1:length(samples)) { df.svm &lt;- data.frame(x = handwritTime, y = hand[, i, 1]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.03, gamma = gamma, cost = 1, tolerance = 0.001, shrinking = TRUE, scale = TRUE) # alpha[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty svm.RKHS &lt;- train(y ~ x, data = df.svm, method = &#39;svmRadial&#39;, metric = &quot;RMSE&quot;, preProcess = c(&#39;center&#39;, &#39;scale&#39;), # trControl = trainControl( # method = &quot;repeatedcv&quot;, # number = 5, # repeats = 2, # verboseIter = FALSE # ) trControl = trainControl(method = &quot;none&quot;), # Telling caret not to re-tune tuneGrid = data.frame(sigma = 19, C = 1000) # Specifying the parameters ) alpha[svm.RKHS$finalModel@alphaindex, i] &lt;- svm.RKHS$finalModel@alpha * svm.RKHS$finalModel@scaling$y.scale$`scaled:scale` } Vykresleme si pro lepší představu odhad křivky (červeně) společně s pozorovanou křivkou (modře). Code data.frame(x = handwritTime, y = svm.RKHS$finalModel@fitted * svm.RKHS$finalModel@scaling$y.scale$`scaled:scale` + svm.RKHS$finalModel@scaling$y.scale$`scaled:center`, line = &#39;estimate&#39;) |&gt; rbind(data.frame( x = handwritTime, y = hand[, i, 1], line = &#39;sample&#39; )) |&gt; ggplot(aes(x, y, col = line)) + geom_line() + theme_bw() + theme(legend.position = &#39;bottom&#39;) + labs(x = &#39;Time&#39;, y = &#39;X&#39;, col = &#39;Curve&#39;) Obrázek 3.2: Porovnání pozorované a odhadnuté křivky. Podívejme se nyní konečně na hodnoty \\(\\boldsymbol \\alpha_1\\) a \\(\\boldsymbol \\alpha_2\\) pro dvě pozorování. Code data.frame(alpha = c(alpha[, 1], alpha[, 2]), Time = c(handwritTime, handwritTime), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = length(handwritTime))) |&gt; ggplot(aes(xend = Time, x = Time, y = 0, yend = alpha, colour = Sample)) + geom_segment() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(bold(alpha))[i])) Podívejme se nyní na hodnoty \\(\\boldsymbol \\lambda_1\\) a \\(\\boldsymbol \\lambda_2\\) pro dvě pozorování. Code # spocitame matici K K &lt;- Kernel.RKHS(handwritTime, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors Code # d d.RKHS &lt;- rankMM(K) # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(hand)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace Lambda.RKHS[, 1] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha[, 1]) * eig.vals[1:d.RKHS] Lambda.RKHS[, 2] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha[, 2]) * eig.vals[1:d.RKHS] Code data.frame(lambda = c(Lambda.RKHS[, 1], Lambda.RKHS[, 2]), Time = c(handwritTime, handwritTime), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = length(handwritTime))) |&gt; ggplot(aes(xend = Time, x = Time, y = 0, yend = lambda, colour = Sample)) + geom_segment() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(bold(lambda))[i])) + xlim(handwritTime[1:20] |&gt; range()) + ylim(c(-0.01, 0.01)) + facet_wrap(~Sample) Nakonec si vykresleme rozdíly. Code data.frame(lambda = c(Lambda.RKHS[, 1] - Lambda.RKHS[, 2]), Time = c(handwritTime)) |&gt; ggplot(aes(xend = Time, x = Time, y = 0, yend = lambda)) + geom_segment() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(bold(lambda))[1] - widehat(bold(lambda))[2])) + ylim(c(-0.01, 0.01)) + xlim(handwritTime[1:20] |&gt; range()) 13.2 growth data Podívejme se nyní ještě na tutéž analýzu, ale z pohledu dat growth. Code # nacteni dat samples &lt;- c(54, 55) - 4 dataf &lt;- dataf.growth() t1 &lt;- dataf$dataf[[samples[1]]]$args t2 &lt;- dataf$dataf[[samples[2]]]$args y1 &lt;- dataf$dataf[[samples[1]]]$vals y2 &lt;- dataf$dataf[[samples[2]]]$vals data.gr &lt;- data.frame(t = t1, H1 = y1, H2 = y2) Code gamma &lt;- 0.1 # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;)^2)) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Nyní si spočítejme pomocí funkce svm() z knihovny e1071 vektory koeficientů \\(\\boldsymbol \\alpha_1\\) a \\(\\boldsymbol \\alpha_2\\). Uvědomme si, že funkce svm() vrací koeficienty pouze pro podpůrné vektory, pro ostatní jsou tyto koeficienty nulové. Code # urceni koeficientu alpha z SVM alpha &lt;- matrix(0, nrow = length(data.gr$t), ncol = 2) # prazdny objekt # model for(i in 1:2) { df.svm &lt;- data.frame(x = data.gr$t, y = data.gr[, i + 1]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.5, gamma = gamma, cost = 100000, tolerance = 0.001, shrinking = FALSE, scale = TRUE) # svm.RKHS &lt;- train(y ~ x, data = df.svm, # method = &#39;svmLinear&#39;) # urceni alpha # b &lt;- - svm.RKHS$rho # rho ... the negative intercept # betas &lt;- svm.RKHS$coefs # alphas &lt;- betas + b #/ Gauss.kernel() alpha[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Code data.frame(alpha = c(alpha[, 1], alpha[, 2]), Time = c(data.gr$t, data.gr$t), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = length(data.gr$t))) |&gt; ggplot(aes(xend = Time, x = Time, y = 0, yend = alpha, colour = Sample)) + geom_segment() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(bold(alpha))[i])) Podívejme se nyní na hodnoty \\(\\boldsymbol \\lambda_1\\) a \\(\\boldsymbol \\lambda_2\\) pro dvě pozorování. Code # spocitame matici K K &lt;- Kernel.RKHS(data.gr$t, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors Code # d d.RKHS &lt;- rankMM(K) # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = 2, nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace Lambda.RKHS[, 1] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha[, 1]) * eig.vals[1:d.RKHS] Lambda.RKHS[, 2] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha[, 2]) * eig.vals[1:d.RKHS] Code data.frame(lambda = c(Lambda.RKHS[, 1], Lambda.RKHS[, 2]), Time = c(1:d.RKHS, 1:d.RKHS), Sample = rep(c(&#39;A&#39;, &#39;B&#39;), each = d.RKHS)) |&gt; ggplot(aes(xend = Time, x = Time, y = 0, yend = lambda, colour = Sample)) + geom_segment() + theme_bw() + labs(x = &#39;Index&#39;, y = expression(widehat(bold(lambda))[i])) + facet_wrap(~Sample) Nakonec si vykresleme rozdíly. Code data.frame(lambda = c(Lambda.RKHS[, 1] - Lambda.RKHS[, 2]), Time = c(1:d.RKHS)) |&gt; ggplot(aes(xend = Time, x = Time, y = 0, yend = lambda)) + geom_segment() + theme_bw() + labs(x = &#39;Index&#39;, y = expression(widehat(bold(lambda))[1] - widehat(bold(lambda))[2])) + ylim(c(-5, 3)) Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["podpůrné-materiály-pro-diplomovou-práci.html", "Kapitola 14 Podpůrné materiály pro diplomovou práci 14.1 Materiály pro Kapitolu 1 14.2 Materiály pro Kapitolu 2 14.3 Materiály pro Kapitolu 3 14.4 Materiály pro Kapitolu 4 14.5 Materiály pro Kapitolu 5 14.6 Materiály pro Kapitolu 6 14.7 Materiály pro Kapitolu 7", " Kapitola 14 Podpůrné materiály pro diplomovou práci V této poslední kapitole jsou uvedeny zdrojové kódy pro vygenerování grafů a dalších případných materiálů, které jsou použity v diplomové práci. Jedná se především o ilustrativní grafy určitých vlastností a fenoménů spojených s funkcionálními daty. Kapitola je členěna do sekcí, které odpovídají jednotlivým kapitolám v diplomové práci. Všechny grafy jsou vytvořeny pomocí balíčku ggplot2, který poskytuje celou řadu grafických funkcionalit, pomocí kterých jsme (alespoň subjektivně) schopni dosáhnout podstatně lépe a profesionálněji vypadajících grafických výstupů v porovnání s klasickou grafikou v R. Všechny grafy jsou uloženy pomocí funkce ggsave() ve formátu pdf nebo tikz, který umožňuje lepší kombinaci grafiky a symbolů v \\(\\LaTeX\\)u. Code # nacteme potrebne balicky library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(tidyverse) library(patchwork) library(tikzDevice) set.seed(42) options(tz = &quot;UTC&quot;) # nacteni dat data &lt;- read.delim2(&#39;phoneme.txt&#39;, header = T, sep = &#39;,&#39;) # zmenime dve promenne na typ factor data &lt;- data |&gt; mutate(g = factor(g), speaker = factor(speaker)) # numericke promenne prevedeme opravdu na numericke data[, 2:257] &lt;- as.numeric(data[, 2:257] |&gt; as.matrix()) tr_vs_test &lt;- str_split(data$speaker, &#39;\\\\.&#39;) |&gt; unlist() tr_vs_test &lt;- tr_vs_test[seq(1, length(tr_vs_test), by = 4)] data$train &lt;- ifelse(tr_vs_test == &#39;train&#39;, TRUE, FALSE) # vybrane fonemy ke klasifikaci phoneme_subset &lt;- c(&#39;aa&#39;, &#39;ao&#39;) # testovaci a trenovaci data data_train &lt;- data |&gt; filter(train) |&gt; filter(g %in% phoneme_subset) data_test &lt;- data |&gt; filter(!train) |&gt; filter(g %in% phoneme_subset) # odstranime sloupce, ktere nenesou informaci o frekvenci a # transponujeme tak, aby ve sloupcich byly jednotlive zaznamy X_train &lt;- data_train[, -c(1, 258, 259, 260)] |&gt; t() X_test &lt;- data_test[, -c(1, 258, 259, 260)] |&gt; t() # prejmenujeme radky a sloupce rownames(X_train) &lt;- 1:256 colnames(X_train) &lt;- paste0(&#39;train&#39;, data_train$row.names) rownames(X_test) &lt;- 1:256 colnames(X_test) &lt;- paste0(&#39;test&#39;, data_test$row.names) # definujeme vektor fonemu y_train &lt;- data_train[, 258] |&gt; factor(levels = phoneme_subset) y_test &lt;- data_test[, 258] |&gt; factor(levels = phoneme_subset) y &lt;- c(y_train, y_test) 14.1 Materiály pro Kapitolu 1 V této sekci uvedeme podpůrné grafy pro první kapitolu diplomové práce. 14.1.1 Funkcionální průměr Pro data phoneme spočítáme průměrný průběh log-periodogramů. Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() XXaa &lt;- XX[, y == phoneme_subset[1]] lambda.vect &lt;- 10^seq(from = 1, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) ## pouze pro aa lambda.vect &lt;- 10^seq(from = 1, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmoothaa &lt;- smooth.basis(t, XXaa, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmoothaa$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmoothaa &lt;- smooth.basis(t, XXaa, curv.fdPar) XXfdaa &lt;- BSmoothaa$fd fdobjSmoothevalaa &lt;- eval.fd(fdobj = XXfdaa, evalarg = t) # prumer meanfd &lt;- mean.fd(XXfdaa) fdmean &lt;- eval.fd(fdobj = meanfd, evalarg = t) Code n &lt;- dim(XX)[2] DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Phoneme = rep(y, each = length(t))) |&gt; filter(Phoneme == &#39;aa&#39;) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(fdmean, fdmean), Phoneme = factor(rep(phoneme_subset, each = length(t)), levels = levels(y)) ) |&gt; filter(Phoneme == &#39;aa&#39;) # tikz(file = &quot;figures/DP_kap1_mean.tex&quot;, width = 4.2, height = 3.5) DFsmooth |&gt; filter(time %in% as.character(1:100)) |&gt; ggplot(aes(x = t, y = Smooth)) + geom_line(aes(group = time), linewidth = 0.2, colour = &#39;deepskyblue2&#39;, alpha = 0.6) + theme_bw() + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Phoneme&#39;) + scale_colour_discrete(labels = phoneme_subset) + geom_line(data = DFmean, aes(x = t, y = Mean, group = Phoneme), linewidth = 1, linetype = &#39;solid&#39;, colour = &#39;grey2&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 1.1: Vykreslení prvních 100 vyhlazených pozorovaných křivek. Černou čarou je zakreslen průměr. Code # dev.off() # ggsave(&quot;figures/DP_kap1_mean.pdf&quot;, width = 6, height = 5) 14.1.2 Variance Pro data phoneme spočítáme průběh varianční funkce. Code varfd &lt;- var.fd(XXfdaa) fdvar &lt;- eval.bifd(t, t, varfd) Code dfs &lt;- data.frame( time = t, value = c(fdobjSmoothevalaa)) df &lt;- data.frame(dfs, fdmean = fdmean, fdvar = diag(fdvar)) # tikz(file = &quot;figures/DP_kap1_variance.tex&quot;, width = 6, height = 5) # df &lt;- df[seq(1, length(df$time), length = 1001), ] ggplot(data = df, aes(x = time, y = fdvar)) + geom_line(color = &#39;deepskyblue2&#39;, linewidth = 0.8) + labs(x = &#39;Frekvence&#39;, y = &#39;Variance&#39;, colour = &#39;Phoneme&#39;) + theme_bw() Obrázek 3.1: Varianční funkce. Code # dev.off() # ggsave(&quot;figures/DP_kap1_variance.tex&quot;, width = 4.2, height = 3.5, device = tikz) 14.1.3 Kovariance a Korelace Pro data phoneme spočítáme kovarianční a korelační funkce. Code fdcor &lt;- cor.fd(t, XXfdaa) df &lt;- merge(t, t) df &lt;- data.frame(df, fdcov = c(fdvar), fdcor = c(fdcor)) df &lt;- df[seq(1, length(df$x), length = 68001), ] # tikz(file = &quot;figures/DP_kap1_cov.tex&quot;, width = 4, height = 4) p1 &lt;- ggplot(data = df, aes (x, y, z = fdcov)) + geom_raster(aes(fill = fdcov)) + geom_contour(colour = &quot;white&quot;) + labs(x = &#39;Frekvence&#39;, y = &#39;Frekvence&#39;, fill = &#39;Kovariance&#39;) + coord_fixed(ratio = 1) + theme_bw() + theme(legend.position = &#39;bottom&#39;) + scale_y_continuous(expand = c(0,0) + 0.01) + scale_x_continuous(expand = c(0,0) + 0.01) p1 Code # dev.off() # ggsave(&quot;figures/DP_kap1_cov.tex&quot;, width = 6, height = 6, device = tikz) # tikz(file = &quot;figures/DP_kap1_cor.tex&quot;, width = 4, height = 4) p2 &lt;- ggplot(data = df, aes (x, y, z = fdcor)) + geom_raster(aes(fill = fdcor)) + geom_contour(colour = &quot;white&quot;) + labs(x = &#39;Frekvence&#39;, y = &#39;Frekvence&#39;, fill = &#39;Korelace&#39;) + coord_fixed(ratio = 1) + theme_bw() + theme(legend.position = &#39;bottom&#39;) + scale_y_continuous(expand = c(0,0) + 0.01) + scale_x_continuous(expand = c(0,0) + 0.01) p2 Code # dev.off() # ggsave(&quot;figures/DP_kap1_cor.tex&quot;, width = 6, height = 6, device = tikz) 14.1.4 B-splinová báze Podívejme se na princip, jak se pomocí splinové báze dostaneme od diskrétních naměřených hodnot k funkcionálním datům. Uvažujme pro přehlednost opět data phoneme a pouze malý počet bázových funkcí. Uvedeme tři obrázky, jeden se znázorněnými bázovými funkcemi, druhý s bázovými funkcemi přenásobenými vypočtenou hodnotou parametru a třetí výslednou křivku poskládanou sečtením jednotlivých přeškálovaných bázových funkcí. Code # definice barev cols7 &lt;- c(&quot;#12DEE8&quot;, &quot;#4ECBF3&quot;, &quot;#127DE8&quot;, &quot;#4C3CD3&quot;, &quot;#4E65F3&quot;, &quot;#4E9EF3&quot;, &quot;#081D58&quot;) cols5 &lt;- c(&quot;#12DEE8&quot;, &quot;#1D89BC&quot;, &quot;#4E9EF3&quot;, &quot;#4C3CD3&quot;, &quot;#081D58&quot;) 14.1.4.1 pro norder = 2 Code df &lt;- data.frame(x = 1:256, y = data[5, 2:257] |&gt; c() |&gt; unlist()) breaks &lt;- quantile(df$x, probs = seq(0.1, 0.9, by = 0.2)) norder &lt;- 2 rangeval &lt;- range(df$x) bbasis &lt;- create.bspline.basis(rangeval, norder = norder, breaks = breaks) BSmooth &lt;- smooth.basis(df$x, df$y, bbasis) Code fdBSmootheval &lt;- eval.fd(fdobj = BSmooth$fd, evalarg = df$x) fdB &lt;- eval.basis(basisobj = bbasis, evalarg = df$x) basisdf1 &lt;- data.frame(bs = c(fdB), x = df$x, basis = rep(colnames(fdB), each = length(df$x))) ebchan &lt;- fdB * matrix(rep(BSmooth$fd$coefs, each = length(df$x)), nrow = length(df$x)) basisdf2 &lt;- data.frame(bs = c(ebchan), x = df$x, basis = rep(colnames(fdB), each = length(df$x))) library(RColorBrewer) # tikz(file = &quot;figures/DP_kap1_Bbasis_norder2.tex&quot;, width = 9, height = 3) # samotna baze p1 &lt;- ggplot(data = basisdf1, aes(x = x, y = bs * 10, colour = basis)) + geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey2&#39;) + geom_line() + labs(x = &#39;Frekvence&#39;, y = &#39;B-splajnová báze&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + ylim(c(0, 22)) + # scale_color_brewer(palette = &#39;Blues&#39;) + # scale_color_manual(values = colorRampPalette(brewer.pal(9, &quot;YlGnBu&quot;))(12)[c(6,8,9,10,12)]) scale_color_manual(values = cols5) # prenasobena koeficienty p2 &lt;- ggplot(data = basisdf2, aes(x = x, y = bs, colour = basis)) + geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey2&#39;) + geom_line() + labs(x = &#39;Frekvence&#39;, y = &#39;B-splajnová báze (škálovaná)&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + ylim(c(0, 22)) + # scale_color_brewer() # scale_color_manual(values = colorRampPalette(brewer.pal(9, &quot;YlGnBu&quot;))(12)[c(6,8,9,10,12)]) scale_color_manual(values = cols5) # vyhlazena data p3 &lt;- ggplot(data = df, aes(x, y)) + geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey2&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval)) + theme_classic() + #guides (colour = FALSE) + ylim(c(0, 22)) + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) (p1 | p2 | p3) Obrázek 1.3: B-spliny. Code # dev.off() # ggsave(&#39;figures/DP_kap1_Bbasis_norder2.tex&#39;, device = tikz) 14.1.4.2 pro norder = 4 Code df &lt;- data.frame(x = 1:256, y = data[5, 2:257] |&gt; c() |&gt; unlist()) breaks &lt;- quantile(df$x, probs = seq(0.1, 0.9, by = 0.2)) norder &lt;- 4 rangeval &lt;- range(df$x) bbasis &lt;- create.bspline.basis (rangeval, norder = norder, breaks = breaks) BSmooth &lt;- smooth.basis(df$x, df$y, bbasis) Code fdBSmootheval &lt;- eval.fd(fdobj = BSmooth$fd, evalarg = df$x) fdB &lt;- eval.basis(basisobj = bbasis, evalarg = df$x) basisdf1 &lt;- data.frame(bs = c(fdB), x = df$x, basis = rep(colnames(fdB), each = length(df$x))) ebchan &lt;- fdB * matrix(rep(BSmooth$fd$coefs, each = length(df$x)), nrow = length(df$x)) basisdf2 &lt;- data.frame(bs = c(ebchan), x = df$x, basis = rep(colnames(fdB), each = length(df$x))) # tikz(file = &quot;figures/DP_kap1_Bbasis_norder4.tex&quot;, width = 9, height = 3) # samotna baze p1 &lt;- ggplot(data = basisdf1, aes(x = x, y = bs * 10, colour = basis)) + geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey2&#39;) + geom_line() + labs(x = &#39;Frekvence&#39;, y = &#39;B-splajnová báze&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + ylim(c(0, 22)) + # scale_color_brewer(palette = &#39;Blues&#39;) + # scale_color_manual(values = colorRampPalette(brewer.pal(9, &quot;YlGnBu&quot;))(12)[c(6,8,9,10,12)]) scale_color_manual(values = cols7) # prenasobena koeficienty p2 &lt;- ggplot(data = basisdf2, aes(x = x, y = bs, colour = basis)) + geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey2&#39;) + geom_line() + labs(x = &#39;Frekvence&#39;, y = &#39;B-splajnová báze (škálovaná)&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + ylim(c(0, 22)) + # scale_color_brewer() # scale_color_manual(values = colorRampPalette(brewer.pal(9, &quot;YlGnBu&quot;))(12)[c(6,8,9,10,12)]) scale_color_manual(values = cols7) # vyhlazena data p3 &lt;- ggplot(data = df, aes(x, y)) + geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey2&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval)) + theme_classic() + #guides (colour = FALSE) + ylim(c(0, 22)) + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) (p1 | p2 | p3) Obrázek 1.5: B-spliny. Code # dev.off() #ggsave(&#39;figures/DP_kap1_Bbasis_norder4.pdf&#39;) 14.1.5 Fourierova báze Podívejme se na princip, jak se pomocí Fourierovské báze dostaneme od diskrétních naměřených hodnot k funkcionálním datům. Uvažujme pro přehlednost opět data phoneme a pouze malý počet bázových funkcí. Uvedeme tři obrázky, jeden se znázorněnými bázovými funkcemi, druhý s bázovými funkcemi přenásobenými vypočtenou hodnotou parametru a třetí výslednou křivku poskládanou sečtením jednotlivých přeškálovaných bázových funkcí. 14.1.5.1 pro nbasis = 5 Code df &lt;- data.frame(x = 1:256, y = data[5, 2:257] |&gt; c() |&gt; unlist()) nbasis &lt;- 5 rangeval &lt;- range(df$x) fbasis &lt;- create.fourier.basis(rangeval, nbasis = nbasis, period = 256) FSmooth &lt;- smooth.basis(df$x, df$y, fbasis) Code fdBSmootheval &lt;- eval.fd(fdobj = FSmooth$fd, evalarg = df$x) fdF &lt;- eval.basis(basisobj = fbasis, evalarg = df$x) basisdf1 &lt;- data.frame(bs = c(fdF), x = df$x, basis = rep(colnames(fdF), each = length(df$x))) ebchan &lt;- fdF * matrix(rep(FSmooth$fd$coefs, each = length(df$x)), nrow = length(df$x)) basisdf2 &lt;- data.frame(bs = c(ebchan), x = df$x, basis = rep(colnames(fdF), each = length(df$x))) # tikz(file = &quot;figures/DP_kap1_Fbasis_nbasis5.tex&quot;, width = 9, height = 3) # samotna baze p1 &lt;- ggplot(data = basisdf1, aes(x = x, y = bs, colour = basis)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_line() + labs(x = &#39;Frekvence&#39;, y = &#39;Fourierova báze&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + #ylim(c(0, 22)) + # scale_color_brewer(palette = &#39;Blues&#39;) scale_color_manual(values = cols5) # prenasobena koeficienty p2 &lt;- ggplot(data = basisdf2, aes(x = x, y = bs, colour = basis)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_line() + labs(x = &#39;Frekvence&#39;, y = &#39;Fourierova báze (škálovaná)&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + #ylim(c(0, 22)) + # scale_color_brewer() scale_color_manual(values = cols5) # vyhlazena data p3 &lt;- ggplot(data = df, aes(x, y)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval)) + theme_classic() + #guides (colour = FALSE) + ylim(c(0, 22)) + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) (p1 | p2 | p3) Obrázek 2.2: Fourierova baze. Code # dev.off() #ggsave(&#39;figures/DP_kap1_Fbasis_nbasis5.pdf&#39;) 14.1.5.2 pro nbasis = 7 Code df &lt;- data.frame(x = 1:256, y = data[5, 2:257] |&gt; c() |&gt; unlist()) nbasis &lt;- 7 rangeval &lt;- range(df$x) fbasis &lt;- create.fourier.basis(rangeval, nbasis = nbasis, period = 256) FSmooth &lt;- smooth.basis(df$x, df$y, fbasis) Code fdBSmootheval &lt;- eval.fd(fdobj = FSmooth$fd, evalarg = df$x) fdF &lt;- eval.basis(basisobj = fbasis, evalarg = df$x) basisdf1 &lt;- data.frame(bs = c(fdF), x = df$x, basis = rep(colnames(fdF), each = length(df$x))) ebchan &lt;- fdF * matrix(rep(FSmooth$fd$coefs, each = length(df$x)), nrow = length(df$x)) basisdf2 &lt;- data.frame(bs = c(ebchan), x = df$x, basis = rep(colnames(fdF), each = length(df$x))) # tikz(file = &quot;figures/DP_kap1_Fbasis_nbasis7.tex&quot;, width = 12, height = 4) # samotna baze p1 &lt;- ggplot(data = basisdf1, aes(x = x, y = bs, colour = basis)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_line() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Fourierova báze&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + #ylim(c(0, 22)) + # scale_color_brewer(palette = &#39;Blues&#39;) scale_color_manual(values = cols7) # prenasobena koeficienty p2 &lt;- ggplot(data = basisdf2, aes(x = x, y = bs, colour = basis)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_line() + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Fourierova báze (škálovaná)&#39;, colour = &#39;Foném&#39;) + theme_classic() + guides(colour = FALSE) + #ylim(c(0, 22)) + # scale_color_brewer() scale_color_manual(values = cols7) # vyhlazena data p3 &lt;- ggplot(data = df, aes(x, y)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval)) + theme_classic() + #guides (colour = FALSE) + ylim(c(0, 22)) + labs(x = &#39;Frekvence [Hz]&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) (p1 | p2 | p3) Obrázek 6.1: Fourierova baze. Code # dev.off() #ggsave(&#39;figures/DP_kap1_Fbasis_nbasis7.pdf&#39;) 14.2 Materiály pro Kapitolu 2 Ve druhé sekci se podíváme na materiály pro Kapitolu 2 diplomové práce. Bude nás zajímat vliv vyhlazovacího parametru \\(\\lambda\\) na výslednou odhadnutou křivku z diskrétních dat. Dále se podíváme na funkcionální analýzu hlavních komponent. Nejprve se ale podívejme na vliv počtu bázových funkcí na výsledný odhad funkce. 14.2.1 Počet bázových funkcí a výsledný odhad Code df &lt;- data.frame(x = 1:256, y = data[5, 2:257] |&gt; c() |&gt; unlist()) norder &lt;- 4 rangeval &lt;- range(df$x) bbasis1 &lt;- create.bspline.basis (rangeval, norder = norder, nbasis = 5) BSmooth1 &lt;- smooth.basis(df$x, df$y, bbasis1) fdBSmootheval1 &lt;- eval.fd(fdobj = BSmooth1$fd, evalarg = df$x) bbasis2 &lt;- create.bspline.basis (rangeval, norder = norder, nbasis = 15) BSmooth2 &lt;- smooth.basis(df$x, df$y, bbasis2) fdBSmootheval2 &lt;- eval.fd(fdobj = BSmooth2$fd, evalarg = df$x) bbasis3 &lt;- create.bspline.basis (rangeval, norder = norder, nbasis = 25) BSmooth3 &lt;- smooth.basis(df$x, df$y, bbasis3) fdBSmootheval3 &lt;- eval.fd(fdobj = BSmooth3$fd, evalarg = df$x) # 10 bazovych funkci p1 &lt;- ggplot(data = df, aes(x, y)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval1), linewidth = 0.7) + theme_classic() + ylim(c(0, 22)) + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) # 10 bazovych funkci p2 &lt;- ggplot(data = df, aes(x, y)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval2), linewidth = 0.7) + theme_classic() + ylim(c(0, 22)) + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) # 10 bazovych funkci p3 &lt;- ggplot(data = df, aes(x, y)) + # geom_vline(xintercept = breaks, linetype = &quot;dotted&quot;, linewidth = 0.1, colour = &#39;grey&#39;) + geom_point(colour = &#39;deepskyblue2&#39;, size = 0.8, alpha = 0.75) + geom_line(aes(y = fdBSmootheval3), linewidth = 0.7) + theme_classic() + ylim(c(0, 22)) + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) # tikz(file = &quot;figures/DP_kap2_differentnbasis.tex&quot;, width = 12, height = 4) (p1 | p2 | p3) Obrázek 5.2: Počet bázových funkcí a výsledný odhad. Code # dev.off() ggsave(&#39;figures/DP_kap2_differentnbasis.tex&#39;, width = 9, height = 3, device = tikz) 14.2.2 Volba \\(\\lambda\\) Začněme volbou vyhlazovacího parametru \\(\\lambda &gt; 0\\). S rostoucí hodnotou \\(\\lambda\\) dáváme v penalizované sumě čtverců \\[ SS_{pen} = (\\boldsymbol y - \\boldsymbol B \\boldsymbol c)^\\top (\\boldsymbol y - \\boldsymbol B \\boldsymbol c) + \\lambda \\boldsymbol c^\\top \\boldsymbol R \\boldsymbol c \\] větší váhu penalizačnímu členu, tedy dostaneme více penalizované, více hladké křivky blížící se lineární funkci. Vykreslíme si obrázky, ve kterých bude zřejmé, jak se s měnící se hodnotou \\(\\lambda\\) mění výsledná vyhlazená křivka. Ke znázornění tohoto chování použijeme data phoneme z jedné z předchozích kapitol. Vybereme jedno zajímavé pozorování a ukážeme na něm toto chování. Za uzly bereme celý vektor frekvencí (1 až 256 Hz), standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() Zvolme nyní nějakých 6 hodnot pro vyhlazovací parametr \\(\\lambda\\) a spočítejme vyhlazené křivky pro jeden vybraný záznam. Code lambdas &lt;- c(0.01, 0.1, 50, 500, 10000, 1000000) # vektor lambd tt &lt;- seq(min(t), max(t), length = 1001) # objekt, do ktereho ulozime hodnoty res_plot &lt;- matrix(NA, ncol = length(lambdas), nrow = length(tt)) for(i in 1:length(lambdas)) { curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambdas[i]) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = tt)[, 1] res_plot[, i] &lt;- fdobjSmootheval } Code options(scipen = 999) library(scales) lam_labs &lt;- paste0(&#39;$\\\\lambda = &#39;, lambdas, &quot;$&quot;) names(lam_labs) &lt;- lambdas # tikz(file = &quot;figures/DP_kap2_lambdas.tex&quot;, width = 9, height = 6) data.frame(time = rep(tt, length(lambdas)), value = c(res_plot), lambda = rep(lambdas, each = length(tt))) |&gt; # mutate(lambda = factor(lambda)) |&gt; ggplot(aes(x = time, y = value)) + geom_point(data = data.frame(time = rep(t, length(lambdas)), value = rep(c(data[5, 2:257]) |&gt; unlist(), length(lambdas)), lambda = rep(lambdas, each = length(t))) , alpha = 0.5, size = 0.75, colour = &quot;deepskyblue2&quot;) + geom_line(linewidth = 0.7, colour = &quot;grey2&quot;) + facet_wrap(~lambda, ncol = 3, nrow = 2, labeller = labeller(lambda = lam_labs)) + theme_bw() + theme(legend.position = &#39;none&#39;) + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) Obrázek 5.4: Log-periodogram vybraného fonému pro zvolené hodnoty vyhlazovacího parametru. Code # scale_color_brewer() # dev.off() # ggsave(&#39;figures/DP_kap2_lambdas.pdf&#39;) 14.2.3 Vyhlazení s optimální \\(\\lambda\\) V Kapitole Aplikace na reálných datech 2 jsme zjistili optimální hodnotu vyhlazovacího parametru. Tu nyní použijeme. Code lambdas &lt;- c(175.75) tt &lt;- seq(min(t), max(t), length = 1001) # objekt, do ktereho ulozime hodnoty res_plot &lt;- matrix(NA, ncol = length(lambdas), nrow = length(tt)) for(i in 1:length(lambdas)) { curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambdas[i]) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = tt)[, 1] res_plot[, i] &lt;- fdobjSmootheval } Code options(scipen = 999) library(scales) lam_labs &lt;- paste0(&#39;$\\\\lambda = &#39;, lambdas, &quot;$&quot;) names(lam_labs) &lt;- lambdas # tikz(file = &quot;figures/DP_kap2_optimal_lambda.tex&quot;, width = 6, height = 4) data.frame(time = rep(tt, length(lambdas)), value = c(res_plot), lambda = rep(lambdas, each = length(tt))) |&gt; # mutate(lambda = factor(lambda)) |&gt; ggplot(aes(x = time, y = value)) + geom_point(data = data.frame(time = rep(t, length(lambdas)), value = rep(c(data[5, 2:257]) |&gt; unlist(), length(lambdas)), lambda = rep(lambdas, each = length(t))) , alpha = 0.5, size = 0.75, colour = &quot;deepskyblue2&quot;) + geom_line(linewidth = 0.7, colour = &quot;grey2&quot;) + # facet_wrap(~lambda, ncol = 3, nrow = 2, labeller = labeller(lambda = lam_labs)) + theme_bw() + theme(legend.position = &#39;none&#39;) + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;) Obrázek 2.4: Log-periodogram vybraného fonému pro zvolené hodnoty vyhlazovacího parametru. Code # scale_color_brewer() # dev.off() # ggsave(&#39;figures/DP_kap2_lambdas.pdf&#39;) 14.2.4 Funkcionální PCA Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Code # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = 1, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(XXfd, nharm = 20) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p # data.PCA &lt;- pca.fd(XXfd, nharm = 20) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(y) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 9, které dohromady vysvětlují 90.47 % variability v datech. První hlavní komponenta potom vysvětluje 44.79 % a druhá 13.37 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code p1 &lt;- data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.2, alpha = 0.75) + labs(x = paste(&#39;1. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[1], 2), &#39;\\\\%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;\\\\%)&#39;), colour = &#39;Foném&#39;) + # scale_colour_discrete(labels = phoneme_subset) + theme_bw() + theme(legend.position = &#39;none&#39;) + lims(x = c(-70, 62), y = c(-70, 62)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) p2 &lt;- data.PCA.train |&gt; ggplot(aes(x = V1, y = V3, colour = Y)) + geom_point(size = 1.2, alpha = 0.75) + labs(x = paste(&#39;1. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[1], 2), &#39;\\\\%)&#39;), y = paste(&#39;3. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[3], 2), &#39;\\\\%)&#39;), colour = &#39;Foném&#39;) + # scale_colour_discrete(labels = phoneme_subset) + theme_bw() + theme(legend.position = &#39;none&#39;) + lims(x = c(-70, 62), y = c(-70, 62)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) p3 &lt;- data.PCA.train |&gt; ggplot(aes(x = V2, y = V3, colour = Y)) + geom_point(size = 1.2, alpha = 0.75) + labs(x = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;\\\\%)&#39;), y = paste(&#39;3. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[3], 2), &#39;\\\\%)&#39;), colour = &#39;Foném&#39;) + # scale_colour_discrete(labels = phoneme_subset) + theme_bw() + lims(x = c(-70, 62), y = c(-70, 62)) + theme(legend.position = c(0.84, 0.84)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) (p1|p2|p3) Obrázek 5.5: Hodnoty skórů prvních tří hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Code # tikz(file = &quot;figures/kap2_PCA_scores1.tex&quot;, width = 3, height = 3) # p1 # dev.off() # tikz(file = &quot;figures/kap2_PCA_scores2.tex&quot;, width = 3, height = 3) # p2 # dev.off() # tikz(file = &quot;figures/kap2_PCA_scores3.tex&quot;, width = 3, height = 3) # p3 # dev.off() # ggsave(&quot;figures/kap2_PCA_scores.tex&quot;, device = tikz, width = 12, height = 4) Podívejme se ještě na 3D graf skórů prvních třech hlavních komponent. Code # 3D plot library(plotly) plot_ly(data = data.PCA.train, x = ~V1, y = ~V2, z = ~V3, color = ~Y, colors = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), type=&quot;scatter3d&quot;, mode=&quot;markers&quot;, marker = list(size = 2.5)) %&gt;% layout(scene = list(xaxis = list(title = &#39;1. hlavní komponenta&#39;), yaxis = list(title = &#39;2. hlavní komponenta&#39;), zaxis = list(title = &#39;3. hlavní komponenta&#39;))) Obrázek 3.5: 3D graf skórů prvních třech hlavních komponent. Vykresleme si i průběh kumulativní vysvětlené variability. Code data.frame(x = 1:15, y = pca.fd(XXfd, nharm = 15)$varprop |&gt; cumsum() * 100) |&gt; ggplot(aes(x, y)) + geom_point(col = &#39;deepskyblue2&#39;) + geom_line(col = &#39;deepskyblue2&#39;) + theme_bw() + labs(x = &#39;Počet hlavních komponent&#39;, y = &quot;Kumulativní vysvětlená variabilita [v \\\\%]&quot;) + geom_hline(aes(yintercept = 90), linetype = &#39;dashed&#39;, col = &#39;grey2&#39;) + scale_x_continuous(breaks = 1:15) + theme(panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank()) Obrázek 2.5: Kumulativní vysvětlená variabilita [v %] proti počtu hlavních komponent. Code # ggsave(&quot;figures/kap2_PCA_nharm.tex&quot;, device = tikz, width = 5, height = 3) Také nás zajímá průběh prvních třech funkcionálních hlavních komponent. Code ## Looking at the principal components: fdobjPCAeval &lt;- eval.fd(fdobj = data.PCA$harmonics[1:3], evalarg = t) df.comp &lt;- data.frame( time = t, harmonics = c(fdobjPCAeval), component = factor(rep(1:3, each = 256)) ) df.comp |&gt; ggplot(aes(x = time, y = harmonics, color = component)) + geom_line(linewidth = 0.7) + labs(x = &quot;Frekvence&quot;, y = &quot;Hlavní komponenty&quot;, colour = &#39;&#39;) + theme_bw() + scale_color_manual(values=c(&quot;#127DE8&quot;, &quot;#4C3CD3&quot;, &quot;#12DEE8&quot;)) Obrázek 7.2: Hlavní komponenty. Code # ggsave(&quot;figures/kap2_PCA_components.tex&quot;, device = tikz, width = 5, height = 3) Můžeme se také podívat na vliv prvních třech hlavních komponent na průměrnou křivku. Vždy je od průměru přičten nebo odečten dvojnásobek hlavní komponenty (škálovaný rozptylem). Code library(plyr) freq3 &lt;- seq(1,256) fdobjPCAeval3 &lt;- eval.fd(fdobj = data.PCA$harmonics[1:3], evalarg = freq3) df.comp3 &lt;- data.frame( freq = freq3, harmonics = c(fdobjPCAeval3), component = factor(rep(1:3, each = length(freq3))) ) fdm3 &lt;- c(eval.fd(fdobj = meanfd, evalarg = freq3)) df.3 &lt;- data.frame(df.comp3, m = fdm3) df.pv3 &lt;- ddply(df.3, .(component), mutate, m1 = m + 2*sqrt(data.PCA$values[component])*harmonics, m2 = m - 2*sqrt(data.PCA$values[component])*harmonics, pov = paste0(&quot;Komponenta &quot;, component,&quot;, Vysvětlená variabilita = &quot;, round(100*data.PCA$varprop[component], 1), &#39; \\\\%&#39;)) df.pv3 |&gt; ggplot(aes (x = freq, y = m)) + geom_line() + geom_line(aes(y = m1), linetype = &#39;solid&#39;, color = &#39;deepskyblue2&#39;) + geom_line(aes(y = m2), linetype = &#39;dashed&#39;, color = &#39;deepskyblue2&#39;) + labs(x = &quot;Frekvence&quot;, y = &quot;Log-periodogram&quot;, colour = &#39;Komponenta&#39;) + theme_bw() + theme(legend.position = &#39;none&#39;) + facet_wrap(~ pov, nrow = 1) Obrázek 5.6: Vliv komponent. Code # ggsave(&quot;figures/kap2_PCA_impactofcomponents.tex&quot;, device = tikz, width = 9, height = 3) Podívejme se ještě na rekonstrukci původního log-periodogramu pomocí hlavních komponent. Nejprve uvažujme pouze průměr. Code meanfd &lt;- mean.fd(XXfd) fdm &lt;- eval.fd(fdobj = meanfd, evalarg = t) colnames(fdm) &lt;- NULL scores &lt;- data.PCA$scores PCs &lt;- eval.fd(fdobj = data.PCA$harmonics, evalarg = t) # vyhodnoceni df &lt;- data.frame(dfs[1:256, ], reconstruction = fdm, estimate = &quot;mean&quot;) p0 &lt;- ggplot(data = df, aes (x = time, y = value)) + geom_line(color = &quot;grey2&quot;, linewidth = 0.5, alpha = 0.7) + geom_line(aes(y = reconstruction), colour = &quot;deepskyblue2&quot;, linewidth = 0.6) + labs(x = &quot;Frekvence&quot;, y = &quot;Log-periodogram&quot;) + theme_bw() print(p0) Obrázek 1.9: Původní křivka a průměr. Konečně se podívejme na vybraný počet hlavních komponent a příslušnou rekonstrukci. Code for (k in 1:20){ df1 &lt;- data.frame(dfs[1:256, ], reconstruction = fdm + c(PCs[, 1:k] %*% t(scores[, 1:k]))[1:256], estimate = paste0(&quot;comp&quot;, k)) df &lt;- rbind(df, df1) p1 &lt;- ggplot(data = df1, aes (x = time, y = value)) + geom_line(color = &quot;grey2&quot;, linewidth = 0.5, alpha = 0.7) + geom_line(aes(y = reconstruction), colour = &quot;deepskyblue2&quot;, linewidth = 0.6) + labs(x = &quot;Frekvence&quot;, y = &quot;Log-periodogram&quot;) + theme_bw() # print(p1) } df |&gt; mutate(estimate = factor(estimate)) |&gt; filter(estimate %in% c(&#39;mean&#39;, &#39;comp1&#39;, &#39;comp2&#39;, &#39;comp3&#39;, &#39;comp9&#39;, &#39;comp20&#39;)) |&gt; mutate(estimate = factor(estimate, levels = c(&#39;mean&#39;, &#39;comp1&#39;, &#39;comp2&#39;, &#39;comp3&#39;, &#39;comp9&#39;, &#39;comp20&#39;))) |&gt; ggplot(aes (x = time, y = value)) + geom_line(color = &quot;grey2&quot;, linewidth = 0.5, alpha = 0.5) + geom_line(aes(y = reconstruction), colour = &quot;deepskyblue2&quot;, linewidth = 0.7) + labs(x = &quot;Frekvence&quot;, y = &quot;Log-periodogram&quot;) + theme_bw() + facet_wrap(~estimate, ncol = 3, nrow = 2) Obrázek 7.3: Původní křivka a její rekonstrukce. Code # ggsave(&quot;figures/kap2_PCA_reconstruction.tex&quot;, device = tikz, width = 9, height = 6) 14.3 Materiály pro Kapitolu 3 Tyto materiály jsou převzaty z Kapitoly 11. 14.4 Materiály pro Kapitolu 4 V této sekci uvedeme podpůrné grafy pro čtvrtou kapitolu diplomové práce. 14.4.1 Maximal margin classifier Nejprve simulujeme data ze dvou klasifikačních tříd, které budou lineárně separabilní. Code library(MASS) library(dplyr) library(ggplot2) set.seed(21) # simulace dat n_0 &lt;- 40 n_1 &lt;- 40 mu_0 &lt;- c(0, 0) mu_1 &lt;- c(3, 4.5) Sigma_0 &lt;- matrix(c(1.3, -0.7, -0.7, 1.3), ncol = 2) Sigma_1 &lt;- matrix(c(1.5, -0.25, -0.25, 1.5), ncol = 2) df_MMC &lt;- rbind( mvrnorm(n = n_0, mu = mu_0, Sigma = Sigma_0), mvrnorm(n = n_1, mu = mu_1, Sigma = Sigma_1)) |&gt; as.data.frame() |&gt; mutate(Y = rep(c(&#39;-1&#39;, &#39;1&#39;), c(n_0, n_1))) colnames(df_MMC) &lt;- c(&#39;x1&#39;, &#39;x2&#39;, &#39;Y&#39;) Nyní vykreslíme data. Code p1 &lt;- ggplot(data = df_MMC, aes(x = x1, y = x2, colour = Y)) + geom_point() + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.title.align = 0.5) + scale_x_continuous(breaks = seq(-2, 6, by = 2), limits = c(-3.5, 6.5)) + scale_y_continuous(breaks = seq(-4, 8, by = 2), limits = c(-2.5, 7)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) + labs(x = &#39;$X_1$&#39;, y = &#39;$X_2$&#39;, colour = &#39;Klasifikační\\n třída&#39;) p1 Natrénujeme klasifikátor a vykreslíme dělicí nadrovinu společně s podpůrnými vektory. Code library(e1071) clf &lt;- svm(Y ~ ., data = df_MMC, type = &#39;C-classification&#39;, scale = FALSE, kernel = &#39;linear&#39;) Code df_SV &lt;- df_MMC[clf$index, ] p2 &lt;- p1 + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.7, size = 2) p2 Dokreslíme dělicí nadrovinu. Code # vektor koeficientů w &lt;- t(clf$coefs) %*% clf$SV slope &lt;- - w[1] / w[2] intercept &lt;- clf$rho / w[2] p3 &lt;- p2 + geom_abline(slope = slope, intercept = intercept, col = &#39;grey2&#39;, linewidth = 0.7, alpha = 0.8) + geom_abline(slope = slope, intercept = intercept - 1 / w[2], col = &#39;grey2&#39;, linewidth = 0.5, alpha = 0.8, linetype = &#39;dashed&#39;) + geom_abline(slope = slope, intercept = intercept + 1 / w[2], col = &#39;grey2&#39;, linewidth = 0.5, alpha = 0.8, linetype = &#39;dashed&#39;) + geom_point() + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.4, size = 2) p3 Code # ggsave(&quot;figures/kap4_MMC.tex&quot;, device = tikz, width = 6, height = 4) 14.4.2 Support vector classifier Nejprve simulujeme data ze dvou klasifikačních tříd, které budou lineárně neseparabilní. Code set.seed(42) # simulace dat n_0 &lt;- 50 n_1 &lt;- 50 mu_0 &lt;- c(0, 0) mu_1 &lt;- c(3, 4.5) Sigma_0 &lt;- matrix(c(2, -0.55, -0.55, 2), ncol = 2) Sigma_1 &lt;- matrix(c(2.75, -0.3, -0.3, 2.75), ncol = 2) df_MMC &lt;- rbind( mvrnorm(n = n_0, mu = mu_0, Sigma = Sigma_0), mvrnorm(n = n_1, mu = mu_1, Sigma = Sigma_1)) |&gt; as.data.frame() |&gt; mutate(Y = rep(c(&#39;-1&#39;, &#39;1&#39;), c(n_0, n_1))) colnames(df_MMC) &lt;- c(&#39;x1&#39;, &#39;x2&#39;, &#39;Y&#39;) Nyní vykreslíme data. Code p1 &lt;- ggplot(data = df_MMC, aes(x = x1, y = x2, colour = Y)) + geom_point() + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.title.align = 0.5) + scale_x_continuous(breaks = seq(-2, 6, by = 2), limits = c(-2.75, 6)) + scale_y_continuous(breaks = seq(-4, 8, by = 2), limits = c(-4.5, 8)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) + labs(x = &#39;$X_1$&#39;, y = &#39;$X_2$&#39;, colour = &#39;Klasifikační\\n třída&#39;) p1 Natrénujeme klasifikátor a vykreslíme dělicí nadrovinu společně s podpůrnými vektory. Code clf &lt;- svm(Y ~ ., data = df_MMC, type = &#39;C-classification&#39;, scale = FALSE, kernel = &#39;linear&#39;) Code df_SV &lt;- df_MMC[clf$index, ] p2 &lt;- p1 + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.7, size = 2) p2 Dokreslíme dělicí nadrovinu. Code # vektor koeficientů w &lt;- t(clf$coefs) %*% clf$SV slope &lt;- - w[1] / w[2] intercept &lt;- clf$rho / w[2] p3 &lt;- p2 + geom_abline(slope = slope, intercept = intercept, col = &#39;grey2&#39;, linewidth = 0.7, alpha = 0.8) + geom_abline(slope = slope, intercept = intercept - 1 / w[2], col = &#39;grey2&#39;, linewidth = 0.5, alpha = 0.8, linetype = &#39;dashed&#39;) + geom_abline(slope = slope, intercept = intercept + 1 / w[2], col = &#39;grey2&#39;, linewidth = 0.5, alpha = 0.8, linetype = &#39;dashed&#39;) + geom_point() + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.4, size = 2) p3 Code # ggsave(&quot;figures/kap4_SVC.tex&quot;, device = tikz, width = 6, height = 4) Nakonec přidáme popisky k podpůrným vektorům. Code df_SVlab &lt;- cbind(df_SV, data.frame(label = 1:dim(df_SV)[1])) p4 &lt;- p3 + geom_text(data = df_SVlab, aes(label = label), colour = &#39;grey2&#39;, check_overlap = T, size = 3, vjust = -0.55, hjust = 1) p4 Code # ggsave(&quot;figures/kap4_SVC.tex&quot;, device = tikz, width = 6, height = 4) 14.4.2.1 Změna šířky tolerančního pásma při změně hyperparametru \\(C\\) Podívejme se ještě na změnu tolerančního pásma v závislosti na hyperparametru \\(C\\). Code C &lt;- c(0.005, 0.1, 100) clf1 &lt;- svm(Y ~ ., data = df_MMC, type = &#39;C-classification&#39;, scale = FALSE, kernel = &#39;linear&#39;, cost = C[1]) clf2 &lt;- svm(Y ~ ., data = df_MMC, type = &#39;C-classification&#39;, scale = FALSE, kernel = &#39;linear&#39;, cost = C[2]) clf3 &lt;- svm(Y ~ ., data = df_MMC, type = &#39;C-classification&#39;, scale = FALSE, kernel = &#39;linear&#39;, cost = C[3]) df_SV &lt;- rbind(df_MMC[clf1$index, ] |&gt; mutate(cost = C[1]), df_MMC[clf2$index, ] |&gt; mutate(cost = C[2]), df_MMC[clf3$index, ] |&gt; mutate(cost = C[3])) p2 &lt;- p1 + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.7, size = 2) + facet_wrap(~cost) # vektor koeficientů w &lt;- t(clf1$coefs) %*% clf1$SV slope &lt;- - w[1] / w[2] intercept &lt;- clf1$rho / w[2] df_lines &lt;- data.frame(slope = slope, intercept = intercept, lb = intercept - 1 / w[2], rb = intercept + 1 / w[2], cost = C[1]) # pro clf2 w &lt;- t(clf2$coefs) %*% clf2$SV slope &lt;- - w[1] / w[2] intercept &lt;- clf2$rho / w[2] df_lines &lt;- rbind(df_lines, data.frame(slope = slope, intercept = intercept, lb = intercept - 1 / w[2], rb = intercept + 1 / w[2], cost = C[2]) ) # pro clf3 w &lt;- t(clf3$coefs) %*% clf3$SV slope &lt;- - w[1] / w[2] intercept &lt;- clf3$rho / w[2] df_lines &lt;- rbind(df_lines, data.frame(slope = slope, intercept = intercept, lb = intercept - 1 / w[2], rb = intercept + 1 / w[2], cost = C[3]) ) p3 &lt;- p2 + geom_abline(data = df_lines, aes(slope = slope, intercept = intercept), col = &#39;grey2&#39;, linewidth = 0.7, alpha = 0.8) + geom_abline(data = df_lines, aes(slope = slope, intercept = lb), col = &#39;grey2&#39;, linewidth = 0.5, alpha = 0.8, linetype = &#39;dashed&#39;) + geom_abline(data = df_lines, aes(slope = slope, intercept = rb), col = &#39;grey2&#39;, linewidth = 0.5, alpha = 0.8, linetype = &#39;dashed&#39;) + geom_point() + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.4, size = 2) + theme(legend.position = &#39;none&#39;) p3 Code # ggsave(&quot;figures/kap4_SVC_C_dependency.tex&quot;, device = tikz, width = 8, height = 3) 14.4.3 Support vector machines Pro ilustraci této metody využijeme data tecator, kterým se podrobně věnujeme v Kapitole 11. Code # nacteni dat library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) data &lt;- ddalpha::dataf.tecator() data.gr &lt;- data$dataf[[1]]$vals for(i in 2:length(data$labels)) { data.gr &lt;- rbind(data.gr, data$dataf[[i]]$vals) } data.gr &lt;- cbind(data.frame(wave = data$dataf[[1]]$args), t(data.gr)) # vektor trid labels &lt;- data$labels |&gt; unlist() # prejmenovani podle tridy colnames(data.gr) &lt;- c(&#39;wavelength&#39;, paste0(labels, 1:length(data$labels))) Code t &lt;- data.gr$wavelength rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # penalizujeme 4. derivaci # spojeni pozorovani do jedne matice XX &lt;- data.gr[, -1] |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast set.seed(42) library(caTools) split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 # Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) # X.train &lt;- XXfd # Y.train &lt;- Y # table(Y.train) # # # relativni zastoupeni # table(Y.train) / sum(table(Y.train)) Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 # aby bylo mozne vykreslovat grafy, # potrebujeme alespon 2 HK data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid U všech třech jader projdeme hodnoty hyperparametru \\(C\\) v intervalu \\([10^{-3}, 10^{3}]\\), přičemž u jádra polynomiálního zafixujeme hyperparametr \\(p\\) na hodnotě 3, neboť pro jiné celočíselné hodnoty metoda nedává zdaleka tak dobré výsledky. Naopak pro radiální jádro využijeme k volbě optimální hodnoty hyperparametru \\(\\gamma\\) opět 10-násobnou CV, přičemž uvažujeme hodnoty v intervalu \\([10^{-3}, 10^{2}]\\). Zvolíme coef0 \\(= 1\\). Code set.seed(42) k_cv &lt;- 10 # rozdelime trenovaci data na k casti library(caret) folds &lt;- createMultiFolds(1:length(Y.train), k = k_cv, time = 1) # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 15) C.cv &lt;- 10^seq(-3, 3, length = 20) p.cv &lt;- c(2, 3, 4, 5) coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Nyní zprůměrujeme výsledky 10-násobné CV tak, abychom pro jednu hodnotu hyperparametru (případně jednu kombinaci hodnot) měli jeden odhad validační chybovosti. Přitom určíme i optimální hodnoty jednotlivých hyperparametrů. Code # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1) - 5, to = max(data.PCA.train$V1) + 5, length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2) - 5, to = max(data.PCA.train$V2) + 5, length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;Lineární&#39;, &#39;Polynomiální&#39;, &#39;Radiální&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) df_SV &lt;- rbind(data.PCA.train[clf.SVM.l.PCA$index, ] |&gt; mutate(kernel = &#39;Lineární&#39;), data.PCA.train[clf.SVM.p.PCA$index, ] |&gt; mutate(kernel = &#39;Polynomiální&#39;), data.PCA.train[clf.SVM.r.PCA$index, ] |&gt; mutate(kernel = &#39;Radiální&#39;)) pSVM &lt;- ggplot(data = data.PCA.train, aes(x = V1, y = V2, colour = Y)) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;grey2&#39;, size = 0.25) + labs(x = &#39;$X_1$&#39;, y = &#39;$X_2$&#39;, colour = &#39;Klasifikační\\n třída&#39;, fill = &#39;none&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) + theme_bw() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.title.align = 0.5, legend.position = &#39;none&#39;) + scale_y_continuous(expand = c(-0.02, -0.02), limits = c(-3.5, 2.5)) + scale_x_continuous(expand = c(-0.02, -0.02), limits = c(-15, 26)) + facet_wrap(~kernel) + geom_contour_filled(data = nd, aes(x = V1, y = V2, z = prd, colour = prd), breaks = c(1, 2, 3), alpha = 0.1, show.legend = F) + scale_fill_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.7, size = 1.5) + geom_point(size = 1.2) + geom_point(data = df_SV, col = &#39;grey2&#39;, alpha = 0.4, size = 1.5) pSVM Obrázek 9.1: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code # ggsave(&quot;figures/kap4_SVM.tex&quot;, device = tikz, width = 8, height = 3) 14.5 Materiály pro Kapitolu 5 V této sekci uvedeme podpůrné grafy pro pátou kapitolu diplomové práce. 14.5.1 Diskretizace intervalu Chtěli bychom se podívat na hodnoty skalárních součinů funkcí, které jsou blízko u sebe a naopak které se tvarem velmi liší. 14.5.1.1 tecator data Podívejme se také na data tecator. Code data &lt;- ddalpha::dataf.tecator() data.gr &lt;- data$dataf[[1]]$vals for(i in 2:length(data$labels)) { data.gr &lt;- rbind(data.gr, data$dataf[[i]]$vals) } data.gr &lt;- cbind(data.frame(wave = data$dataf[[1]]$args), t(data.gr)) # vektor trid labels &lt;- data$labels |&gt; unlist() # prejmenovani podle tridy colnames(data.gr) &lt;- c(&#39;wavelength&#39;, paste0(labels, 1:length(data$labels))) Code library(fda.usc) t &lt;- data.gr$wavelength rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # penalizujeme 4. derivaci # spojeni pozorovani do jedne matice XX &lt;- data.gr[, -1] |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd # * as.numeric(1 / norm.fd(BSmooth$fd[1])) # set norm equal to one norms &lt;- c() for (i in 1:215) {norms &lt;- c(norms, as.numeric(1 / norm.fd(BSmooth$fd[i])))} XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = 215, nrow = 104, byrow = T) fdobjSmootheval &lt;- eval.fd(fdobj = XXfd_norm, evalarg = t) # rozdeleni na testovaci a trenovaci cast set.seed(42) library(caTools) split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 # Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) # X.train &lt;- XXfd # Y.train &lt;- Y Spočítáme skalární součiny prvního s ostatními. Code n &lt;- dim(XX)[2] abs.labs &lt;- c(&quot;$&lt; 20 \\\\%$&quot;, &quot;$&gt; 20 \\\\%$&quot;) # abs.labs &lt;- c(&quot;$Y = {-1}$&quot;, &quot;$Y = 1$&quot;) names(abs.labs) &lt;- c(&#39;small&#39;, &#39;large&#39;) DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Fat = factor(rep(labels, each = length(t)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , labels == &#39;small&#39;], 1, mean), apply(fdobjSmootheval[ , labels == &#39;large&#39;], 1, mean)), Fat = factor(rep(c(&#39;small&#39;, &#39;large&#39;), each = length(t)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFsmooth |&gt; filter(time %in% as.character(c(nn))) |&gt; ggplot(aes(x = t, y = Smooth, color = Fat)) + geom_line(linewidth = 1.1, aes(group = time, linetype = &#39;apozor x1&#39;)) + geom_line(data = DFsmooth |&gt; filter(time %in% as.character(order(Inprod_vect)[1:4])), aes(group = time, linetype = &#39;nejmensi&#39;), linewidth = 0.6) + geom_line(data = DFsmooth |&gt; filter(time %in% as.character(rev(order(Inprod_vect))[1:5])), aes(group = time, linetype = &#39;nejvetsi&#39;), linewidth = 0.6) + geom_line(linewidth = 1.1, aes(group = time, linetype = &#39;apozor x1&#39;)) + theme_bw() + # facet_wrap(~Fat, # labeller = labeller(Fat = abs.labs)) + labs(x = &quot;Vlnová délka [v nm]&quot;, y = &quot;Absorbance&quot;, colour = &#39;Obsah tuku&#39;,#&quot;Klasifikační\\n třída&quot;, linetype = &#39;Styl čáry&#39;) + # scale_color_discrete(guide=&quot;none&quot;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), labels = abs.labs) + # guides(color = guide_legend(position = &#39;none&#39;)) + # scale_color_discrete(labels = abs.labs) + scale_linetype_manual(values=c(&#39;solid&#39;, &quot;dotted&quot;, &quot;longdash&quot;)) + theme(legend.position = c(0.15, 0.75), #legend.box=&quot;vertical&quot;, panel.grid = element_blank()) Obrázek 3.8: Vykreslení čtyř pozorování s minimální a maximální hodnotou skalárního součinu. Code # ggsave(&quot;figures/kap5_discretization_tecator.tex&quot;, device = tikz, width = 4.5, height = 4.5) 14.5.1.2 phoneme data Použijeme data phoneme. Code library(fda.usc) # nacteni dat data &lt;- read.delim2(&#39;phoneme.txt&#39;, header = T, sep = &#39;,&#39;) # zmenime dve promenne na typ factor data &lt;- data |&gt; mutate(g = factor(g), speaker = factor(speaker)) # numericke promenne prevedeme opravdu na numericke data[, 2:257] &lt;- as.numeric(data[, 2:257] |&gt; as.matrix()) tr_vs_test &lt;- str_split(data$speaker, &#39;\\\\.&#39;) |&gt; unlist() tr_vs_test &lt;- tr_vs_test[seq(1, length(tr_vs_test), by = 4)] data$train &lt;- ifelse(tr_vs_test == &#39;train&#39;, TRUE, FALSE) # vybrane fonemy ke klasifikaci phoneme_subset &lt;- c(&#39;aa&#39;, &#39;ao&#39;) # testovaci a trenovaci data data_train &lt;- data |&gt; filter(train) |&gt; filter(g %in% phoneme_subset) data_test &lt;- data |&gt; filter(!train) |&gt; filter(g %in% phoneme_subset) # odstranime sloupce, ktere nenesou informaci o frekvenci a # transponujeme tak, aby ve sloupcich byly jednotlive zaznamy X_train &lt;- data_train[, -c(1, 258, 259, 260)] |&gt; t() X_test &lt;- data_test[, -c(1, 258, 259, 260)] |&gt; t() # prejmenujeme radky a sloupce rownames(X_train) &lt;- 1:256 colnames(X_train) &lt;- paste0(&#39;train&#39;, data_train$row.names) rownames(X_test) &lt;- 1:256 colnames(X_test) &lt;- paste0(&#39;test&#39;, data_test$row.names) # definujeme vektor fonemu y_train &lt;- data_train[, 258] |&gt; factor(levels = phoneme_subset) y_test &lt;- data_test[, 258] |&gt; factor(levels = phoneme_subset) y &lt;- c(y_train, y_test) Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = 1, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(BSmooth$fd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) fdobjSmootheval &lt;- eval.fd(fdobj = XXfd_norm, evalarg = t) Spočítáme skalární součiny prvního log-periodogramu s ostatními. Code n &lt;- dim(XX)[2] y &lt;- c(y_train, y_test) DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Phoneme = rep(y, each = length(t))) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , y == phoneme_subset[1]], 1, mean), apply(fdobjSmootheval[ , y == phoneme_subset[2]], 1, mean)), Phoneme = factor(rep(phoneme_subset, each = length(t)), levels = levels(y)) ) DFsmooth |&gt; filter(time %in% as.character(nn)) |&gt; ggplot(aes(x = t, y = Smooth, color = Phoneme)) + geom_line(linewidth = 1.1, aes(group = time, linetype = &#39;apozor x1&#39;)) + geom_line(data = DFsmooth |&gt; filter(time %in% as.character(order(Inprod_vect)[1:4])), aes(group = time, linetype = &#39;nejmensi&#39;), linewidth = 0.6) + geom_line(data = DFsmooth |&gt; filter(time %in% as.character(rev(order(Inprod_vect))[1:5])), aes(group = time, linetype = &#39;nejvetsi&#39;), linewidth = 0.6) + geom_line(linewidth = 1.1, aes(group = time, linetype = &#39;apozor x1&#39;)) + theme_bw() + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Foném&#39;, linetype = &#39;Styl čáry&#39;) + # scale_colour_discrete(labels = phoneme_subset) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), labels = phoneme_subset) + scale_linetype_manual(values=c(&#39;solid&#39;, &quot;dotted&quot;, &quot;longdash&quot;)) + theme(legend.position = c(0.8, 0.75), panel.grid = element_blank()) Obrázek 12.2: Vykreslení čtyř pozorování s minimální a maximální hodnotou skalárního součinu. Code # ggsave(&quot;figures/kap5_discretization_phoneme.tex&quot;, device = tikz, width = 4.5, height = 4.5) 14.5.2 Support vector regression (SVR) Ukázka metody SVR na obou datových souborech. 14.5.2.1 tecator data Code t &lt;- data.gr$wavelength rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # penalizujeme 4. derivaci # spojeni pozorovani do jedne matice XX &lt;- data.gr[, -1] |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Code library(e1071) library(caret) df_plot &lt;- data.frame() # model for(i in 1:5) { df.svm &lt;- data.frame(x = t, y = fdobjSmootheval[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.03, gamma = 0.5, cost = 1, tolerance = 0.001, shrinking = TRUE, scale = TRUE) svm.RKHS &lt;- train(y ~ x, data = df.svm, method = &#39;svmRadial&#39;, metric = &quot;RMSE&quot;, preProcess = c(&#39;center&#39;, &#39;scale&#39;), trControl = trainControl( method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE ) # trControl = trainControl(method = &quot;none&quot;), # Telling caret not to re-tune # tuneGrid = data.frame(sigma = 1000, C = 1000) # Specifying the parameters ) df_plot &lt;- rbind( df_plot, data.frame( x = t, y = svm.RKHS$finalModel@fitted * svm.RKHS$finalModel@scaling$y.scale$`scaled:scale` + svm.RKHS$finalModel@scaling$y.scale$`scaled:center`, line = &#39;estimate&#39;, curve = as.character(i)) |&gt; rbind(data.frame( x = t, y = fdobjSmootheval[, i], line = &#39;sample&#39;, curve = as.character(i) ))) } Vykresleme si pro lepší představu odhad křivky (červeně) společně s pozorovanou křivkou (modře). Code df_plot |&gt; filter(curve %in% c(&#39;1&#39;, &#39;2&#39;, &#39;3&#39;)) |&gt; ggplot(aes(x, y, col = line, linetype = curve)) + geom_line(linewidth = 0.8) + theme_bw() + labs(x = &quot;Vlnová délka [v nm]&quot;, y = &quot;Absorbance&quot;, colour = &#39;Křivka&#39;) + # scale_colour_discrete(labels = phoneme_subset) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), labels = c(&quot;odhadnutá&quot;, &quot;pozorovaná&quot;)) + scale_linetype_manual(values = c(&#39;solid&#39;, &quot;dotted&quot;, &quot;dashed&quot;)) + theme(legend.position = c(0.17, 0.85), panel.grid = element_blank()) + guides(linetype = &#39;none&#39;) Obrázek 1.20: Porovnání pozorované a odhadnuté křivky. Code # ggsave(&quot;figures/kap5_SVR_tecator.tex&quot;, device = tikz, width = 4.5, height = 4.5) 14.5.2.2 phoneme data Code data &lt;- read.delim2(&#39;phoneme.txt&#39;, header = T, sep = &#39;,&#39;) # zmenime dve promenne na typ factor data &lt;- data |&gt; mutate(g = factor(g), speaker = factor(speaker)) # numericke promenne prevedeme opravdu na numericke data[, 2:257] &lt;- as.numeric(data[, 2:257] |&gt; as.matrix()) tr_vs_test &lt;- str_split(data$speaker, &#39;\\\\.&#39;) |&gt; unlist() tr_vs_test &lt;- tr_vs_test[seq(1, length(tr_vs_test), by = 4)] data$train &lt;- ifelse(tr_vs_test == &#39;train&#39;, TRUE, FALSE) # vybrane fonemy ke klasifikaci phoneme_subset &lt;- c(&#39;aa&#39;, &#39;ao&#39;) # testovaci a trenovaci data data_train &lt;- data |&gt; filter(train) |&gt; filter(g %in% phoneme_subset) data_test &lt;- data |&gt; filter(!train) |&gt; filter(g %in% phoneme_subset) # odstranime sloupce, ktere nenesou informaci o frekvenci a # transponujeme tak, aby ve sloupcich byly jednotlive zaznamy X_train &lt;- data_train[, -c(1, 258, 259, 260)] |&gt; t() X_test &lt;- data_test[, -c(1, 258, 259, 260)] |&gt; t() # prejmenujeme radky a sloupce rownames(X_train) &lt;- 1:256 colnames(X_train) &lt;- paste0(&#39;train&#39;, data_train$row.names) rownames(X_test) &lt;- 1:256 colnames(X_test) &lt;- paste0(&#39;test&#39;, data_test$row.names) # definujeme vektor fonemu y_train &lt;- data_train[, 258] |&gt; factor(levels = phoneme_subset) y_test &lt;- data_test[, 258] |&gt; factor(levels = phoneme_subset) y &lt;- c(y_train, y_test) Code t &lt;- 1:256 rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci # spojeni pozorovani do jedne matice XX &lt;- cbind(X_train, X_test) |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = 1, to = 3, length.out = 35) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Code df_plot &lt;- data.frame() # model for(i in 1:5) { df.svm &lt;- data.frame(x = t, y = fdobjSmootheval[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.03, gamma = 0.5, cost = 1, tolerance = 0.001, shrinking = TRUE, scale = TRUE) svm.RKHS &lt;- train(y ~ x, data = df.svm, method = &#39;svmRadial&#39;, metric = &quot;RMSE&quot;, preProcess = c(&#39;center&#39;, &#39;scale&#39;), trControl = trainControl( method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE ) # trControl = trainControl(method = &quot;none&quot;), # Telling caret not to re-tune # tuneGrid = data.frame(sigma = 1000, C = 1000) # Specifying the parameters ) df_plot &lt;- rbind( df_plot, data.frame( x = t, y = svm.RKHS$finalModel@fitted * svm.RKHS$finalModel@scaling$y.scale$`scaled:scale` + svm.RKHS$finalModel@scaling$y.scale$`scaled:center`, line = &#39;estimate&#39;, curve = as.character(i)) |&gt; rbind(data.frame( x = t, y = fdobjSmootheval[, i], line = &#39;sample&#39;, curve = as.character(i) ))) } Vykresleme si pro lepší představu odhad křivky (červeně) společně s pozorovanou křivkou (modře). Code df_plot |&gt; filter(curve %in% c(&#39;1&#39;, &#39;5&#39;, &#39;3&#39;)) |&gt; ggplot(aes(x, y, col = line, linetype = curve)) + geom_line(linewidth = 0.8) + theme_bw() + labs(x = &#39;Frekvence&#39;, y = &#39;Log-periodogram&#39;, colour = &#39;Křivka&#39;) + # scale_colour_discrete(labels = phoneme_subset) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;), labels = c(&quot;odhadnutá&quot;, &quot;pozorovaná&quot;)) + scale_linetype_manual(values = c(&#39;solid&#39;, &quot;dotted&quot;, &quot;dashed&quot;)) + theme(legend.position = c(0.8, 0.85), panel.grid = element_blank()) + guides(linetype = &#39;none&#39;) Obrázek 9.2: Porovnání pozorované a odhadnuté křivky. Code # ggsave(&quot;figures/kap5_SVR_phoneme.tex&quot;, device = tikz, width = 4.5, height = 4.5) 14.6 Materiály pro Kapitolu 6 Veškeré grafické podklady i číselné výstupy prezentované v Diplomové práci v Kapitole 6 jsou k dispozici v Kapitole 5 (případně také v Kapitolách 6, 7 a 8) a v Kapitole 9. Krabicové diagramy testovacích chybovostí jsme vizuálně upravili pro potřeby diplomové práce (barevnost, změna měřítka, popisky), kód použitý k jejich vygenerování je k vidění v příslušných buňkách (zapoznámkovaný). 14.7 Materiály pro Kapitolu 7 Veškeré grafické podklady i číselné výstupy prezentované v Diplomové práci v Kapitole 7 jsou k dispozici v Kapitole 11, která je věnována datovému souboru phoneme, a Kapitole 12 věnované datovému souboru tecator. Krabicové diagramy testovacích chybovostí jsme vizuálně upravili pro potřeby diplomové práce (barevnost, změna měřítka, popisky), kód použitý k jejich vygenerování je k vidění v příslušných buňkách (zapoznámkovaný). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
