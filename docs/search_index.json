[["aplikace-na-reálných-datech-1.html", "Kapitola 4 Aplikace na reálných datech 1", " Kapitola 4 Aplikace na reálných datech 1 Nyní se věnujme aplikaci na reálných datech. Budeme pracovat s daty growth(), které jsou dostupné v knihovně fda. Nejprve si data načteme a vykreslíme. Code # nacteni dat library(fda) library(ggplot2) library(dplyr) library(tidyr) data &lt;- growth Code data.gr &lt;- cbind(data.frame(age = data$age), data$hgtf, data$hgtm) n_girls &lt;- 54 n_boys &lt;- 39 Code pivot_longer(data.gr, cols = girl01:boy39, names_to = &#39;sample&#39;, values_to = &#39;height&#39;, cols_vary = &#39;slowest&#39;) |&gt; mutate(Individual = as.factor(sample), Gender = factor(rep(rep(c(&#39;girl&#39;, &#39;boy&#39;), c(n_girls, n_boys)), each = length(data.gr$age)), levels = c(&#39;girl&#39;, &#39;boy&#39;))) |&gt; ggplot(aes(x = age, y = height, colour = Gender, group = Individual)) + geom_line() + theme_bw() "],["vyhlazení-pozorovaných-křivek-2.html", "4.1 Vyhlazení pozorovaných křivek", " 4.1 Vyhlazení pozorovaných křivek Nyní převedeme pozorované diskrétní hodnoty (vektory hodnot) na funkcionální objekty, se kterými budeme následně pracovat. Jelikož se nejedná o periodické křivky na intervalu \\(I = [0, 1]\\), využijeme k vyhlazení B-sline bázi. Za uzly bereme celý vektor time, standardně uvažujeme kubické spliny, proto volíme (implicitní volba v R) norder = 4. Budeme penalizovat druhou derivaci funkcí. Code t &lt;- data.gr$age rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci Najdeme vhodnou hodnotu vyhlazovacího parametru \\(\\lambda &gt; 0\\) pomocí \\(GCV(\\lambda)\\), tedy pomocí zobecněné cross–validace. Hodnotu \\(\\lambda\\) budeme uvažovat pro obě klasifikační skupiny stejnou, neboť pro testovací pozorování bychom dopředu nevěděli, kterou hodnotu \\(\\lambda\\), v případě rozdílné volby pro každou třídu, máme volit. Code # spojeni pozorovani do jedne matice XX &lt;- data.gr[, -1] |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -7, to = 1, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] Pro lepší znázornění si vykreslíme průběh \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;dashed&#39;, linewidth = 0.8) + geom_point(size = 2.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) Obrázek 4.1: Průběh \\(GCV(\\lambda)\\) pro zvolený vektor \\(\\boldsymbol\\lambda\\). Na ose \\(x\\) jsou hodnoty vyneseny v logaritmické škále. Červeně je znázorněna optimální hodnota vyhlazovacího parametru \\(\\lambda_{optimal}\\). S touto optimální volbou vyhlazovacího parametru \\(\\lambda\\) nyní vyhladíme všechny funkce. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Ještě znázorněme všechny křivky včetně průměru zvlášť pro každou třídu. Code n &lt;- n_girls + n_boys DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n_girls], 1, mean), n_girls), rep(apply(fdobjSmootheval[ , (n_girls + 1) : n], 1, mean), n_boys)), Gender = factor(rep(c(&#39;girls&#39;, &#39;boys&#39;), each = n * length(t)), levels = c(&#39;girls&#39;, &#39;boys&#39;)) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n_girls], 1, mean), apply(fdobjSmootheval[ , (n_girls + 1) : n], 1, mean)), group = factor(rep(c(&#39;girls&#39;, &#39;boys&#39;), each = length(t)), levels = c(&#39;girls&#39;, &#39;boys&#39;)) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, color = Gender, group = time)) + geom_line(linewidth = 0.25) + theme_bw() + labs(x = &#39;Time&#39;, y = &#39;Function&#39;, colour = &#39;Gender&#39;) + scale_colour_discrete(labels = c(&#39;girls&#39;, &#39;boys&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;black&#39;, linewidth = 1, linetype = &#39;twodash&#39;) "],["klasifikace-křivek-2.html", "4.2 Klasifikace křivek", " 4.2 Klasifikace křivek Nejprve načteme potřebné knihovny pro klasifikaci. Code library(caTools) # pro rozdeleni na testovaci a trenovaci library(caret) # pro k-fold CV library(fda.usc) # pro KNN, fLR library(MASS) # pro LDA library(fdapace) library(pracma) library(refund) # pro LR na skorech library(nnet) # pro LR na skorech library(caret) library(rpart) # stromy library(rattle) # grafika library(e1071) library(randomForest) # nahodny les Code # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), c(n_girls, n_boys)) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Ještě se podíváme na zastoupení jednotlivých skupin v testovací a trénovací části dat. Code # absolutni zastoupeni table(Y.train) ## Y.train ## 0 1 ## 35 30 Code table(Y.test) ## Y.test ## 0 1 ## 19 9 Code # relativni zastoupeni table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5384615 0.4615385 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.6785714 0.3214286 4.2.1 \\(K\\) nejbližších sousedů Začněme neparametrickou klasifikační metodou, a to metodou \\(K\\) nejbližších sousedů. Nejprve si vytvoříme potřebné objekty tak, abychom s nimi mohli pomocí funkce classif.knn() z knihovny fda.usc dále pracovat. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Nyní můžeme definovat model a podívat se na jeho úspěšnost klasifikace. Poslední otázkou však zůstává, jak volit optimální počet sousedů \\(K\\). Mohli bychom tento počet volit jako takové \\(K\\), při kterém nastává minimální chybovost na trénovacích datech. To by ale mohlo vést k přeučení modelu, proto využijeme cross-validaci. Vzhledem k výpočetní náročnosti a rozsahu souboru zvolíme \\(k\\)-násobnou CV, my zvolíme například hodnotu \\(k = {10}\\). Code # model pro vsechna trenovaci data pro K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) summary(neighb.model) # shrnuti modelu ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.9714286 1.0000000 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 34 1 ## 2 0 30 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 1 2 3 4 5 6 7 8 ## 0.9846 0.9538 0.9846 0.9692 0.9692 0.9538 0.9692 0.9538 ## ## -Optimal number of neighbors: knn.opt= 1 ## with highest probability of correct classification max.prob= 0.9846154 ## ## -Probability of correct classification: 0.9846 Code plot(neighb.model$gcv, pch = 16) # vykresleni zavislosti GCV na poctu sousedu K Code neighb.model$max.prob # maximalni presnost ## [1] 0.9846154 Code (K.opt &lt;- neighb.model$h.opt) # optimalni hodnota K ## [1] 1 Proveďme předchozí postup pro trénovací data, která rozdělíme na \\(k\\) částí a tedy zopakujeme tuto část kódu \\(k\\)-krát. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu K sousedu CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) CV.results ## [1] 0.9875000 0.9500000 0.9875000 0.9675000 0.9675000 0.9475000 0.9675000 ## [8] 0.9475000 0.9675000 0.9475000 0.9475000 0.9475000 0.9475000 0.9332143 ## [15] 0.9675000 0.9421032 0.9675000 0.9082143 Vidíme, že nejlépe vychází hodnota parametru \\(K\\) jako 1 s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9875. Pro přehlednost si ještě vykresleme průběh validační chybovosti v závislosti na počtu sousedů \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linestyle = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = neighbours) ## Warning in geom_line(linestyle = &quot;dashed&quot;, colour = &quot;grey&quot;): Ignoring unknown ## parameters: `linestyle` Obrázek 4.2: Závislost validační chybovosti na hodnotě \\(K\\), tedy na počtu sousedů. Nyní známe optimální hodnotu parametru \\(K\\) a tudíž můžeme sestavit finální model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) summary(neighb.model) ## - SUMMARY - ## ## -Probability of correct classification by group (prob.classification): ## y ## 1 2 ## 0.9714286 1.0000000 ## ## -Confusion matrix between the theoretical groups (by rows) ## and estimated groups (by column) ## ## 1 2 ## 1 34 1 ## 2 0 30 ## ## -Vector of probability of correct classification ## by number of neighbors (knn): ## 1 ## 0.9846 ## ## -Optimal number of neighbors: knn.opt= 1 ## with highest probability of correct classification max.prob= 0.9846154 ## ## -Probability of correct classification: 0.9846 Code # presnost na testovacich datech presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # chybovost 1 - presnost ## [1] 0.1428571 Vidíme tedy, že přesnost modelu sestrojeného pomocí metody \\(K\\) nejbližších sousedů s optimální volbou \\(K_{optimal}\\) rovnou 1, kterou jsme určili cross-validací, je na trénovacích datech rovna 0.0154 a na testovacích datech 0.1429. K porovnání jendotlivých modelů můžeme použít oba typy chybovostí, pro přehlednost si je budeme ukládat do tabulky. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 4.2.2 Lineární diskriminační analýza Jako druhou metodu pro sestrojení klasifikátoru budeme uvažovat lineární diskriminační analýzu (LDA). Jelikož tato metoda nelze aplikovat na funkcionální data, musíme je nejprve diskretizovat, což provedeme pomocí funkcionální analýzy hlavních komponent. Klasifikační algoritmus následně provedeme na skórech prvních \\(p\\) hlavních komponent. Počet komponent \\(p\\) zvolíme tak, aby prvních \\(p\\) hlavních komponent dohromady vysvětlovalo alespoň 90 % variability v datech. Proveďme tedy nejprve funkcionální analýzu hlavních komponent a určeme počet \\(p\\). Code # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid V tomto konkrétním případě jsme za počet hlavních komponent vzali \\(p\\) = 2, které dohromady vysvětlují 94.34 % variability v datech. První hlavní komponenta potom vysvětluje 81.64 % a druhá 12.7 % variability. Graficky si můžeme zobrazit hodnoty skórů prvních dvou hlavních komponent, barevně odlišených podle příslušnosti do klasifikační třídy. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Obrázek 4.3: Hodnoty skórů prvních dvou hlavních komponent pro trénovací data. Barevně jsou odlišeny body podle příslušnosti do klasifikační třídy. Abychom mohli určit přesnost klasifikace na testovacích datech, potřebujeme spočítat skóre pro první 2 hlavní komponenty pro testovací data. Tato skóre určíme pomocí vzorce: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] kde \\(\\mu(t)\\) je střední hodnota (průměrná funkce) a \\(\\rho_j(t)\\) vlastní fukce (funkcionální hlavní komponenty). Code # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Nyní již můžeme sestrojit klasifikátor na trénovací části dat. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme jednak přesnost klasifikátoru na trénovacích (98.46 %), tak i na testovacích datech (96.43 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour(). Code # pridame diskriminacni hranici np &lt;- 1001 # pocet bodu site # x-ova osa ... 1. HK nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-ova osa ... 2. HK nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # pripad pro 2 HK ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # pokud p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # pokud p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # pokud p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # pridame Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 4.4: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí LDA. Vidíme, že dělící hranicí je přímka, lineární funkce v prostoru 2D, což jsme ostatně od LDA čekali. Nakonec přidáme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.2.3 Kvadratická diskriminační analýza Jako další sestrojme klasifikátor pomocí kvadratické diskriminační analýzy (QDA). Jedná se o analogický případ jako LDA s tím rozdílem, že nyní připouštíme pro každou ze tříd rozdílnou kovarianční matici normálního rozdělení, ze kterého pocházejí příslušné skóry. Tento vypuštěný předpoklad o rovnosti kovariančních matic vede ke kvadratické hranici mezi třídami. V R se provede QDA analogicky jako LDA v předchozí části, tedy opět bychom pomocí funkcionální analýzy hlavních komponent spočítali skóre pro trénovací i testovací funkce, sestrojili klasifikátor na skórech prvních \\(p\\) hlavních komponent a pomocí něj predikovali příslušnost testovacích křivek do třídy \\(Y^* \\in \\{0, 1\\}\\). Funkcionální PCA provádět nemusíme, využijeme výsledků z části LDA. Můžeme tedy rovnou přistoupit k sestrojení klasifikátoru, což provedeme pomocí funkce qda(). Následně spočítáme přesnost klasifikátoru na testovacích a trénovacích datech. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy jednak přesnost klasifikátoru na trénovacích (96.92 %), tak i na testovacích datech (96.43 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 4.5: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (parabola v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí QDA. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní parabola. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.2.4 Logistická regrese Logistickou regresi můžeme provést dvěma způsoby. Jednak použít funkcionální obdobu klasické logistické regrese, druhak klasickou mnohorozměrnou logistickou regresi, kterou provedeme na skórech prvních \\(p\\) hlavních komponent. 4.2.4.1 Funkcionální logistická regrese Analogicky jako v případě konečné dimenze vstupních dat uvažujeme logistický model ve tvaru: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] kde \\(\\eta(x)\\) je lineární prediktor nabývající hodnot z intervalu \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) je linková funkce, v případě logistické regrese se jedná o logitovou funkci \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\) a \\(\\pi(x)\\) podmíněná pravděpodobnost \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] přičemž \\(\\alpha\\) je konstanta a \\(\\beta(t) \\in L^2[a, b]\\) je parametrická funkce. Naším cílem je odhadnout tuto parametrickou funkci. Pro funkcionální logistickou regresi použijeme funkci fregre.glm() z balíčku fda.usc. Nejprve si vytvoříme vhodné objekty pro konstrukci klasifikátoru. Code # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis Abychom mohli odhadnout parametrickou funkci \\(\\beta(t)\\), potřebujeme ji vyjádřit v nějaké bazické reprezentaci, v našem případě B-splinové bázi. K tomu však potřebujeme najít vhodný počet bázových funkcí. To bychom mohli určit na základě chybovosti na trénovacích datech, avšak tato data budou upřenostňovat výběr velkého počtu bází a bude docházet k přeučení modelu. Ilustrujme si to na následujícím případě. Pro každý z počtu bází \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\) natrénujeme model na trénovacích datech, určíme na nich chybovost a také spočítáme chybovost na testovacích datech. Připomeňme, že k výběru vhodného počtu bází nemůžeme využít stejná data jako pro odhad testovací chybovosti, neboť bychom tuto chybovost podcenili. Code n.basis.max &lt;- 6 n.basis &lt;- 20#:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # vztah f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) # vyhlazene data basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Znázorněme si průběh obou typů chybovostí v grafu v závislosti na počtu bazických funkcí. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Chybovost&#39;) Vidíme, že s rostoucím počtem bází pro \\(\\beta(t)\\) má trénovací chybovost (modrá čára) tendenci klesat a tedy bychom na jejím základě volili velké hodnoty \\(n_{basis}\\). Naopak optimální volbou na základě testovací chybovosti je \\(n\\) rovno NA, tedy výrazně menší hodnota než 50. Naopak s rostoucím \\(n\\) roste testovací chyvost, což ukazuje na přeučení modelu. Z výše uvedených důvodů pro určení optimálního počtu bazických funkcí pro \\(\\beta(t)\\) využijeme 10-ti násobnou cross-validaci. Jako maximální počet uvažovaných bazických funkcí bereme 25, neboť jak jsme viděli výše, nad touto hodnotou dochází již k přeučení modelu. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro dany pocet bazi CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Nyní již máme vše připravené pro spočítání chybovosti na každé z deseti podmnožin trénovací množiny. Následně určíme průměr a jako optimální \\(n\\) vezmeme argument minima validační chybovosti. Code for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) CV.results Vykresleme si ještě průběh validační chybovosti i se zvýrazněnou optimální hodnotou \\(n_{optimal}\\) rovnou NA s validační chybovostí NA. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validační chybovost&#39;) + scale_x_continuous(breaks = n.basis) Nyní již tedy můžeme definovat finální model pomocí funkcionální logistické regrese, přičemž bázi pro \\(\\beta(t)\\) volíme B-splinovou bázi s NA bázemi. Code # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme trénovací chybovost (rovna NA %) i testovací chybovost (rovna NA %). Pro lepší představu si ještě můžeme vykreslit hodnoty odhadnutých pravděpodobností příslušnosti do klasifikační třídy \\(Y = 1\\) na trénovacích datech v závislosti na hodnotách lineárního prediktoru. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Lineární prediktor&#39;, y = &#39;Odhadnuté pravděpodobnosti Pr(Y = 1|X = x)&#39;, colour = &#39;Třída&#39;) Můžeme si ještě pro informaci zobrazit průběh odhadnuté parametrické funkce \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 1, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Vidíme, že hodnoty funkce \\(\\hat\\beta(t)\\) se drží kolem nuly pro časy \\(t\\) z prostředka intervalu \\([0, 1]\\), zatímco pro krajní časy jsou hodnoty vyšší. To implikuje rozdílnost funkcí z klasifikačních tříd na začátku a konci intervalu, zatímco uprostřed intervalu jsou funkce velmi podobné. Výsledky opět přidáme do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.2.4.2 Logistická regrese s analýzou hlavních komponent Abychom mohli sesrojit tento klasifikátor, potřebujeme provést funkcionální analýzu hlavních komponent, určit vhodný počet komponent a spočítat hodnoty skórů pro testovací data. To jsme již provedli v části u lineární diskriminační analýzy, proto využijeme tyto výsledky v následující části. Můžeme tedy rovnou sestrojit model logistické regrese pomocí funkce glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Spočítali jsme tedy přesnost klasifikátoru na trénovacích (96.92 %) i na testovacích datech (96.43 %). Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v případě LDA i QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Obrázek 4.6: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí logistické regrese. Všimněme si, že dělící hranicí mezi klasifikačními třídami je nyní přímka jako v případě LDA. Nakonec ještě doplníme chybovosti do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.2.5 Rozhodovací stromy V této části se podíváme na velmi odlišný přístup k sestrojení klasifikátoru, než byly například LDA či logistická regrese. Rozhodovací stromy jsou velmi oblíbeným nástrojem ke klasifikaci, avšak jako v případě některých předchozích metod nejsou přímo určeny pro funkcionální data. Existují však postupy, jak funkcionální objekty převést na mnohorozměrné a následně na ně aplikovat algoritmus rozhodovacích stromů. Můžeme uvažovat následující postupy: algoritmus sestrojený na bázových koeficientech, využití skórů hlavních komponent, použít diskretizaci intervalu a vyhodnotit funkci jen na nějaké konečné síti bodů. My se nejprve zaměříme na diskretizaci intervalu a následně porovnáme výsledky se zbylými dvěma přístupy k sestrojení rozhodovacího stromu. 4.2.5.1 Diskretizace intervalu Nejprve si musíme definovat body z intervalu \\(I = [0, 1]\\), ve kterých funkce vyhodnotíme. Následně vytvoříme objekt, ve kterém budou řádky představovat jednotlivé (diskretizované) funkce a sloupce časy. Nakonec připojíme sloupec \\(Y\\) s informací o příslušnosti do klasifikační třídy a totéž zopakujeme i pro testovací data. Code # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(rangeval[1], rangeval[2], length = 51) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Nyní mážeme sestrojit rozhodovací strom, ve kterém budou jakožto prediktory vystupovat všechny časy z vektoru t.seq. Tato klasifikační není náchylná na multikolinearitu, tudíž se jí nemusíme zabývat. Jako metriku zvolíme přesnost. Code # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost klasifikátoru na testovacích datech je tedy 78.57 % a na trénovacích datech 87.69 %. Graficky si rozhodovací strom můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 4.7: Grafické znázornění neprořezaného rozhodovacího stromu. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree$finalModel, # finalni model ... prorezany strom extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 4.8: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - diskr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.2.5.2 Skóre hlavních komponent Další možností pro sestrojení rozhodovacího stromu je použít skóre hlavních komponent. Jelikož jsme již skóre počítali pro předchozí klasifikační metody, využijeme těchto poznatků a sestrojíme rozhodovací strom na skórech prvních 2 hlavních komponent. Code # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na testovacích datech je tedy 85.71 % a na trénovacích datech 93.85 %. Graficky si rozhodovací strom sestrojený na skórech hlavních komponent můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 4.9: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na skórech hlavních komponent. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 4.10: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.2.5.3 Bázové koeficienty Poslední možností, kterou využijeme pro sestrojení rozhodovacího stromu, je použití koeficientů ve vyjádření funkcí v B-splinové bázi. Nejprve si definujme potřebné datové soubory s koeficienty. Code # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Nyní již můžeme sestrojit klasifikátor. Code # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 87.69 % a na testovacích datech 78.57 %. Graficky si rozhodovací strom sestrojený na koeficientech B-splinového vyjádření můžeme vykreslit pomocí funkce fancyRpartPlot(). Nastavíme barvy uzlů tak, aby reflektovaly předchozí barevné odlišení. Jedná se o neprořezaný strom. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Obrázek 4.11: Grafické znázornění neprořezaného rozhodovacího stromu sestrojeného na bázových koeficientech. Modrými odstíny jsou vykresleny uzly příslušející klasifikační třídě 1 a červenými odstíny třídě 0. Můžeme si také vykreslit již prořezaný finální rozhodovací strom. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # finalni model extra = 104, # zobrazeni pozadovanych informaci box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Obrázek 4.12: Finální prořezaný rozhodovací strom. Nakonec opět přidejme trénovací a testovací chybovost do souhrnné tabulky. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.2.6 Náhodné lesy Klasifikátor sestrojený pomocí metody náhodných lesů spočívá v sestrojení několika jednotlivých rozhodovacích stromů, které se následně zkombinují a vytvoří společný klasifikátor (společným “hlasováním”). Tak jako v případě rozhodovacích stromů máme několik možností na to, jaká data (konečně-rozměrná) použijeme pro sestrojení modelu. Budeme opět uvažovat výše diskutované tři přístupy. Datové soubory s příslušnými veličinami pro všechny tři přístupy již máme připravené z minulé sekce, proto můžeme přímo sestrojit dané modely, spočítat charakteristiky daného klasifikátoru a přidat výsledky do souhrnné tabulky. 4.2.6.1 Diskretizace intervalu V prvním případě využíváme vyhodnocení funkcí na dané síti bodů intervalu \\(I = [0, 1]\\). Code # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost náhodného lesu na trénovacích datech je tedy 100 % a na testovacích datech 92.86 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.2.6.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních p = 2 hlavních komponent. Code # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost rozhodovacího stromu na trénovacích datech je tedy 100 % a na testovacích datech 92.86 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.2.6.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost tohoto klasifikátoru na trénovacích datech je 98.46 % a na testovacích datech 92.86 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 4.2.7 Support Vector Machines Nyní se podívejme na klasifikaci našich nasimulovaných křivek pomocí metody podpůrných vektorů (ang. Support Vector Machines, SVM). Výhodou této klasifikační metody je její výpočetní nenáročnost, neboť pro definici hraniční křivky mezi třídami využívá pouze několik (často málo) pozorování. Hlavní výhodou SVM je použití tzv. jádrového triku (kernel trick), pomocí kterého nahradíme obyčejný skalární součin jiným skalárním součinem transformovaných dat, aniž bychom tuto transformaci museli přímo definovat. Tím dostaneme obecně nelineární dělící hranici mezi klasifikačními třídami. Jádro (jádrová funkce, ang. kernel, kernel function) \\(K\\) je taková funkce, která splňuje \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] kde \\(\\phi\\) je nějaká (neznámá) transformace (ang. feature map), \\(\\mathcal H\\) je Hilbertův prostor a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) je nějaký skalární součin na tomto Hilbertově prostoru. Nejčastěji se v praxi volí tři typy jádrových funkcí: lineární jádro – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomiální jádro – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radiální (gaussovské) jádro – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). U všech výše zmíněných jader musíme zvolit konstantu \\(C &gt; 0\\), která udává míru penalizace za překročení dělící hranice mezi třídami (ang. inverse regularization parameter). S rostoucí hodnotou \\(C\\) bude metoda více penalizovat špatně klasifikovaná data a méně tvar hranice, naopak pro malé hodnoty \\(C\\) metoda nedává takový význam špatně klasifikovaným datům, ale zaměřuje se více na penalizaci tvaru hranice. Tato konstanta \\(C\\) se defaultně volí rovna 1, můžeme ji určit i přímo například pomocí cross-validace. Využitím cross-validace můžeme také určit optimální hodnoty ostatních hyperparametrů, které nyní závisí na naší volbě jádrové funkce. V případě lineárního jádra nevolíme žádný další parametr kromě konstanty \\(C\\), u polynomiálního jádra musíme určit hodnoty hyperparametrů \\(\\alpha_0, \\gamma \\text{ a } d\\), jejichž defaultní hodnoty v R jsou postupně \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ a } d^{default} = 3\\). Při volbě radiálního jádra máme pouze jeden další hyperparametr \\(\\gamma\\), jehož defaultní hodnota v R je totožná jako u polynomiálního jádra. Opět bychom mohli hodnoty hyperparametrů určit jako optimální pro naše data, avšak vzhledem k relativní výpočetní náročnosti necháme hodnoty příslušných hyperparametrů na jejich defaultních hodnotách. V případě funkcionálních dat máme několik možností, jak použít metodu SVM. Nejjednodušší variantou je použít tuto klasifikační metodu přímo na diskretizovanou funkci. Další možností je opět využít skóre hlavních komponent a klasifikovat křivky pomocí jejich reprezentace. Další přímočarou variantou je využít vyjádření křivek pomocí B-splinové báze a klasifikovat křivky na základě koeficientů jejich vyjádření v této bázi. Složitější úvahou můžeme dospět k několika dalším možnostem, které využívají funkcionální podstatu dat. Jednak můžeme místo klasifikace původní křivky využít její derivaci (případně druhou derivaci, třetí, …), druhak můžeme definovat novou jádrovou funkci, která vezme do úvahy funkcionální podstatu dat. To je možné zejména proto, že v definici metody SVM pracujeme s libovolným Hilbertovým prostorem (bez omezení na konečnou dimenzi), tedy připouštíme i Hilbertův prostor funkcionálních dat. 4.2.7.1 Diskretizace intervalu Začněme nejprve aplikací metody podpůrných vektorů přímo na diskretizovaná data (vyhodnocení funkce na dané síti bodů na intervalu \\(I = [0, 1]\\)), přičemž budeme uvažovat všech tři výše zmíněné jádrové funkce. Code # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM na trénovacích datech je tedy 98.46 % pro lineární jádro, 78.46 % pro polynomiální jádro a 96.92 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 96.43 % pro lineární jádro, 92.86 % pro polynomiální jádro a 89.29 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 4.2.7.2 Skóre hlavních komponent V tomto případě využijeme skóre prvních p = 2 hlavních komponent. Code # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na skóre hlavních komponent na trénovacích datech je tedy 98.46 % pro lineární jádro, 95.38 % pro polynomiální jádro a 98.46 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 96.43 % pro lineární jádro, 96.43 % pro polynomiální jádro a 96.43 % pro radiální jádro. Pro grafické znázornění metody můžeme zaznačit dělící hranici do grafu skórů prvních dvou hlavních komponent. Tuto hranici spočítáme na husté síti bodů a zobrazíme ji pomocí funkce geom_contour() stejně jako v předchozích případech, kdy jsme také vykreslovali klasifikační hranici. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1. hlavní komponenta (vysvětlená variabilita&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2. hlavní komponenta (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Obrázek 4.13: Skóre prvních dvou hlavních komponent, barevně odlišené podle příslušnosti do klasifikační třídy. Černě je vyznačena dělící hranice (přímka, resp. křivky v rovině prvních dvou hlavních komponent) mezi třídami sestrojená pomocí metody SVM. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 4.2.7.3 Bázové koeficienty Nakonec použijeme vyjádření funkcí pomocí B-splinové báze. Code # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 98.46 % pro lineární jádro, 98.46 % pro polynomiální jádro a 98.46 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 96.43 % pro lineární jádro, 92.86 % pro polynomiální jádro a 92.86 % pro radiální jádro. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 4.2.7.4 Projekce na B-splinovou bázi Další možností, jak použít klasickou metodu SVM pro funkcionální data, je projektovat původní data na nějaký \\(d\\)-dimenzionální podprostor našeho Hilbertova prostoru \\(\\mathcal H\\), označme jej \\(V_d\\). Předpokládejme, že tento podprostor \\(V_d\\) má ortonormální bázi \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). Definujeme transformaci \\(P_{V_d}\\) jakožto ortogonální projekci na podprostor \\(V_d\\), tedy můžeme psát \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Nyní můžeme pro klasifikaci použít koeficienty z ortogonální projekce, tedy aplikujeme standardní SVM na vektory \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). Využitím této transformace jsme tedy definovali nové, tzv. adaptované jádro, které je složené z ortogonální projekce \\(P_{V_d}\\) a jádrové funkce standardní metody podpůrných vektorů. Máme tedy (adaptované) jádro \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). Jde tedy o metodu redukce dimenze, kterou můžeme nazvat filtrace. Pro samotnou projekci použijeme v R funkci project.basis() z knihovny fda. Na jejím vstupu bude matice původních diskrétních (nevyhlazených) dat, hodnoty, ve kterých měříme hodnoty v matici původních dat a bázový objekt, na který chceme data projektovat. My zvolíme projekci na B-splinovou bázi, protože využití Fourierovy báze není pro naše neperiodická data vhodné. Další možností je využít wavelet basis. Dimenzi \\(d\\) volíme buď z nějaké předchozí expertní znalosti, nebo pomocí cross-validace. V našem případě určíme optimální dimenzi podprostoru \\(V_d\\) pomocí \\(k\\)-násobné cross-validace (volíme \\(k \\ll n\\) kvůli výpočetní náročnosti metody, často se volí \\(k = 5\\) nebo \\(k = 10\\)). Požadujeme B-spliny řádu 4, pro počet bázových funkcí potom platí vztah \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] kde \\(n_{breaks}\\) je počet uzlů a \\(n_{order} = 4\\). Minimální dimenzi tedy (pro \\(n_{breaks} = 1\\)) volíme \\(n_{basis} = 3\\) a maximální (pro \\(n_{breaks} = 51\\) odpovídající počtu původních diskrétních dat) \\(n_{basis} = 53\\). V R však hodnota \\(n_{basis}\\) musí být alespoň \\(n_{order} = 4\\) a pro velké hodnoty \\(n_{basis}\\) již dochází k přefitování modelu, tudíž volíme za maximální \\(n_{basis}\\) menší číslo, řekněme 43. Code k_cv &lt;- 10 # k-fold CV # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # vsechny dimenze, ktere chceme vyzkouset # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list se tremi slozkami ... maticemi pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro danou cast trenovaci mnoziny # v radcich budou hodnoty pro danou hodnotu dimenze CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projekce diskretnich dat na B-splinovou bazi o dimenzi d Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 23 0.0125 ## poly 6 0.0250 ## radial 23 0.0125 Vidíme, že nejlépe vychází hodnota parametru \\(d\\) jako 23 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9875, 6 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.975 a 23 pro radiální jádro s hodnotou přesnosti 0.9875. Pro přehlednost si ještě vykresleme průběh validačních chybovostí v závislosti na dimenzi \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validační chybovost&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Obrázek 4.14: Závislost validační chybovosti na dimenzi podprostoru \\(V_d\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Černými body jsou vyznačeny optimální hodnoty dimenze \\(V_d\\) pro jednotlivé jádrové funkce. Nyní již můžeme natrénovat jednotlivé klasifikátory na všech trénovacích datech a podívat se na jejich úspěšnost na testovacích datech. Pro každou jádrovou funkci volíme dimenzi podprostoru, na který projektujeme, podle výsledků cross-validace. V proměnné Projection máme uloženou matici koeficientů ortogonální projekce, tedy \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # bazovy objekt bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projekce diskretnich dat na B-splinovou bazi Projection &lt;- project.basis(y = XX, # matice diskretnich dat argvals = t, # vektor argumentu basisobj = bbasis) # bazovy objekt # rozdeleni na trenovaci a testovaci data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Přesnost metody SVM aplikované na bázové koeficienty na trénovacích datech je tedy 1.54 % pro lineární jádro, 3.08 % pro polynomiální jádro a 1.54 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 3.57 % pro lineární jádro, 7.14 % pro polynomiální jádro a 7.14 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 4.2.7.5 RKHS + SVM V této sekci se podíváme na další možnost, jak využít metodu podpůrných vektorů pro klasifikaci funkcionálních dat. V tomto případě půjde opět o již nám známý princip, kdy nejprve funkcionální data vyjádříme jakožto nějaké konečně-rozměrné objekty a na tyto objekty následně aplikujeme klasickou metodu SVM. Nyní však metodu SVM použijeme i pro samotnou reprezentaci funkcionálních dat pomocí určitého konečně-rozměrného objektu. Jak již název napovídá, půjde o kombinaci dvou konceptů – jednak metody podpůrných vektorů a druhak prostoru, který se nazývá v anglické literatuře Reproducing Kernel Hilbert Space. Pro tento prostor je klíčovým pojmem jádro – kernel. Definice [Jádro]: Jádro je taková funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\), že pro každou dvojici \\(\\boldsymbol x, \\tilde{\\boldsymbol x} \\in \\mathcal X\\) platí \\[ K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H}, \\] kde \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) je zobrazení z prostoru \\(\\mathcal X\\) do prostoru \\(\\mathcal H\\). Aby funkce byla jádrem, musí splňovat určité podmínky. Lemma: Nechť \\(\\mathcal X\\) je nějaký Hilbertův prostor. Potom symetrická funkce \\(K : \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem, pokud \\(\\forall k \\geq 1, \\boldsymbol x_1, \\dots, \\boldsymbol x_k \\in \\mathcal X\\) a \\(c_1, \\dots, c_k \\in \\mathbb R\\) platí \\[ \\sum_{i, j = 1}^k c_ic_j K(\\boldsymbol x_i, \\boldsymbol x_j) \\geq 0. \\] Vlastnost výše se nazývá pozitivní semidefinitnost. Platí také následující tvrzení. Tvrzení: Funkce \\(K: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R\\) je jádrem právě tehdy, když existuje Hilbertův prostor \\(\\mathcal H\\) a zobrazení \\(\\boldsymbol\\phi : \\mathcal X \\rightarrow \\mathcal H\\) takové, že \\[ K(\\boldsymbol x, \\tilde{\\boldsymbol x}) = \\big\\langle \\boldsymbol\\phi(\\boldsymbol x), \\boldsymbol\\phi(\\tilde{\\boldsymbol x}) \\big\\rangle_{\\mathcal H} \\quad \\forall \\boldsymbol x, \\tilde{\\boldsymbol x}\\in \\mathcal X. \\] Nyní již máme připravenou půdu pro zavedení pojmu Reproducing Kernel Hilbert Space. 4.2.7.5.1 Reproducing Kernel Hilbert Space (RKHS) Uvažujme Hilbertův prostor \\(\\mathcal H\\) jakožto prostor funkcí. Naším cílem je definovat prostor \\(\\mathcal H\\) a zobrazení \\(\\phi\\) takové, že \\(\\phi(x) \\in \\mathcal H, \\ \\forall x \\in \\mathcal X\\). Označme \\(\\phi(x) = k_x\\). Každé funkci \\(x \\in \\mathcal X\\) tedy přiřadíme funkci \\(x \\mapsto k_x \\in \\mathcal H, k_x := K(x, \\cdot), k_x: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\phi: \\mathcal X \\rightarrow \\mathbb R^{\\mathcal X}\\), můžeme tedy souhrnně napsat \\[ x \\in \\mathcal X \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal H, \\] Bod (funkce) \\(x \\in \\mathcal X\\) je zobrazen na funkci \\(k_x: \\mathcal X \\rightarrow \\mathbb R, k_x(y) = K(x, y)\\). Uvažujme množinu všech obrazů \\(\\{k_x | x \\in \\mathcal X\\}\\) a definujme lineární obal této množiny vektorů jakožto \\[ \\mathcal G := \\text{span}\\{k_x | x \\in \\mathcal X\\} = \\left\\{\\sum_{i = 1}^r\\alpha_i K(x_i, \\cdot)\\ \\Big|\\ \\alpha_i \\in \\mathbb R, r \\in \\mathbb N, x_i \\in \\mathcal X\\right\\}. \\] Potom skalární součin \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal X \\] a obecně \\[ f, g \\in \\mathcal G, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal G} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j\\alpha_i\\beta_j K(x_i, y_j). \\] Prostor \\(\\mathcal H := \\overline{\\mathcal G}\\), který je zúplněním prostoru \\(\\mathcal G\\), nazýváme Reproducing Kernel Hilbert Space (RKHS). Významnou vlastností tohoto prostoru je \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal H}. \\] Poznámka: Jméno Reproducing vychází z následujícího faktu. Mějme libovolnou funkci \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\). Potom \\[\\begin{align*} \\langle K(x, \\cdot), f\\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Vlastnosti: nechť \\(\\mathcal H\\) je Hilbertův prostor funkcí \\(g: \\mathcal X \\rightarrow \\mathbb R\\). Potom \\(\\mathcal H\\) je RKHS \\(\\Leftrightarrow\\) všechny funkcionály (evaluation functionals) \\(\\delta_x: \\mathcal H \\rightarrow \\mathbb R, g \\mapsto g(x)\\) jsou spojité, pro dané jádro \\(K\\) existuje právě jeden prostor RKHS (až na isometrickou izomofrii), pro daný RKHS je jádro \\(K\\) určeno jednoznačně, funkce v RKHS jsou bodově korektně definovány, RKHS je obecně nekonečně-rozměrný vektorový prostor, v praxi však pracujeme pouze s jeho konečně-rozměrným podprostorem. Na konec této sekce si uveďme jedno důležité tvrzení. Tvrzení 4.1 (The representer theorem) Nechť \\(K\\) je jádro a \\(\\mathcal H\\) je příslušný RKHS s normou a skalárním součinem \\(\\|\\cdot\\|_{\\mathcal H}\\) a \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Předpokládejme, že chceme zjistit lineární funkci \\(f: \\mathcal H \\rightarrow \\mathbb R\\) na Hilbertově prostoru \\(\\mathcal H\\) definovaného jádrem \\(K\\). Funkce \\(f\\) má tvar \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) pro nějaké \\(\\omega \\in \\mathcal H\\). Uvažujme regularizovaný minimalizační problém \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{4.1} \\end{equation}\\] kde \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) je striktně monotonně rostoucí funkce (regularizer), \\(R_n(\\cdot)\\) je empirická ztráta (empirical risk) klasifikátoru vzhledem ke ztrátové funkci \\(\\ell\\). Potom optimalizační úloha (4.1) má vždy optimální řešení a to je tvaru \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{4.2} \\end{equation}\\] kde \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) je množina trénovacích hodnot. \\(\\mathcal H\\) je obecně nekočně-rozměrný prostor, ale pro konečný datový soubor velikosti \\(n\\) má \\(\\mathcal H\\) dimenzi nejvýše \\(n\\). Každý \\(n\\)-dimenzionální podprostor Hilbertova prostoru je navíc izometrický s \\(\\mathbb R^n\\), tudíž můžeme předpokládat, že zobrazení (feature map) zobrazuje právě do \\(\\mathbb R^n\\). Jádro \\(K\\) je univerzální pokud RKHS \\(\\mathcal H\\) je hustá množina v \\(\\mathcal C(\\mathcal X)\\) (množina spojitých funkcí). Navíc platí následující poznatky: univerzální jádra jsou dobrá pro aproximaci, Gaussovo jádro s pevnou hodnotou \\(\\sigma\\) je univerzální, univerzalita je nutnou podmínkou pro konzistenci. 4.2.7.5.2 Klasifikace pomocí RKHS Základní myšlenkou je projekce původních dat na podprostor prostoru RKHS, označme jej \\(\\mathcal H_K\\) (index \\({}_K\\) odkazuje na fakt, že tento prostor je definován jádrem \\(K\\)). Cílem je tedy transformovat křivku (pozorovaný objekt, funkce) na bod v RKHS. Označme \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\) množinu pozorovaných křivek, přičemž každá křivka \\(\\hat c_l\\) je definována daty \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), kde \\(\\mathcal X\\) je prostor vstupních proměnných a nejčastěji \\(\\mathcal Y = \\mathbb R\\). Předpokládejme, že pro každou funkci \\(\\hat c_l\\) existuje spojitá funkce \\(c_l:\\mathcal X \\rightarrow \\mathcal Y, \\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). Předpokládejme také, že \\(\\boldsymbol x_i\\) jsou společné pro všechny křivky. Muñoz a González ve svém článku1 navrhují následující postup. Křivku \\(c_l^*\\) můžeme napsat ve tvaru \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] kde \\(\\alpha_{il} \\in \\mathbb R\\). Tyto koeficienty získáme v praxi řešením optimalizačního problému \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\gamma &gt; 0, \\varepsilon \\geq 0, \\] tedy právě například pomocí metody SVM. Díky známé vlastnosti této metody pak bude mnoho koeficientů \\(\\alpha_{il} = 0\\). Minimalizací výše uvedeného výrazu získáme funkce \\(c_1^*, \\dots, c_n^*\\) odpovídající původním křivkám \\(\\hat c_1, \\dots, \\hat c_n\\). Metoda SVM tedy dává smysluplnou reprezentaci původních křivek pomocí vektoru koeficientů \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) pro \\(\\hat c_l\\). Tato reprezentace je však velmi nestabilní, neboť i při malé změně původních hodnot může dojít ke změně v množině podpůrných vektorů pro danou funkci, a tedy dojde k výrazné změně celé reprezentace této křivky (reprezentace není spojitá ve vstupních hodnotách). Definujeme proto novou reprezentaci původních křivek, která již nebude trpět tímto nedostatkem. Tvrzení: Nechť \\(c\\) je funkce, jejíž pozorovaná verze je \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\) a \\(K\\) je jádro s vlastními funkcemi \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (báze \\(\\mathcal H_K\\)). Potom funkce \\(c^*(\\boldsymbol x)\\) může být vyjádřena ve tvaru \\[ c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\] kde \\(\\lambda_j^*\\) jsou váhy projekce \\(c^*(\\boldsymbol x)\\) na prostor funkcí generovaný vlastními funkcemi jádra \\(K\\) a \\(d\\) je dimenze prostoru \\(\\mathcal H\\). V praxi, kdy máme k dispozici pouze konečně mnoho pozorování, \\(\\lambda_j^*\\) mohou být odhadnuty pomocí \\[ \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\] kde \\(\\hat\\lambda_j\\) je \\(j\\)-té vlastní číslo příslušné \\(j\\)-tému vlastnímu vektoru \\(\\hat\\phi_j\\) matice \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m, \\hat d = \\text{rank}(K_S)\\) a \\(\\alpha_i\\) jsou řešením optimalizačního problému. 4.2.7.5.3 Implementace metody v R Z poslední části Tvrzení vyplývá, jak máme spočítat v praxi reprezentace křivek. Budeme pracovat s diskretizovanými daty po vyhlazení křivek. Nejprve si definujeme jádro pro prostor RKHS. Využijeme Gaussovské jádro s parametrem \\(\\gamma\\). Hodnota tohoto hyperparametru výrazně ovlivňuje chování a tedy i úspěšnost metody, proto jeho volbě musíme věnovat zvláštní pozornost (volíme pomocí cross-validace). 4.2.7.5.3.1 Gaussovké jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;)^2)) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Spočítejme nyní matici \\(K_S\\) a její vlastní čísla a příslušné vlastní vektory. Code # spocitame matici K gamma &lt;- 0.1 # pevna hodnota gamma, optimalni urcime pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors K výpočtu koeficientů v reprezentaci křivek, tedy výpočtu vektorů \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), potřebujeme ještě koeficienty z SVM. Narozdíl od klasifikačního problému nyní řešíme problém regrese, neboť se snažíme vyjádřit naše pozorované křivky v nějaké (námi zvolené pomocí jádra \\(K\\)) bázi. Proto využijeme metodu Support Vector Regression, z níž následně získáme koeficienty \\(\\alpha_{il}\\). Code # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } Nyní již můžeme spočítat reprezentace jednotlivých křivek. Nejprve zvolme za \\(\\hat d\\) celou dimenzi, tedy \\(\\hat d = m ={}\\) 51, následně určíme optimální \\(\\hat d\\) pomocí cross-validace. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Nyní máme v matici Lambda.RKHS uloženy ve sloupcích vektory \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) pro každou křivku. Tyto vektory nyní využijeme jakožto reprezentaci daných křivek a klasifikujeme data podle této diskretizace. Code # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 4.1: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0000 0.1429 SVM poly - RKHS 0.0769 0.2500 SVM rbf - RKHS 0.0154 0.2143 Vidíme, že model u všech třech jader velmi dobře klasifikuje trénovací data, zatímco jeho úspěšnost na testovacích datech není vůbec dobrá. Je zřejmé, že došlo k overfittingu, proto využijeme cross-validaci, abychom určili optimální hodnoty \\(\\gamma\\) a \\(d\\). Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:20 # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 0, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean, na.rm = TRUE) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l, na.rm = TRUE), min(CV.results$SVM.p, na.rm = TRUE), min(CV.results$SVM.r, na.rm = TRUE)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 4.2: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 18 0.1000 0.0268 linear poly 11 0.0193 0.0417 polynomial radial 14 0.5179 0.0292 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 18 a \\(\\gamma={}\\) 0.1 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9732, \\(d={}\\) 11 a \\(\\gamma={}\\) 0.0193 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9583 a \\(d={}\\) 14 a \\(\\gamma={}\\) 0.5179 pro radiální jádro s hodnotou přesnosti 0.9708. Pro zajímavost si ještě vykresleme funkci validační chybovosti v závislosti na dimenzi \\(d\\) a hodnotě hyperparametru \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Obrázek 4.15: Závislost validační chybovosti na volbě hyperparametrů \\(d\\) a \\(\\gamma\\), zvlášť pro všechna tři uvažovaná jádra v metodě SVM. Na grafech výše vidíme, jak se měnila validační chybovost v závislosti na hodnotách hyperparametrů \\(d\\) a \\(\\gamma\\). Všimněme si zejména, že ve všech třech grafech pro jednotlivá jádra jsou patrné výrazné horizontální útvary. Z toho můžeme usoudit významné teoretické i praktické zjištění – uvažovaná klasifikační metoda (projekce na RKHS pomocí SVM + klasifikace SVM) je robustní na volbu hyperparametru \\(d\\) (tj. při malé změně v hodnotě tohoto parametru nedojde k výraznému zhoršení validační chybovosti), zatímco při volbě hyperparametru \\(\\gamma\\) musíme být velmi obezřetní (i malá změna v jeho hodnotě může vést k velké změně validační chybovosti). Toto chování je nejlépe patrné u Gaussova jádra. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 4.3: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0154 0.1429 SVM poly - RKHS - radial 0.0462 0.1429 SVM rbf - RKHS - radial 0.0308 0.1786 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 1.54 % pro lineární jádro, 4.62 % pro polynomiální jádro a 3.08 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 14.29 % pro lineární jádro, 14.29 % pro polynomiální jádro a 17.86 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 4.2.7.5.3.2 Polynomiální jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:20 # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean, na.rm = TRUE) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l, na.rm = TRUE), min(CV.results$SVM.p, na.rm = TRUE), min(CV.results$SVM.r, na.rm = TRUE)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Code Vidíme, že nejlépe vychází hodnota parametru $d={}$ `r d.opt[1]` a $p={}$ `r poly.opt[1]` pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV `r round(1-err.opt.cv[1], 4)`, $d={}$ `r d.opt[2]` a $p={}$ `r poly.opt[2]` pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV `r round(1 - err.opt.cv[2], 4)` a $d={}$ `r d.opt[3]` a $p={}$ `r poly.opt[3]` pro radiální jádro s hodnotou přesnosti `r round(1 - err.opt.cv[3], 4)`. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Code Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna `r 100 * round(Res[1, 2], 4)` % pro lineární jádro, `r 100 * round(Res[2, 2], 4)` % pro polynomiální jádro a `r 100 * round(Res[3, 2], 4)` % pro gaussovské jádro. Na testovacích datech je potom přesnost metody `r 100 * round(Res[1, 3], 4)` % pro lineární jádro, `r 100 * round(Res[2, 3], 4)` % pro polynomiální jádro a `r 100 * round(Res[3, 3], 4)` % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) 4.2.7.5.3.3 Lineární jádro Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- 3:20 # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean, na.rm = TRUE) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Tabulka 4.4: Souhrnné výsledky cross-validace pro metodu SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 13 0.0718 linear poly 19 0.1200 polynomial radial 14 0.0593 radial Vidíme, že nejlépe vychází hodnota parametru \\(d={}\\) 13 pro lineární jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.9282, \\(d={}\\) 19 pro polynomiální jádro s hodnotou přesnosti spočtenou pomocí 10-násobné CV 0.88 a \\(d={}\\) 14 pro radiální jádro s hodnotou přesnosti 0.9407. Jelikož již máme nalezeny optimální hodnoty hyperparametrů, můžeme zkounstruovat finální modely a určit jejich úspěšnost klasifikace na testovacích datech. Code # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Tabulka 4.5: Souhrnné výsledky metody SVM v kombinaci s RKHS na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.0462 0.1429 SVM poly - RKHS - linear 0.0308 0.1786 SVM rbf - RKHS - linear 0.0462 0.1429 Přesnost metody SVM v kombinaci s projekcí na Reproducing Kernel Hilbert Space je tedy na trénovacích datech rovna 4.62 % pro lineární jádro, 3.08 % pro polynomiální jádro a 4.62 % pro gaussovské jádro. Na testovacích datech je potom přesnost metody 14.29 % pro lineární jádro, 17.86 % pro polynomiální jádro a 14.29 % pro radiální jádro. Code RESULTS &lt;- rbind(RESULTS, Res) Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["tabulka-výsledků-2.html", "4.3 Tabulka výsledků", " 4.3 Tabulka výsledků Tabulka 4.6: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti a \\(\\widehat{Err}_{test}\\) testovací chybovosti. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.0154 0.1429 LDA 0.0154 0.0357 QDA 0.0308 0.0357 LR score 0.0308 0.0357 Tree - diskr. 0.1231 0.2143 Tree - score 0.0615 0.1429 Tree - Bbasis 0.1231 0.2143 RForest - diskr 0.0000 0.0714 RForest - score 0.0000 0.0714 RForest - Bbasis 0.0154 0.0714 SVM linear - diskr 0.0154 0.0357 SVM poly - diskr 0.2154 0.0714 SVM rbf - diskr 0.0308 0.1071 SVM linear - PCA 0.0154 0.0357 SVM poly - PCA 0.0462 0.0357 SVM rbf - PCA 0.0154 0.0357 SVM linear - Bbasis 0.0154 0.0357 SVM poly - Bbasis 0.0154 0.0714 SVM rbf - Bbasis 0.0154 0.0714 SVM linear - projection 0.0154 0.0357 SVM poly - projection 0.0308 0.0714 SVM rbf - projection 0.0154 0.0714 SVM linear - RKHS - radial 0.0154 0.1429 SVM poly - RKHS - radial 0.0462 0.1429 SVM rbf - RKHS - radial 0.0308 0.1786 SVM linear - RKHS - linear 0.0462 0.1429 SVM poly - RKHS - linear 0.0308 0.1786 SVM rbf - RKHS - linear 0.0462 0.1429 "],["opakování.html", "Kapitola 5 Opakování", " Kapitola 5 Opakování Code # pocet simulaci n.sim &lt;- 100 ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA) Code ## SIMULACE for(sim in 1:n.sim) { t &lt;- data.gr$age rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizujeme 2. derivaci # spojeni pozorovani do jedne matice XX &lt;- data.gr[, -1] |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -7, to = 1, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), c(n_girls, n_boys)) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) ## 1) K nejbližších sousedů k_cv &lt;- 5 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis # ### 10-fold cross-validation # n.basis.max &lt;- 25 # n.basis &lt;- 4:n.basis.max # k_cv &lt;- 10 # k-fold CV # # rozdelime trenovaci data na k casti # folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # ## prvky, ktere se behem cyklu nemeni # # body, ve kterych jsou funkce vyhodnoceny # tt &lt;- x.train[[&quot;argvals&quot;]] # rangeval &lt;- range(tt) # # B-spline baze # basis1 &lt;- X.train$basis # # vztah # f &lt;- Y ~ x # # baze pro x # basis.x &lt;- list(&quot;x&quot; = basis1) # # CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, # dimnames = list(n.basis, 1:k_cv)) # # for (index in 1:k_cv) { # # definujeme danou indexovou mnozinu # fold &lt;- folds[[index]] # # x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; # fdata() # y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; # as.numeric() # # x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; # fdata() # y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; # as.numeric() # # dataf &lt;- as.data.frame(y.train.cv) # colnames(dataf) &lt;- &quot;Y&quot; # # for (i in n.basis) { # # baze pro bety # basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) # # basis.b &lt;- list(&quot;x&quot; = basis2) # # vstupni data do modelu # ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # # binomicky model ... model logisticke regrese # model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, # basis.x = basis.x, basis.b = basis.b) # # # presnost na validacni casti # newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) # predictions.valid &lt;- predict(model.glm, newx = newldata) # predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) # presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # # vlozime do matice # CV.results[as.character(i), as.character(index)] &lt;- presnost.valid # } # } # # # spocitame prumerne presnosti pro jednotliva n pres folds # CV.results &lt;- apply(CV.results, 1, mean) # n.basis.opt &lt;- n.basis[which.max(CV.results)] # presnost.opt.cv &lt;- max(CV.results) # # # optimalni model # basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) # f &lt;- Y ~ x # # baze pro x a bety # basis.x &lt;- list(&quot;x&quot; = basis1) # basis.b &lt;- list(&quot;x&quot; = basis2) # # vstupni data do modelu # dataf &lt;- as.data.frame(y.train) # colnames(dataf) &lt;- &quot;Y&quot; # ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # # binomicky model ... model logisticke regrese # model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, # basis.x = basis.x, basis.b = basis.b) # # # presnost na trenovacich datech # predictions.train &lt;- predict(model.glm, newx = ldata) # predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) # presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # # presnost na trenovacich datech # newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) # predictions.test &lt;- predict(model.glm, newx = newldata) # predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) # presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # Res &lt;- data.frame(model = &#39;LR_functional&#39;, # Err.train = 1 - presnost.train, # Err.test = 1 - presnost.test) # # RESULTS &lt;- rbind(RESULTS, Res) # ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(rangeval[1], rangeval[2], length = 51) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 2:4] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 20, by =2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 1, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean, na.rm = TRUE) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l, na.rm = TRUE), min(CV.results$SVM.p, na.rm = TRUE), min(CV.results$SVM.r, na.rm = TRUE)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p # Poly.kernel &lt;- function(x, y, p) { # return((1 + x * y)^p) # } # # Kernel.RKHS &lt;- function(x, p) { # K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) # for(i in 1:nrow(K)) { # for(j in 1:ncol(K)) { # K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) # } # } # return(K) # } # # # rozdelime trenovaci data na k casti # folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # # odstranime posledni sloupec, ve kterem jsou hodnoty Y # data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # # # hodnoty hyperparametru, ktere budeme prochazet # dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # poly.cv &lt;- 2:5 # # # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # # prazdna matice, do ktere vlozime jednotlive vysledky # # ve sloupcich budou hodnoty presnosti pro dane # # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds # dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), # d = paste0(&#39;d:&#39;, dimensions), # CV = paste0(&#39;cv:&#39;, 1:k_cv)) # # CV.results &lt;- list( # SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), # dimnames = dim.names), # SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), # dimnames = dim.names), # SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), # dimnames = dim.names)) # # # samotna CV # for (p in poly.cv) { # K &lt;- Kernel.RKHS(t.seq, p = p) # Eig &lt;- eigen(K) # eig.vals &lt;- Eig$values # eig.vectors &lt;- Eig$vectors # alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # # # model # for(i in 1:dim(data.RKHS)[2]) { # df.svm &lt;- data.frame(x = t.seq, # y = data.RKHS[, i]) # svm.RKHS &lt;- svm(y ~ x, data = df.svm, # kernel = &#39;polynomial&#39;, # type = &#39;eps-regression&#39;, # epsilon = 0.1, # degree = p) # alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # } # # # projdeme dimenze # for(d.RKHS in dimensions) { # Lambda.RKHS &lt;- matrix(NA, # ncol = dim(data.RKHS)[2], # nrow = d.RKHS) # # vypocet reprezentace # for(l in 1:dim(data.RKHS)[2]) { # Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% # alpha.RKHS[, l]) * eig.vals[1:d.RKHS] # } # # projdeme folds # for (index_cv in 1:k_cv) { # # definice testovaci a trenovaci casti pro CV # fold &lt;- folds[[index_cv]] # # rozdeleni na trenovaci a validacni data # XX.train &lt;- Lambda.RKHS[, fold] # XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # # pripravime si datovou tabulku pro ulozeni vysledku # Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, # &#39;SVM poly - RKHS&#39;, # &#39;SVM rbf - RKHS&#39;), # Err.test = NA) # # projdeme jednotliva jadra # for (kernel_number in 1:3) { # kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # # data.RKHS.train &lt;- as.data.frame(t(XX.train)) # data.RKHS.train$Y &lt;- factor(Y.train[fold]) # # data.RKHS.test &lt;- as.data.frame(t(XX.test)) # data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # # # sestrojeni modelu # clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, # type = &#39;C-classification&#39;, # scale = TRUE, # kernel = kernel_type) # # # presnost na validacnich datech # predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) # presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # # ulozeni vysledku # Res[kernel_number, 2] &lt;- 1 - presnost.test # } # # presnosti vlozime na pozice pro dane d, gamma a fold # CV.results$SVM.l[paste0(&#39;p:&#39;, p), # paste0(&#39;d:&#39;, d.RKHS), # index_cv] &lt;- Res[1, 2] # CV.results$SVM.p[paste0(&#39;p:&#39;, p), # paste0(&#39;d:&#39;, d.RKHS), # index_cv] &lt;- Res[2, 2] # CV.results$SVM.r[paste0(&#39;p:&#39;, p), # paste0(&#39;d:&#39;, d.RKHS), # index_cv] &lt;- Res[3, 2] # } # } # } # # # spocitame prumerne presnosti pro jednotliva d pres folds # for (n_method in 1:length(CV.results)) { # CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) # } # # poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), # which.min(CV.results$SVM.p) %% length(poly.cv), # which.min(CV.results$SVM.r) %% length(poly.cv)) # poly.opt[poly.opt == 0] &lt;- length(poly.cv) # poly.opt &lt;- poly.cv[poly.opt] # # d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), # which.min(t(CV.results$SVM.p)) %% length(dimensions), # which.min(t(CV.results$SVM.r)) %% length(dimensions)) # d.opt[d.opt == 0] &lt;- length(dimensions) # d.opt &lt;- dimensions[d.opt] # # err.opt.cv &lt;- c(min(CV.results$SVM.l, na.rm = TRUE), # min(CV.results$SVM.p, na.rm = TRUE), # min(CV.results$SVM.r, na.rm = TRUE)) # df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, # Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), # row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) # # # odstranime posledni sloupec, ve kterem jsou hodnoty Y # data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # # pridame i testovaci data # data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # # # pripravime si datovou tabulku pro ulozeni vysledku # Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, # &#39;SVM poly - RKHS - poly&#39;, # &#39;SVM rbf - RKHS - poly&#39;), # Err.train = NA, # Err.test = NA) # # # projdeme jednotliva jadra # for (kernel_number in 1:3) { # # spocitame matici K # p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV # K &lt;- Kernel.RKHS(t.seq, p = p) # # # urcime vlastni cisla a vektory # Eig &lt;- eigen(K) # eig.vals &lt;- Eig$values # eig.vectors &lt;- Eig$vectors # # urceni koeficientu alpha z SVM # alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], # ncol = dim(data.RKHS)[2]) # prazdny objekt # # # model # for(i in 1:dim(data.RKHS)[2]) { # df.svm &lt;- data.frame(x = t.seq, # y = data.RKHS[, i]) # svm.RKHS &lt;- svm(y ~ x, data = df.svm, # kernel = &#39;polynomial&#39;, # type = &#39;eps-regression&#39;, # epsilon = 0.1, # degree = p) # # urceni alpha # alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty # } # # d # d.RKHS &lt;- d.opt[kernel_number] # # # urceni vektoru lambda # Lambda.RKHS &lt;- matrix(NA, # ncol = dim(data.RKHS)[2], # nrow = d.RKHS) # vytvoreni prazdneho objektu # # # vypocet reprezentace # for(l in 1:dim(data.RKHS)[2]) { # Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] # } # # # rozdeleni na trenovaci a testovaci data # XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] # XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # # kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # # data.RKHS.train &lt;- as.data.frame(t(XX.train)) # data.RKHS.train$Y &lt;- factor(Y.train) # # data.RKHS.test &lt;- as.data.frame(t(XX.test)) # data.RKHS.test$Y &lt;- factor(Y.test) # # # sestrojeni modelu # clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, # type = &#39;C-classification&#39;, # scale = TRUE, # kernel = kernel_type) # # # presnost na trenovacich datech # predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) # presnost.train &lt;- table(Y.train, predictions.train) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # # presnost na testovacich datech # predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) # presnost.test &lt;- table(Y.test, predictions.test) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # # ulozeni vysledku # Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) # } # # RESULTS &lt;- rbind(RESULTS, Res) # ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 20, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean, na.rm = TRUE) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l, na.rm = TRUE), min(CV.results$SVM.p, na.rm = TRUE), min(CV.results$SVM.r, na.rm = TRUE)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test } ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. Code # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) 5.0.1 Výsledky Tabulka 5.1: Souhrnné výsledky použitých metod na simulovaných datech. \\(\\widehat{Err}_{train}\\) značí odhad trénovací chybovosti, \\(\\widehat{Err}_{test}\\) testovací chybovosti, \\(\\widehat{SD}_{train}\\) odhad směrodatné odchylky trénovacích chybovostí a \\(\\widehat{SD}_{test}\\) je odhad směrodatné odchylky testovacích chybovostí. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.0471 0.0296 0.0204 0.0274 LDA 0.0220 0.0229 0.0103 0.0224 QDA 0.0292 0.0364 0.0151 0.0321 LR_score 0.0297 0.0404 0.0161 0.0389 Tree_discr 0.1032 0.1436 0.0209 0.0592 Tree_score 0.0655 0.1246 0.0195 0.0584 Tree_Bbasis 0.1032 0.1436 0.0209 0.0592 RF_discr 0.0174 0.1025 0.0123 0.0667 RF_score 0.0135 0.0768 0.0116 0.0548 RF_Bbasis 0.0191 0.1004 0.0118 0.0635 SVM linear - diskr 0.0202 0.0539 0.0136 0.0361 SVM poly - diskr 0.1955 0.2232 0.0411 0.0897 SVM rbf - diskr 0.0217 0.0479 0.0107 0.0349 SVM linear - PCA 0.0242 0.0304 0.0116 0.0302 SVM poly - PCA 0.0583 0.0693 0.0222 0.0497 SVM rbf - PCA 0.0223 0.0296 0.0108 0.0301 SVM linear - Bbasis 0.0198 0.0489 0.0108 0.0375 SVM poly - Bbasis 0.1506 0.1918 0.0561 0.1070 SVM rbf - Bbasis 0.0217 0.0382 0.0103 0.0293 SVM linear - projection 0.0245 0.0414 0.0138 0.0324 SVM poly - projection 0.0526 0.0814 0.0242 0.0572 SVM rbf - projection 0.0218 0.0439 0.0110 0.0328 SVM linear - RKHS - radial 0.0338 0.0854 0.0176 0.0525 SVM poly - RKHS - radial 0.0391 0.0993 0.0167 0.0615 SVM rbf - RKHS - radial 0.0418 0.0814 0.0171 0.0449 SVM linear - RKHS - linear 0.0534 0.1543 0.0290 0.0638 SVM poly - RKHS - linear 0.0632 0.1775 0.0226 0.0795 SVM rbf - RKHS - linear 0.0529 0.0950 0.0191 0.0535 V tabulce výše jsou uvedeny všechny vypočtené charakteristiky. Jsou zde uvedeny také směrodatné odchylky, abychom mohli porovnat jakousi stálost či míru variability jednotlivých metod. Nakonec ještě můžeme graficky zobrazit vypočtené hodnoty ze simulace pro jednotlivé klasifikační metody pomocí krabicových diagramů, zvlášť pro testovací a trénovací chybovosti. Code # pro trenovaci data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 5.1: Krabicové diagramy trénovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Code # pro testovaci data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Klasifikační metoda&#39;, y = expression(widehat(Err)[test])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Obrázek 5.2: Krabicové diagramy testovacích chybovostí pro 100 simulací zvlášť pro jednotlivé klasifikační metody. Černými symboly \\(+\\) jsou vyznačeny průměry. Nakonec se podívejme, jaké hodnoty hyperparametrů byly nejčastější volbou. Tabulka 5.2: Mediány hodnot hyperparametrů pro vybrané metody, u nichž se určoval nějaký hyperparametr pomocí cross-validace. \\(K\\) pro KNN \\(d\\) pro SVM linear \\(d\\) pro SVM poly \\(d\\) pro SVM radial 4 6 6 7 Code CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram() + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hodnoty hyperparametru&#39;, y = &#39;Absolutní počet&#39;) + theme(legend.position = &#39;none&#39;) Obrázek 5.3: Krabicové diagramy hodnot hyperparametrů. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
